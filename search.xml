<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[搭建Tinc实现异地构建局域网]]></title>
    <url>%2F2021%2F12%2F07%2F%E6%90%AD%E5%BB%BATinc%E5%AE%9E%E7%8E%B0%E5%BC%82%E5%9C%B0%E6%9E%84%E5%BB%BA%E5%B1%80%E5%9F%9F%E7%BD%91%2F</url>
    <content type="text"><![CDATA[现实需求：单位电脑和家里电脑可以互连和访问，手机可以随时随地连接单位和家里电脑，众多设备之间最好可以 Peer-to-Peer Tinc的优点linux采用的是kernel 2.1.60 中就有的ethertap device，所以在任何的Linux系统上几乎都可以直接用Tinc有一个最大的优点，就是P2P自动全网状路由，无论您如何设置Tinc守护进程以相互连接，流量始终（如果可能）直接发送到目标，而不通过中间跃点。 Tinc对于国内来说还有一个特别的优势，它同时支持UDP，TCP。由于国内和跨境UDP容易被QoS，然而TCP做这种大流量传输容易出现问题，所以既支持TCP也支持UDP的协议就很有用了。Tinc 居然是先去尝试UDP连接，如果UDP不通后再尝试TCP，完美解决这个困难。 tinc网络实际使用来做什么1234567891011希望能进行异地之间数据的交换（远程办公，远程访问家庭NAS，远程备份……）远程控制，ssh 远程桌面 VNC代理转发​个人有时候做些项目，需要。例如开发微信，给别人看产品样品，​通过代理访问受限资源个人/小型公司的代码管理等等 连接多个以太网段形成1个网络安全、管理和运维​秘钥长度的选择​压缩的选择通过工具，例如 ansible 简化客户端管理通过NAT，简化对客户端的干预 Tinc VPN 特点123456加密 / 认证 / 压缩自动全网状路由易于扩展网络节点能够进行网络的桥接跨平台支持IPv6 支持 Tinc模式选择Tinc 的配置还是十分自由的。其中一个重要的点就是模式的选择。共有三种模式支持： 路由器模式(router) 该模式需要通过各个 host 配置文件中的 Subnet 来决定包要发到哪 交换机模式(switch) 该模式下 Tinc 的转发会和交换机类似，即：通过动态建立 ARP 表的方式决定包发到哪里 集线器模式(hub) 该模式会把所有发到 Tinc 使用的 Interface 上的包在任何情况下向所有接入该 Tinc 网络的节点广播。由于是在路由器上进行 Tinc 连接，一个最简单的想法便是我们只需要将所有来自内网的、需要转发到远端服务器上再连出的包通过 Tinc 的 Interface 发出，服务端接收到这些包再 NAT 转发出去即可。所以可以采用交换机模式，只利用系统路由来完成包转发路由的确定，从而跳过 Tinc 在路由器模式下内部需要的路由配置。 Tinc配置目录格式Tinc 的配置有着比较规范的目录格式。其配置目录格式如下： Tinc服务端配置server:CentOS Linux release 7.8.2003 安装tinc过程 先下载tinc的源码，当前tinc的最新稳定版本为1.0.36。 wget https://www.tinc-vpn.org/packages/tinc-1.0.36.tar.gz https://www.tinc-vpn.org/packages/windows/ (windows安装包) 下载后解压 1tar zxvf tinc-1.0.36.tar.gz 安装编译依赖环境 123456yum -y install gcc readline-devel zlib-devel lzo-devel openssl-devel ncurses-develcd tinc-1.1./configuremakemake install 公网节点的部署(Master节点) 需要预先定义定义一个网络名 本次以tincnet为例NETNAME = tincnet每个节点均需要以以下目录结构创建好配置文件夹 1/etc/tinc/tincnet 目录结构 123456/etc/tinc/dock 目录下的文件都属于dock这个网络，可以根据自己的情况修改名称，相应下面的命令也需要修改/etc/tinc/dock/hosts 目录是存放其他用户或者说是其他网络的public key以及他们的 ip 地址rsa_key.priv 本网络的私钥tinc.conf 本网络的配置文件tinc-down 本网络关闭时执行的脚本tinc-up 本网络启动时执行的脚本 客户端和服务端都需要手动新建相同的目录结构 Windows端无需新建tinc-up和tinc-down两个文件 第一部分：服务端配置 配置Service 123sudo mkdir -p /usr/local/var/run/sudo mkdir -p /usr/local/etc/tinc/sudo ln -s /usr/local/etc/tinc /etc/tinc 首先需要创建运行目录： 首先开启 Linux 转发，在/etc/sysctl.conf设置net.ipv4.ip_forward = 1，并通过sysctl -p来应用配置。 修改tinc.conf配置文件 1234567Name = ServerInterface = tincMode = switchCompression=9Cipher = aes-256-cbcDigest = sha256PrivateKeyFile=/etc/tinc/dock/rsa_key.priv （可选） 12345678Name 主机名称Interface 隧道所使用的网卡名称Mode 有三种模式，分别是 router / switch / hub ，相对应我们平时使用到的路由、交换机、集线器 (默认模式 router)Compression UDP 数据包压缩级别。可选有 0 (关闭)，1 (fast zlib) 至 9 (best zlib)，10 (fast lzo) 和 11 (best lzo)Cipher 加密类型。可选 aes-128-cbc aes-256-cbc 等Digest rsa 加密协议强度。可选 sha128 sha1 等PrivateKeyFile 服务器私钥的位置 Linux和MacOS修改tinc-up和tinc-down tinc-up 12#!/bin/shifconfig $INTERFACE &lt;内网ip&gt; netmask 255.255.255.0 tinc-down 12#!/bin/shifconfig $INTERFACE down 运行命令添加执行权限 1chmod +x tinc-* 在hosts文件夹内新建在tinc.conf设置的主机名称的节点配置文件(可选) 123Subnet 宣告的路由地址Address 服务器的外网 IPPort 指定 tinc 连接端口(默认端口655) 使用命令生成私钥和公钥 执行 tincd 生成脚本， -n 指定网络名称，-K 指明生成密钥，可以在 -K 后以数字指定密钥长度，普通用途用默认值（2048）即可。命令执行过程中，需要指定文件名，不用管直接两次回车用默认值即可。 1tincd -n [name主机名称网络名称] -K4096 私钥生成在与tinc.conf配置文件相同的文件夹，生成的公钥会自动添加到hosts文件夹内的节点配置文件里 Auto start 12systemctl stop tinc@ [name主机名称网络名称]systemctl start tinc@ [name主机名称网络名称] 第二部分：客户端配置 客户端的tinc.conf与服务器的参数基本上相同，只需要修改Name 12345在hosts文件夹内新建在tinc.conf设置的主机名称的节点配置文件Subnet=10.1.3.2/32tinc-up和tinc-down跟服务器配置基本一样，只需要修改tinc-up的内网ip，Windows客户端无需这两个文件 使用命令生成私钥和公钥 1sudo tincd -n [name主机名称网络名称] -K4096 第三部分：windows客户端配置目前能下载到的最新稳定版是 1.0.35。下载完成后运行安装程序，直接 Next，Next 到 Finish 即可，整个过程非常的简单。 windows客户端生成密钥 1tincd -n [name主机名称网络名称] -K4096 启动服务 1tincd.exe -n [name主机名称网络名称] 参考文档： Tinc 路由配置备忘 搭建tinc实现异地构建局域网 TincVPN：组建虚拟局域网]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>Tinc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes集群搭建流程及出坑记录]]></title>
    <url>%2F2019%2F03%2F26%2Fkubernetes%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%87%BA%E5%9D%91%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[环境说明: Kubernetes版本：1.13.1 操作系统：CentOS-7.5 Docker版本：docker-ce-18.06.1.ce 所有节点都需要安装以下组件： Docker：不用多说了吧 kubelet：运行于所有 Node上，负责启动容器和 Pod kubeadm：负责初始化集群 kubectl： k8s命令行工具，通过其可以部署/管理应用 以及CRUD各种资源 1kubeadm config images list 1234567 kube-apiserver:v1.13.1kube-controller-manager:v1.13.1kube-scheduler:v1.13.1kube-proxy:v1.13.1pause:3.1etcd:3.2.24coredns:1.2.6 虚拟机环境要求： 操作系统 CentOS 7.5 内存 2G 【至少】 CPU 2核【至少】 硬盘 20G 【至少】 各主机的主机名及ip配置: 本次实战中一共用到三台主机，一台用于Master的部署，领导两台分别为node1和node2。主机名和IP的对应关系如下： 安装好centos7.5之后，要提前准备的工作： 所有的节点修改/etc/hosts文件， 主机名/IP加入 hosts解析 1234echo &quot;192.168.1.110 k8s-master&quot; &gt;&gt; /etc/hostsecho &quot;192.168.1.125 k8s-node-1&quot; &gt;&gt; /etc/hostsecho &quot;192.168.1.220 k8s-node-2&quot; &gt;&gt; /etc/hosts 设置所有节点主机名 123hostnamectl --static set-hostname k8s-masterhostnamectl --static set-hostname k8s-node-1hostnamectl --static set-hostname k8s-node-2 关闭防火墙 (全部节点) 防火墙一定要提前关闭，否则在后续安装K8S集群的时候是个trouble maker。执行下面语句关闭，并禁用开机启动： 1systemctl stop firewalld &amp; systemctl disable firewalld 关闭Swap(全部节点) 类似ElasticSearch集群，在安装K8S集群时，Linux的Swap内存交换机制是一定要关闭的，否则会因为内存交换而影响性能以及稳定性。这里，我们可以提前进行设置： 执行swapoff -a可临时关闭，但系统重启后恢复编辑/etc/fstab，注释掉包含swap的那一行即可，重启后可永久关闭，如下所示： 执行swapoff -a可临时关闭，但系统重启后恢复 编辑/etc/fstab，注释掉包含swap的那一行即可，重启后可永久关闭，如下所示 123/dev/mapper/centos-root / xfs defaults 0 0UUID=20ca01ff-c5eb-47bc-99a0-6527b8cb246e /boot xfs defaults 0 0# /dev/mapper/centos-swap swap 或直接执行: 1sed -i &apos;/ swap / s/^/#/&apos; /etc/fstab 关闭成功后，使用top命令查看，如下图所示表示正常： 关闭SeLinux (全部节点) 12345/usr/sbin/sestatus -vsed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/config重启生效 创建/etc/sysctl.d/k8s.conf文件，添加如下内容：(全部节点) 开启内核ipv4转发需要加载br_netfilter模块 123456789net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1#加载br_netfilter模块(centos7没有加载)modprobe br_netfilter#生效系统参数sysctl -p /etc/sysctl.d/k8s.conf 安装docker-ce-18.06.1.ce（全部节点） Centos的系统一定要是64位的，不管版本是什么。并且内核版本至少是3.10以上 安装K8S必须要先安装Docker 执行以下命令，安装最新版Docker： 123456789101112131415161718192021卸载老版本的 docker 及其相关依赖(centos7.5无需这一步)yum remove docker docker-common container-selinux docker-selinux docker-engine安装 yum-utils，它提供了 yum-config-manager，可用来管理yum源yum install -y yum-utils添加yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo更新索引yum makecache fast目前比较稳定而且支持k8s的docker版本为:yum update &amp;&amp; yum install docker-ce-18.06.1.ce启动Docker服务并激活开机启动systemctl start docker &amp; systemctl enable dockerdocker --version 卸载docker 查看已安装的docker 1yum list installed | grep docke 使用yum卸载 1yum remove -y docker-ce.x86_64 #docker-ce.x86_64为已安装的docker，以实际安装的版本为准 安装K8S组件 （所有节点） kubelet、kubeadm、kubectl安装（所有节点） 首先准备repo Configure Kubernetes Repository 1234567cat&gt;&gt;/etc/yum.repos.d/kubrenetes.repo&lt;&lt;EOF[kubernetes]name=Kubernetes Repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpgEOF 执行如下指令来进行安装 12yum install -y kubelet kubeadm kubectlsystemctl enable kubelet &amp;&amp; systemctl start kubelet 初始化 k8s集群 (所有节点) 12345678910111213141516171819202122232425262728293031323334docker pull mirrorgooglecontainers/kube-apiserver:v1.13.1docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.1docker pull mirrorgooglecontainers/kube-scheduler:v1.13.1docker pull mirrorgooglecontainers/kube-proxy:v1.13.1docker pull mirrorgooglecontainers/pause:3.1docker pull mirrorgooglecontainers/etcd:3.2.24docker pull coredns/coredns:1.2.6docker pull registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64docker tag mirrorgooglecontainers/kube-apiserver:v1.13.1 k8s.gcr.io/kube-apiserver:v1.13.1docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.1 k8s.gcr.io/kube-controller-manager:v1.13.1docker tag mirrorgooglecontainers/kube-scheduler:v1.13.1 k8s.gcr.io/kube-scheduler:v1.13.1docker tag mirrorgooglecontainers/kube-proxy:v1.13.1 k8s.gcr.io/kube-proxy:v1.13.1docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6docker tag registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64docker rmi mirrorgooglecontainers/kube-apiserver:v1.13.1docker rmi mirrorgooglecontainers/kube-controller-manager:v1.13.1docker rmi mirrorgooglecontainers/kube-scheduler:v1.13.1docker rmi mirrorgooglecontainers/kube-proxy:v1.13.1docker rmi mirrorgooglecontainers/pause:3.1docker rmi mirrorgooglecontainers/etcd:3.2.24docker rmi coredns/coredns:1.2.6docker rmi registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64#dashboarddocker pull registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0docker tag registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0docker image rm registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0 1kubeadm init --kubernetes-version=1.13.1 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.170 1234567891011121314151617终于成功了。注意最后的这3段输入，分别是：为正常非root用户所做的配置；部署pod network；添加其它node到这个集群（注意token是保密的，不要泄露给别人）。下面会继续提到。To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.1.170:6443 --token i8ev51.a541c7le1ykjwofre --discovery-token-ca-cert-hash sha256:9962e3ef597c9e3bac74e4915d419166a5f813a9c736bd2b77fd1883a09c3649 配置授权信息的目的: 所需的命令在init成功后也会有提示，主要是为了保存相关的配置信息在用户目录下，这样不用每次都输入相关的认证信息。 123mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 以上的安装所有的机器都要同步 初始化Master节点之后，查看各节点的时间是否一致，如果不一致则需要再次同步个节点的时间。时间同步可以看之前NTP相关的文章。保存安装日志的以下部分以备接下来在各node执行： 123systemctl enable kubelet.servicekubeadm init --kubernetes-version=v1.13.1 --apiserver-advertise-address [192.168.1.110] --pod-network-cidr=10.244.0.0/16 选择flannel作为Pod网络插件，所以上面的命令指定–pod-network-cidr=10.244.0.0/16 kubeadm init成功提示之后的正确操作步骤如下：集群初始化如果遇到问题，可以使用下面的命令进行清理 123456kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/ 配置kubectl认证信息（Master节点操作） 1234echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; /etc/profilesource /etc/profileecho $KUBECONFIG 安装Pod网络 (安装flannel网络（Master节点操作）) 使用flannel，可以配置不同的backend来支持多种类型的网络。当然，如果对网络安全有特殊的限制，可以考虑其他的组件. 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 一旦 Pod网络安装完成，可以执行如下命令检查一下 CoreDNS Pod此刻是否正常运行起来了，一旦其正常运行起来，则可以继续后续步骤 1kubectl get pods --all-namespaces -o wide 拆卸集群 在master节点上执行12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 在node2上面执行 123456kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/ 在node1上面执行 1kubectl delete node node2 三.部署pod network 让node1、node2加入集群 在node1和node2节点上分别执行kubeadm join命令，加入集群： 1kubeadm join 192.168.1.110:6443 --token lu231m.8ieu0hjdcpyhnjwm2 --discovery-token-ca-cert-hash sha256:87333cfe365c7234342744bcd3f3cb58fb94f2a6f717b460efb11810049cb5db 默认情况下，Master节点不参与工作负载，但如果希望安装出一个All-In-One的k8s环境，则可以执行以下命令，让Master节点也成为一个Node节点： 1kubectl taint nodes --all node-role.kubernetes.io/master- 查看节点状态1kubectl get nodes 查看pods状态1kubectl get pods --all-namespaces 查看K8S集群状态1kubectl get cs 重启kubelet service1systemctl restart kubelet.service 查看所有pod状态123kubectl get pods -n kube-systemkubectl get pods --all-namespaces -o wide 查看 dashboard的外网暴露端口1kubectl get service --namespace=kube-system 全部Running则表示集群正常。至此，我们的K8S集群就搭建成功了 查看cluster的状态1kubectl cluster-info 如果 token忘记,可以去 Master上执行如下命令来获取1kubeadm token list 安装dashboard在安装好k8s集群之后，确保集群各个节点都处于ready状态的时候，就可以安装kubernetes-dashboard了 K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 如果你厌倦了命令行的操作，全程使用Dashboard也是可行的。 安装 v1.10.0版本的 kubernetes-dashboard，用于集群可视化的管理。 首先手动下载镜像并重新打标签：（所有节点） 123docker pull registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0docker tag registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0docker image rm registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0 安装 dashboard： 1kubectl create -f dashboard.yaml 查看 dashboard的 pod是否正常启动，如果正常说明安装成功: 1kubectl get pods --namespace=kube-system 查看 dashboard的外网暴露端口 1kubectl get service --namespace=kube-system 在master上面生成私钥和证书签名，然后复制到各node上面的 /home/share/certs目录上面 生成私钥和证书签名 1234openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.keyrm dashboard.pass.keyopenssl req -new -key dashboard.key -out dashboard.csr 生成SSL证书： 1openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt 然后将生成的 dashboard.key 和 dashboard.crt置于路径 /home/share/certs下，该路径会配置到下面即将要操作的 创建 dashboard用户 1kubectl create -f dashboard-user-role.yaml 获取登陆token 1kubectl describe secret/$(kubectl get secret -nkube-system |grep admin|awk &apos;&#123;print $1&#125;&apos;) -nkube-system token既然生成成功，接下来就可以打开浏览器，输入 token来登录进集群管理页面： 查看当前yum里的kubernetes版本1yum info kubernetes 12345kubectl create -f kubernetes-dashboard.yamlkubectl get pods -n kube-systemkubectl describe pod [kubernetes-dashboard-697f86d999-lpz58] -n kube-system 报错1： 1[WARNING HTTPProxy]: Connection to &quot;https://*&quot; uses proxy &quot;http://*:*&quot;. If that is not intended, adjust your proxy settings 1export no_proxy=localhost,127.0.0.1,localaddress,.localdomain.com,.localdomain.local,192.168.0.0/16,10.96.0.0/12,172.25.50.21,172.25.50.22,172.25.50.23,172.25.50.24 报错2： kubeadm init failed to pull image 由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。 123456for i in `kubeadm config images list`; do imageName=$&#123;i#k8s.gcr.io/&#125; docker pull registry.aliyuncs.com/google_containers/$imageName docker tag registry.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.aliyuncs.com/google_containers/$imageNamedone; 报错3： bridge-nf-call-iptables contents are not set to 1 报错4： shell文件报错syntax error near unexpected token ‘$’\r’’本来跑的好好得一个文件，在windows下修改了，然后移植到linux就报错了 解决方法： 123456sed &apos;s/\r//g&apos; task_start.sh &gt; task_start2.shsed &apos;s/\r//g&apos; get-k8s-dashboard.sh &gt; get-k8s-dashboard-1.sh或者dos2unix *.sh 报错5： 1[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 1echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables 报错6： 1the connection to the server localhost:8080 was refused 解决方法： 1234567did you run below commands after kubeadm initTo start using your cluster, you need to run (as a regular user):sudo cp /etc/kubernetes/admin.conf $HOME/sudo chown $(id -u):$(id -g) $HOME/admin.confexport KUBECONFIG=$HOME/admin.conf 报错7： 1Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;) 解决方案： do you have $KUBECONFIG pointing to /etc/kubernetes/kubelet.conf? 12export KUBECONFIG=/etc/kubernetes/kubelet.confkubectl get nodes 报错8： 1Error from server (Forbidden): componentstatuses is forbidden: User &quot;system:node:k8s-m&quot; cannot list resource &quot;componentstatuses&quot; in API group &quot;&quot; at the cluster scope 报错9： 1Error from server (AlreadyExists): error when creating &quot;kubernetes-dashboard.yaml&quot;: secrets &quot;kubernetes-dashboard-certs&quot; already exists 解决方法： 123kubectl create 的反向操作kubectl delete -f kubenetes-dashboard.yml 报错10： I installed a Kubernetes master using kubeadm sucessfully on a VM (esxi). The problem is that if I stop the machine and restart it the master node seems to be down: 1The connection to the server *:6443 was refused - did you specify the right host or port? 1kubectl version 12345678export KUBECONFIG=/etc/kubernetes/admin.confsystemctl restart kubelet.servicesystemctl status kubeletsystemctl daemon-reloadsystemctl restart kubelet After rebooting Master node, cannot connect server anymore 报错11： 12Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service account&apos;s configuration) or the --apiserver-host param points to a server that does not exist. Reason: Get https://10.96.0.1:443/version: dial tcp 10.96.0.1:443: i/o timeoutRefer to our FAQ and wiki pages for more information: https://github.com/kubernetes/dashboard/wiki/FAQ 源文传送门：这里 从零开始搭建Kubernetes集群（二、搭建虚拟机环境） 从零开始搭建Kubernetes集群（三、搭建K8S集群） Kubernetes集群搭建 在 Cent OS 7 上安装 docker-ce lnmp、lamp、lnmpa一键安装包 Centos7安装与卸载Docker shell文件报错syntax error near unexpected token ‘$’\r’’ How to Install Kubernetes (k8s) 1.7 on CentOS 7 / RHEL 7 深入玩转K8S之使用kubeadm安装Kubernetes v1.10以及常见问题解答 k8s指南 国内环境Kubernetes v1.12.1的安装与配置 基于阿里云镜像站安装Kubernetes 利用K8S技术栈打造个人私有云（连载之：K8S集群搭建） 使用kubeadm安装Kubernetes 1.13 利用K8S技术栈打造个人私有云（连载之：初章） 利用Kubeadm部署 Kubernetes 1.13.1集群实践录 你闺女也能看懂的插画版Kubernetes指南 华为是怎么用Kubernetes的？]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装 shadowsocks-libev启用 obfs 混淆]]></title>
    <url>%2F2019%2F03%2F19%2F%E5%AE%89%E8%A3%85-shadowsocks-libev%E5%90%AF%E7%94%A8-obfs-%E6%B7%B7%E6%B7%86%2F</url>
    <content type="text"><![CDATA[编译和安装 ss-libev 该版本的特点是内存占用小（600k左右），低 CPU 消耗，甚至可以安装在基于 OpenWRT 的路由器上。 相比原版Shadowsocks，libev(c#)版本提供了更多的特性支持；而关于ShadowsocksR(Python)， Shadowsocks-libev率先支持AEAD Cipher，并向ShadowsocksR学习引进了Obfs(Simple-Obfs)，且提供稳定的周期性更新； 长远来看，ShadowsocksR更倾向于向none加密化和Obfs化发展，而Shadowsocks-libev则倾向于向强化Cipher的方向发展，两者有不同的侧重点。 简单来说，ShadowsocksR更侧重消除特征以更好穿过GFW，而Shadowsocks-libev则追求更高的安全性.并无优劣之分 加密，以下是四种常见的AEAD Cipher，相较OTA Cipher而言AEAD Cipher极大降低了被主动探测的风险，故推荐使用AEAD Cipher 1234aes-128-gcmaes-192-gcmaes-256-gcmchacha20-ietf-poly1305 以及几种常用的OTA Cipher方式 123rc4-md5aes-128-cfbaes-128-ctr 关于AEAD Cipher的选择，需要注意的是，虽然ARM早在2015就收购了Shadowsocks-libev依赖的加密库之一mbed tls，但至今mbed tls并未对ARMv8做出实质性的优化，导致AES GCM系Cipher在较新的64位移动设备上性能低下。相关的几种Cipher测试数据如下(From Blankwonder) 开启 bbr123yum install git vim wget -yyum install epel-release -yyum install gcc gettext autoconf libtool automake make pcre-devel asciidoc xmlto c-ares-devel libev-devel libsodium-devel mbedtls-devel -y 下载 shadowsocks-libev 的源代码：123git clone https://github.com/shadowsocks/shadowsocks-libev.gitcd shadowsocks-libevgit submodule update --init --recursive 12./autogen.sh &amp;&amp; ./configure --prefix=/usr &amp;&amp; makemake install 配置 仅使用 shadowsocks-libev 12mkdir -p /etc/shadowsocks-libevvim /etc/shadowsocks-libev/config.json 12345678910/etc/shadowsocks-libev/config.json&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:自定端口号, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;自定密码&quot;, &quot;timeout&quot;:60, &quot;method&quot;:&quot;aes-256-gcm&quot;&#125; 如想要同时启用 ipv4 和 ipv6 ，则 config.json 应为 12345678910/etc/shadowsocks-libev/config.json&#123; &quot;server&quot;:[&quot;::0&quot;,&quot;0.0.0.0&quot;], &quot;server_port&quot;:自定端口号, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;自定密码&quot;, &quot;timeout&quot;:60, &quot;method&quot;:&quot;aes-256-gcm&quot;&#125; 编译和安装 simple-obfs12345678yum install zlib-devel openssl-devel -ygit clone https://github.com/shadowsocks/simple-obfs.gitcd simple-obfsgit submodule update --init --recursive./autogen.sh./configure &amp;&amp; makemake install 使用带 obfs 混淆的 shadowsocks-libev123456789101112/etc/shadowsocks-libev/config.json&#123; &quot;server&quot;:[&quot;::0&quot;,&quot;0.0.0.0&quot;], &quot;server_port&quot;:自定端口号, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;自定密码&quot;, &quot;timeout&quot;:60, &quot;method&quot;:&quot;aes-256-gcm&quot;, &quot;plugin&quot;:&quot;obfs-server&quot;, &quot;plugin_opts&quot;:&quot;obfs=tls&quot;&#125; 1234567891011&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:443, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;********&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;chacha20-ietf-poly1305&quot;, &quot;fast_open&quot;:true, &quot;plugin&quot;: &quot;obfs-server&quot;, &quot;plugin_opts&quot;: &quot;obfs=tls;obfs-host=iosapps.itunes.apple.com&quot;&#125; 123456789101112/etc/systemd/system/shadowsocks.service[Unit]Description=Shadowsocks ServerAfter=network.target[Service]ExecStart=/usr/bin/ss-server -c /etc/shadowsocks-libev/config.json -uRestart=on-abort[Install]WantedBy=multi-user.target Windows 客户端的使用方法： 下载最新 Windows 客户端 下载 obfs-local.zip 下载后，解压出来的文件一定要和 Windows 客户端的 exe 文件放在同一文件夹下 插件：obfs-local插件选项：obfs=tls插件参数：obfs-host=www.bing.com 插件选项和插件参数根据你服务端上的配置选择用 http 或者 tls，obfs-host 随便你用什么网址都行 *混淆方式，主要分为tls,http两种，相比http，tls更具隐蔽性 *混淆域名，使用混淆时伪装的域名，一般选择大型企业，流量出入较大、CDN等未被墙的域名作为混淆域名(如伪装的特定端口下的IP或域名来免流)，常用的混淆域名有这些 1234567cloudfront.comcloudflare.comitunes.apple.comwww.icloud.comajax.microsoft.comapps.bdimg.com Shadowsocks for Windows 不支持TCP Fast Open，但是simple-obfs的服务端和客户端都支持，是 –fast-open 参数。 因此，你只需要在服务端关闭shadowsocks-libev的TCP Fast Open，然后开启simple-obfs的TCP Fast Open，并在客户端的simple-obfs中启用 –fast-open 即可。 服务器端的配置文件除了原有内容，应该附加： 123456789&quot;plugin&quot;:&quot;obfs-server&quot;,&quot;plugin_opts&quot;:&quot;obfs=tls;fast-open=true&quot;## 客户端的配置文件除了原有内容，应该附加：&quot;plugin&quot;: &quot;obfs-local&quot;,&quot;plugin_opts&quot;: &quot;obfs=tls&quot;,&quot;plugin_args&quot;: &quot;obfs-host=www.bing.com;fast-open=true&quot;, 12ps -ef | grep -v grep | grep &quot;server&quot; 关闭防火墙更新12345678systemctl stop shadowsocks在 shadowsocks-libev 目录下：git pull./configuremakemake install 123456在 simple-obfs 目录下：git pull./configuremakemake install 1systemctl start shadowsocks 多用户设置 { “server”:”your_server_ip”, “local_address”: “127.0.0.1”, “local_port”:1080, “port_password”:{ “8989”:”password0”, “9001”:”password1”, “9002”:”password2”, “9003”:”password3”, “9004”:”password4” }, “timeout”:300, “method”:”aes-256-cfb”, “fast_open”: true} 优化 基于kvm架构vps的优化这方面SS给出了非常详尽的优化指南，主要有：优化内核参数，开启TCP Fast Open TCP Fast Open 主要通过第一次TCP握手后服务器产生Cookie作为后续TCP连接的认证信息，客户端通过TCP再次连接到服务器时，可以在SYN报文携带数据(RFC793)，降低了握手频率，可避免恶意攻击并大幅降低网络延迟(参考)； 123## 如果 TCPFastOpenPassive 在增长，表示接受到了fast open的tcp连接grep &apos;^TcpExt:&apos; /proc/net/netstat | cut -d &apos; &apos; -f 91-96 | column -t 1234567891011121314echo &quot;net.ipv4.tcp_fastopen = 3&quot; &gt;&gt; /etc/sysctl.conf#运行以上命令，设置IPv4下的TFO默认为开启状态sysctl -e -p#运行以上命令，应用配置#将网络拥塞队列算法设置为性能和延迟最佳的fq_codelnet.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbrnet.ipv4.tcp_fastopen = 3 TCP优化: 修改文件句柄数限制 限制用户档案的体积大小，提高系统稳定性 修改vi /etc/security/limits.conf文件，加入 12* soft nofile 512000 #用户档案警告体积大小(bytes)* hard nofile 1024000 #用户档案最大体积大小(bytes) 修改vi /etc/profile文件，加入 123ulimit -SHn 1024000然后重启服务器执行ulimit -n，查询返回1024000即可。 TCP的各种优化涉及内核控制，TCP包大小，TCP转发，连接超时等优化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950net.core.rmem_max = 12582912#设置内核接收Socket的最大长度(bytes)net.core.wmem_max = 12582912#设置内核发送Socket的最大长度(bytes)net.ipv4.tcp_rmem = 10240 87380 12582912#设置TCP Socket接收长度的最小值，预留值，最大值(bytes)net.ipv4.tcp_rmem = 10240 87380 12582912#设置TCP Socket发送长度的最小值，预留值，最大值(bytes)net.ipv4.ip_forward = 1#开启所有网络设备的IPv4流量转发，用于支持IPv4的正常访问net.ipv4.tcp_syncookies = 1#开启SYN Cookie，用于防范SYN队列溢出后可能收到的攻击net.ipv4.tcp_tw_reuse = 1#允许将等待中的Socket重新用于新的TCP连接，提高TCP性能net.ipv4.tcp_tw_recycle = 0#禁止将等待中的Socket快速回收，提高TCP的稳定性net.ipv4.tcp_fin_timeout = 30#设置客户端断开Sockets连接后TCP在FIN等待状态的实际(s)，保证性能net.ipv4.tcp_keepalive_time = 1200#设置TCP发送keepalive数据包的频率，影响TCP链接保留时间(s)，保证性能net.ipv4.tcp_mtu_probing = 1#开启TCP层的MTU主动探测，提高网络速度net.ipv4.conf.all.accept_source_route = 1net.ipv4.conf.default.accept_source_route = 1#允许接收IPv4环境下带有路由信息的数据包，保证安全性net.ipv4.conf.all.accept_redirects = 0net.ipv4.conf.default.accept_redirects = 0#拒绝接收来自IPv4的ICMP重定向消息，保证安全性net.ipv4.conf.all.send_redirects = 0net.ipv4.conf.default.send_redirects = 0net.ipv4.conf.lo.send_redirects = 0#禁止发送在IPv4下的ICMP重定向消息，保证安全性net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.lo.rp_filter = 0#关闭反向路径回溯进行源地址验证(RFC1812)，提高性能net.ipv4.icmp_echo_ignore_broadcasts = 1#忽略所有ICMP ECHO请求的广播，保证安全性net.ipv4.icmp_ignore_bogus_error_responses = 1#忽略违背RFC1122标准的伪造广播帧，保证安全性net.ipv6.conf.all.accept_source_route = 1net.ipv6.conf.default.accept_source_route = 1#允许接收IPv6环境下带有路由信息的数据包，保证安全性net.ipv6.conf.all.accept_redirects = 0net.ipv6.conf.default.accept_redirects = 0#禁止接收来自IPv6下的ICMPv6重定向消息，保证安全性net.ipv6.conf.all.autoconf = 1#开启自动设定本地连接地址，用于支持IPv6地址的正常分配net.ipv6.conf.all.forwarding = 1#开启所有网络设备的IPv6流量转发，用于支持IPv6的正常访问 安全 Web Server的伪装(使用TCP80或TCP443为端口的Shadowsocks服务器效果更佳) 可以使用nginx来构建一个网站服务器，用于隐藏Shadowsocks服务器的特征，防止Shadowsocks服务器被运营商或防火墙的主动探测发现；使用以下命令配置Apache服务器 123456789yum install epel-releaseyum install nginxsystemctl start nginx# If you are running a firewall, run the following commands to allow HTTP and HTTPS traffic:firewall-cmd --permanent --zone=public --add-service=httpfirewall-cmd --permanent --zone=public --add-service=httpsfirewall-cmd --reload 1234567Shadowsocks(R) 数据流本身就是一种无明显特征的 TCP 数据流。使用混淆实际上都是在增加 Shadowsocks(R) 数据流的特征。如果防火墙可以抓取并详细分析仿真 TLS 握手数据包的每个字段，应该很容易可以判断出服务器是否提供了 Shadowsocks(R) 服务。相比 Shadowsocks TLS 混淆，我认为ShadowsocksR TLS 混淆数据包的特征更明显。因为 ShadowsocksR TLS 混淆握手包的固定字段比较多，而且看起来还有很多错误。所以我觉得 Shadowsocks TLS 混淆比 ShadowsocksR TLS 混淆更安全。 123456789以下是我的一些使用建议：如果不混淆的 Shadowsocks(R) 流不被丢弃/限速，那么请不要使用混淆。如果不需要端口复用，那么请不要使用混淆。如果 Shadowsocks(R) 流很不幸被丢弃/限速了，请尝试更换为知名端口（如 80、443）如果非要使用混淆，那么请使用 Shadowsocks，而不要使用 ShadowsocksR，并将服务开放在 443 端口上 参考文档： CentOS 7 下编译并安装 shadowsocks-libev 并启用 obfs 混淆 如何安装和配置simple-obfs服务端 shadowsocks-libev一键安装脚本 个人分析 Shadowsocks(R) TLS 混淆的安全性 如今我这样科学上网 Centos7开启bbr加速 VPS创建需要做的一些事 如何在Linux VPS上使用Best Trace？在VPS上查看回程 Shadowsocks优化篇 linux TCP Fast Open开启和测试 shadowsocks-windows支持tcp fastopen了是吗]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于k8s安装Jenkins配置出坑记]]></title>
    <url>%2F2019%2F03%2F18%2F%E5%9F%BA%E4%BA%8Ek8s%E5%AE%89%E8%A3%85Jenkins%E9%85%8D%E7%BD%AE%E5%87%BA%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[报错1：ReadTimeoutError: HTTPSConnectionPool(host=’files.pythonhosted.org’, port=443): Read timed out. 直接使用pip安装，由于连接国外，很慢，并且经常断链，导致无法正常安装扩展包。所以使用-i参数，指向国内源。 阿里的还是挺好用的： 1https://mirrors.aliyun.com/pypi/simple/ 例如： 1pip install -i https://mirrors.aliyun.com/pypi/simple/ --upgrade pip 临时使用：可以在使用pip的时候在后面加上-i参数，指定pip源 1eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 永久修改：linux:修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下： 1234567891011mkdir ~/.piptouch ~/.pip/pip.confvim ~/.pip/pip.conf[global]timeout = 6000index-url = http://pypi.douban.com/simple/[install]use-mirrors = truemirrors = http://pypi.douban.com/simple/trutrusted-host = pypi.douban.com windows:直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，内容如下 1234567[global]timeout = 6000index-url = http://pypi.douban.com/simple/[install]use-mirrors = truemirrors = http://pypi.douban.com/simple/trutrusted-host = pypi.douban.com 更换pip源到国内镜像pip国内的一些镜像 1234567891011121314151617https://mirrors.aliyun.com/pypi/simple/中国科技大学https://pypi.mirrors.ustc.edu.cn/simple/豆瓣(douban)http://pypi.douban.com/simple/清华大学https://pypi.tuna.tsinghua.edu.cn/simple/中国科学技术大学http://pypi.mirrors.ustc.edu.cn/simple/ Alpine Linux 源使用帮助12sed -i &apos;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&apos; /etc/apk/repositoriesapk update Alpine Linux 包管理123$ apk add asterisk=1.6.0.21-r0$ apk add &apos;asterisk&lt;1.6.1&apos;$ apk add &apos;asterisk&gt;1.6.1&apos; docker编译慢是因为你没有把国内源的镜像编进去1234567FROM alpineMAINTAINER jack jackadam@sina.comRUN echo &apos;http://mirrors.aliyun.com/alpine/v3.6/community/&apos;&gt;/etc/apk/repositories &amp;&amp;echo &apos;http://mirrors.aliyun.com/alpine/v3.6/main/&apos;&gt;&gt;/etc/apk/repositories &amp;&amp;apk add --no-cache tzdata &amp;&amp;cp /usr/share/zoneinfo/Asia/Chongqing /etc/localtime &amp;&amp;rm -rf /tmp/* 参考文档： pip 国内镜像源 Alpine Linux 下的包管理工具 pillow-installation Trouble installing Django Alpine Linux 源使用帮助 Alpine Linux 配置使用技巧 How to install to python docker 如何让Python pip使用国内镜像体验飞一样的速度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#!/bin/shset -xe#PREPARE#Linux VersionSIG=`cat /etc/*release | grep ^NAME | cut -c7`mkdir -p /usr/share/jenkins/configmkdir -p ~/.pip/cat &lt;&lt; EOF &gt; ~/.pip/pip.conf[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.comEOFcat &lt;&lt; EOF &gt; /usr/share/jenkins/config/init.user.groovy#!groovyimport jenkins.model.*import hudson.security.*def user = System.getenv(&apos;ADMIN_USER&apos;)def password = System.getenv(&apos;ADMIN_PASSWORD&apos;)def instance = Jenkins.getInstance()println &quot;--&gt; creating local user&quot;def hudsonRealm = new HudsonPrivateSecurityRealm(false)hudsonRealm.createAccount(user,password)instance.setSecurityRealm(hudsonRealm)def strategy = new FullControlOnceLoggedInAuthorizationStrategy()instance.setAuthorizationStrategy(strategy)instance.save()EOF# Alpineif [ $SIG = &quot;A&quot; ]; then sed -i &apos;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&apos; /etc/apk/repositories apk update apk add --update git subversion curl ansible tar gzip bashfi# Ubuntuif [ &quot;$SIG&quot; = &quot;U&quot; ]; then export DEBIAN_FRONTEND=&quot;noninteractive&quot; apt-get update apt-get -y upgrade apt-get install -y --no-install-recommends \ apt-transport-https ca-certificates ansible \ curl git subversion unzipficurl -sSL -o /usr/share/jenkins/jenkins.war \http://mirrors.jenkins.io/war-stable/latest/jenkins.warcurl -sSL -o /usr/share/jenkins/slave.jar \https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/$&#123;SLAVE_VER&#125;/remoting-$&#123;SLAVE_VER&#125;.jarcurl -sSL -o apache-maven-$MAVEN_VER-bin.tar.gz \https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/$MAVEN_VER/apache-maven-$MAVEN_VER-bin.tar.gztar zxvf apache-maven-$MAVEN_VER-bin.tar.gzmv apache-maven-$MAVEN_VER $MAVEN_HOMErm apache-maven-$MAVEN_VER-bin.tar.gzcurl -sSL -o /usr/local/bin/kubectl \https://storage.googleapis.com/kubernetes-release/release/v$KUBECTL_VER/bin/linux/amd64/kubectlchmod a+x /usr/local/bin/kubectlcurl -sSL -o sonar-scanner-cli-$SONAR_SCANNER_VER.zip \https://bintray.com/sonarsource/SonarQube/download_file?file_path=org%2Fsonarsource%2Fscanner%2Fcli%2Fsonar-scanner-cli%2F3.1.0.1141%2Fsonar-scanner-cli-$SONAR_SCANNER_VER.zipunzip sonar-scanner-cli-$SONAR_SCANNER_VER.zipmv sonar-scanner-$SONAR_SCANNER_VER $SONAR_HOMErm sonar-scanner-cli-$SONAR_SCANNER_VER.zipcurl -c -sS -o /usr/local/bin/img \https://github.com/genuinetools/img/releases/download/v0.3.2/img-linux-amd64curl -c -sS -o /usr/local/bin/runc \https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64chmod a+x /usr/local/bin/imgchmod a+x /usr/local/bin/runcif [ $SIG = &quot;A&quot; ]; then sed -i &apos;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&apos; /etc/apk/repositories apk update apk add --update py-pip postgresql-dev gcc python-dev musl-dev apk add --no-cache bash pip install --upgrade pip pip install --default-timeout=100 futurefiif [ &quot;$SIG&quot; = &quot;U&quot; ]; then apt-get install python-pip python-setuptools libpq-dev python-dev gcc xvfb firefox \ -y --no-install-recommends pip install --upgrade pip curl -c -sS -o /tmp/geckodriver.tar.gz https://github.com/mozilla/geckodriver/releases/download/v0.11.1/geckodriver-v0.11.1-linux64.tar.gz tar xf /tmp/geckodriver.tar.gz --directory=/usr/local/bin rm /tmp/geckodriver.tar.gzfipip install --upgrade pip setuptoolspip install --no-cache-dir -v &quot;django&lt;2&quot; &quot;django-filter&lt;2&quot; djangorestframeworkpip install &quot;Pillow&lt;2.0.0&quot;#pip install -Uv wxpython -i https://pypi.tuna.tsinghua.edu.cn/simplepip install --no-cache-dir -v decorator docutils \Markdown psycopg2 PyMySQL requests robotframework \robotframework-databaselibrary robotframework-ftplibrary \robotframework-requests \robotframework-selenium2libraryln -s /usr/share/maven/bin/* /usr/local/binmkdir -p /data/jenkinsmkdir -p /data/mavenmkdir -p /data/kubemkdir -p /data/sonarif [ &quot;$SIG&quot; = &quot;A&quot; ]; then apk del --purge postgresql-dev gcc python-dev musl-dev rm -rf /var/cache/apk/* rm -rf /tmp/*.apkfiif [ &quot;$SIG&quot; = &quot;U&quot; ]; then apt-get -y purge libpq-dev python-dev gcc apt-get -y autoremove rm -rf /var/lib/apt/lists/*fi Dockerfile 最后一行CMD里面如果不加路径，会报错！ 1234567891011121314151617181920212223FROM dustise/oracle-jdk:alpine-jdk-0.8.4COPY run.sh /usr/local/binCOPY prepare.sh /usr/local/binCOPY config.xml /usr/share/jenkins/config/config.xmlCOPY install-plugins.sh /usr/local/binCOPY jenkins-support /usr/local/binENV JENKINS_HOME=&quot;/data/jenkins&quot; \ MAVEN_HOME=&quot;/usr/local/share/maven&quot; \ SONAR_HOME=&quot;/usr/local/share/sonar&quot; \ MAVEN_VER=&quot;3.5.3&quot; \ SONAR_SCANNER_VER=&quot;3.1.0.1141-linux&quot; \ KUBECTL_VER=&quot;1.13.2&quot; \ SLAVE_VER=&quot;3.20&quot; \ JENKINS_MODE=&quot;MASTER&quot; \ TIMEZONE=&quot;Asia/Shanghai&quot; \ JENKINS_UC=&quot;https://updates.jenkins-ci.org&quot; \ REF=&quot;$JENKINS_HOME/plugins&quot; \ ADMIN_USER=&quot;root&quot; \ ADMIN_PASSWORD=&quot;123456&quot;RUN prepare.shEXPOSE 8080VOLUME [&quot;/usr/share/jenkins/&quot;, &quot;/data/jenkins&quot;, &quot;/data/maven&quot;, &quot;/data/kube&quot;, &quot;/data/sonar&quot;, &quot;/data/robot&quot;]CMD [&quot;/bin/bash&quot;,&quot;/usr/local/bin/run.sh&quot;]]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker删除images问题]]></title>
    <url>%2F2019%2F03%2F13%2FDocker%E5%88%A0%E9%99%A4images%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[背景偶然间发现服务器上有很多镜像占用不少空间，想清理一下。结果直接进行删除报错 主流的方法有两种1docker rmi -f 方法二：批量删除容器，再删除镜像 12345678# 停止所有容器docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;|xargs docker stop# 删除所有容器docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;|xargs docker rm# 删除所有none镜像docker images|grep none|awk &apos;&#123;print $3 &#125;&apos;|xargs docker rmi 方法三： 查找出所有在指定 image 之后创建的 image 的父 image，本示例看得出是同一个依赖镜 1docker image inspect --format=&apos;&#123;&#123;.RepoTags&#125;&#125; &#123;&#123;.Id&#125;&#125; &#123;&#123;.Parent&#125;&#125;&apos; $(docker image ls -q --filter since=xxxxxx) 删除关联的依赖镜像，关联的none镜像也会被删除 参考文档： image has dependent child images]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端为npm和git配置shadowsocks]]></title>
    <url>%2F2019%2F03%2F06%2F%E7%BB%88%E7%AB%AF%E4%B8%BAnpm%E5%92%8Cgit%E9%85%8D%E7%BD%AEshadowsocks%2F</url>
    <content type="text"><![CDATA[通过cmd命令行执行某些命令，如果这些命令去国外站点下载什么文件，却下载不下来的时候，我们就只能科学上网了。 但是 ShadowSocks 这类工具尽管开启了全局代理，但是cmd里依旧无法下载成功。 这种全局代理只针对使用IE代理的程序才全局，不是像VPN那样的全局。当然也更不支持PAC模式了。 cmd如果要设置代理的话，需要在执行其他命令之前，先执行一下 12set http_proxy=http://127.0.0.1:1189set https_proxy=http://127.0.0.1:1189 (上面代理地址只是示例，请换成你自己的代理地址) 上面命令的作用是设置环境变量，不用担心，这种环境变量只会持续到cmd窗口关闭，不是系统环境变量。 npm全称为Node Packaged Modules。它是一个用于管理基于node.js编写的package的命令行工具。其本身就是基于node.js写的,这有点像gem与ruby的关系。Node.js 的依赖包管理生态系统 npm, 是世界上最大的生态系统开源库。 但国内使用 npm 来安装软件，速度很慢，有时候甚至直接就失败了！ 为npm配置代理方法一：C:\Users\name.npmrc 在文件中写入 12proxy = http://127.0.0.1:1087https-proxy = http://127.0.0.1:1087 然后保存退出即可! 方法二12npm config set proxy http://server:portnpm config set https-proxy http://server:port 如果代理不支持https的话需要修改npm存放package的网站地址 1npm config set registry &quot;http://registry.npmjs.org/&quot; 原文参考： Npm的配置管理及设置代理 终端下为npm和git配置Shadowsocks npm翻墙加速国内镜像 升级node.js和npm npm 更新至最新版本啊 为windows cmd设置代理]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pyenv管理python版本(精编)]]></title>
    <url>%2F2018%2F11%2F18%2F%E4%BD%BF%E7%94%A8pyenv%E7%AE%A1%E7%90%86python%E7%89%88%E6%9C%AC-%E7%B2%BE%E7%BC%96%2F</url>
    <content type="text"><![CDATA[pyenv 是 Python 版本管理工具。 pyenv 可以改变全局的 Python 版本，安装多个版本的 Python， 设置目录级别的 Python 版本，还能创建和管理 virtual python environments 。所有的设置都是用户级别的操作，不需要 sudo 命令。 pyenv 主要用来管理 Python 的版本，比如一个项目需要 Python 2.x ，一个项目需要 Python 3.x 。 ==virtualenv 主要用来管理 Python 包的依赖，不同项目需要依赖的包版本不同，则需要使用虚拟环境== pyenv 通过系统修改环境变量来实现 Python 不同版本的切换。 virtualenv 通过将 Python 包安装到一个目录来作为 Python 包虚拟环境，通过切换目录来实现不同包环境间的切换。 pyenv 的美好之处在于，它并没有使用将不同的 PATH植入不同的shell这种高耦合的工作方式，而是简单地在PATH 的最前面插入了一个垫片路径（shims）：~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin。 所有对 Python 可执行文件的查找都会首先被这个 shims 路径截获，从而使后方的系统路径失效。 1.0 手动安装 123456git clone https://github.com/pyenv/pyenv.git ~/.pyenv$ echo &apos;export PYENV_ROOT=&quot;$HOME/.pyenv&quot;&apos; &gt;&gt; ~/.bash_profile$ echo &apos;export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profileecho -e &apos;if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\n eval &quot;$(pyenv init -)&quot;\nfi&apos; &gt;&gt; ~/.bash_profileexec $SHELL 等同于 source ~/.bash_profile 2.0 pyenv 常用命令 1pyenv versions 3.0 查看可安装 Python 版本 1pyenv install -l 12$ pyenv install 2.7.3 # 安装 python$ pyenv uninstall 2.7.3 # 卸载 python 4.0 python 切换 12$ pyenv global 2.7.3 # 设置全局的 Python 版本，通过将版本号写入 ~/.pyenv/version 文件的方式。$ pyenv local 2.7.3 # 设置 Python 本地版本，通过将版本号写入当前目录下的 .python-version 文件的方式。通过这种方式设置的 Python 版本优先级较 global 高 5.0 python 优先级 1shell &gt; local &gt; global 6.0 pyenv 会从当前目录开始向上逐级查找 .python-version 文件，直到根目录为止。若找不到，就用 global 版本。 1234$ pyenv shell 2.7.3 # 设置面向 shell 的 Python 版本，通过设置当前 shell 的 PYENV_VERSION 环境变量的方式。这个版本的优先级比 local 和 global 都要高。–unset 参数可以用于取消当前 shell 设定的版本。$ pyenv shell --unset$ pyenv rehash # 创建垫片路径（为所有已安装的可执行文件创建 shims，如：~/.pyenv/versions/*/bin/*，因此，每当你增删了 Python 版本或带有可执行文件的包（如 pip）以后，都应该执行一次本命令） 7.0 pyenv-virtualenv pyenv 插件：pyenv-virtualenv pyenv virtualenv 是pyenv的插件，为pyenv设置的python版本提供隔离的虚拟环境，设置虚拟环境后，在这个目录下面安装的第三方库及修改库搜索路径都不会影响其他环境，相当于一个沙盒环境，互相不影响。 1234567git clone git://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv#重新启动shell，以使路径更改生效exec $SHELLpyenv-virtualenv会为pyenv引入一些新的命令，例如 virtualenv/virtualenv-delete用于创建/删除虚拟环境，virtualenvs用于列出所有的虚拟环境，activate /deactivate用于激活和禁用虚拟环境 8.0 创建虚拟环境 1pyenv virtualenv 3.4.3 myenv 若不指定 python 版本，会默认使用当前环境 python 版本。如果指定 Python 版本，则一定要是已经安装过的版本，否则会出错。环境的真实目录位于 ~/.pyenv/versions 下 9.0 列出当前虚拟环境 123pyenv virtualenvspyenv activate env-name # 激活虚拟环境pyenv deactivate #退出虚拟环境，回到系统环境 10 删除虚拟环境 12pyenv uninstall my-virtual-envrm -rf ~/.pyenv/versions/env-name # 或者删除其真实目录 使用 pyenv 来管理 python，使用 pyenv-virtualenv 插件来管理多版本 python 包。此时，还需注意，当我们将项目运行的 env 环境部署到生产环境时，由于我们的 python 包是依赖 python 的，需要注意生产环境的 python 版本问题 11 PyCharm PyCharm 中可以非常方便的切换 Python 环境非常方便。但是因为个人原因 Java 和 Python 同时用的概率比较高，所以一直使用 Intellij IDEA 安装了 Python 插件。PyCharm 是为 Python 单独开发，所以如果不需要在语言之间切换用 PyCharm 还是比较方便的 12 更换 pip 源 因为国内网络环境，如果在局域网内下载 pip 慢，可以尝试使用 aliyun 提供的镜像，创建 vim ~/.pip/pip.conf ，然后填入： 12345[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com 配置过程中的报错 pip升级后 1Import Error:cannot import name main 解决方案: 这个要看你用的是哪里的pip，系统的在/usr/bin/pip 如果是你自己安装的虚拟环境（pyenv、anaconda之类的）那就在对应python虚拟环境里面相应的目录下，实在找不到可以直接在虚拟环境目录下搜索pip，然后看其路径是不是在某个虚拟环境的的bin文件夹内 1less /root/.pyenv/versions/3.4.3/bin/pip 12345from pip import __main__if __name__ == &apos;__main__&apos;: sys.argv[0] = re.sub(r&apos;(-script\.pyw|\.exe)?$&apos;, &apos;&apos;, sys.argv[0]) sys.exit(__main__._main()) 更换即可！ pip升级后Import Error:cannot import name main解决方案 源文参考： Simple Python Version Management 使用 pyenv 管理 python 版本]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux配置Shadowsocks实现终端代理(精编)]]></title>
    <url>%2F2018%2F11%2F18%2Flinux%E9%85%8D%E7%BD%AEShadowsocks%E5%AE%9E%E7%8E%B0%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%90%86-%E7%B2%BE%E7%BC%96%2F</url>
    <content type="text"><![CDATA[背景 场景一： 配置PHP Laravel框架，Laravel框架需要Composer安装。结果安装Composer的时候遭遇到了GFW，光在浏览器上穿墙还不够，还要在终端上穿墙。使用Shadowsocks在浏览器上穿墙很简单，但是在终端穿墙以前没接触过，这次花了5个小时搞定了。 利用linode翻墙安装相关配置时，哪速度，真是让人怀念。所以在局域网的一台机器上面开始动手了。 场景二： 做开发的同学，应该都会经常接触终端，有些时候我们在终端会做一些网络操作，比如下载gradle包等，由于一些你懂我也懂的原因，某些网络操作不是那么理想，这时候我们就需要设置代理来自由地访问网络。 Shadowsocks是我们常用的代理工具，它使用socks5协议，而终端很多工具目前只支持http和https等协议，对socks5协议支持不够好，所以我们为终端设置shadowsocks的思路就是将socks协议转换成http协议，然后为终端设置即可。仔细想想也算是适配器模式的一种现实应用吧。 想要进行转换，需要借助工具，这里我们采用比较知名的polipo来实现。 polipo是一个轻量级的缓存web代理程序。 前题是，已经在服务器端安装好了相关应用。 Shadowsocks 是一个开源安全的 Socks5 代理，中文名称“影梭“，类似于 SSH 代理。与一度非常流行的基于 GAE 的科学上网方式相比，Shadowsocks 部署简单，使用灵活；同时与全局代理的 VPN 不同，Shadowsocks 可以仅针对浏览器代理，轻巧方便，如果说 VPN 是一把 ==屠龙宝刀==，那么 Shadowsocks 就是一把 ==瑞士军刀==，虽小巧但功能强大。 1.ubuntu or centos 安装shadowsocks运行环境安装 123sudo apt-get updatesudo apt-get install python-pipsudo apt-get install python-setuptools m2crypto 123yum install epel-release -yyum install python-pippip install --upgrade pip 接着安装shadowsocks 1pip install shadowsocks 2.启动shadowsocks安装好后，在本地我们要用到sslocal ，终端输入sslocal –help 可以查看帮助，像这样 通过帮助提示我们知道各个参数怎么配置，比如 sslocal -c 后面加上我们的json配置文件，或者像下面这样直接命令参数写上运行。 比如 12sslocal -s 11.22.33.44 -p 50003 -k &quot;123456&quot; -l 1080 -t 600 -m aes-256-cfb -s表示服务IP, -p指的是服务端的端口，-l是本地端口默认是1080, -k 是密码（要加””）, -t超时默认300,-m是加密方法默认aes-256-cfb， 为了方便我推荐直接用sslcoal -c 配置文件路径 这样的方式，简单好用。 我们可以在/home/mudao/ 下新建个文件shadowsocks.json (mudao是我在我电脑上的用户名，这里路径你自己看你的)。内容是这样：12345678&#123;&quot;server&quot;:&quot;11.22.33.44&quot;,&quot;server_port&quot;:50003,&quot;local_port&quot;:1080,&quot;password&quot;:&quot;123456&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 确定上面的配置文件没有问题，然后我们就可以在终端输入 1sslocal -c /etc/shadowsocks.json 如果想增加开启自动启动，执行： 1echo &quot; nohup sslocal -c /etc/shadowsocks.json /dev/null 2&gt;&amp;1 &amp;&quot; &gt;&gt; /etc/rc.local 查看后台sslocal是否运行 1ps aux |grep sslocal |grep -v &quot;grep&quot; 回车运行。如果没有问题的话，下面会是这样… 3.开机后台自动运行ss如果你上面可以代理上网了可以进行这一步，之前我让你不要关掉终端，因为关掉终端的时候代理就随着关闭了，之后你每次开机或者关掉终端之后，下次你再想用代理就要重新在终端输入这样的命令 sslocal -c /home/mudao/shadowsocks.json ，挺麻烦是不？ 我们现在可以在你的ubuntu上安装一个叫做supervisor的程序来管理你的sslocal启动。关于supervisor在前面介绍安装mod-ss-panel时，有介绍！ 1sudo apt-get install supervisor 安装好后我们可以在/etc/supervisor/目录下找到supervisor.conf配置文件，我们可以用以下命令来编辑 1sudo gedit /etc/supervisor/supervisor.conf 在这个文件的最后加上以下内容 1234567[program:shadowsocks]command=/usr/local/bin/sslocal -c /etc/shadowsocks.jsonautostart=trueautorestart=trueuser=rootlog_stderr=truelogfile=/var/log/shadowsocks.log 现在关掉你之前运行sslocal命令的终端，再打开终端输入 1sudo service supervisor restart 然后去打开浏览器看看可不可以继续代理上网。你也可以用 1ps -ef|grep sslocal 命令查看sslocal是否在运行。 这个时候我们需要在/etc下编辑一个叫rc.local的文件 ，让supervisor开机启动。 1sudo gedit /etc/rc.local 在这个配置文件的 1exit 0 前面一行加上 1service supervisor start 保存。 看你是否配置成功你可以在现在关机重启之后直接打开浏览器看是否代理成功。 以上原文引自这里 centos下安装supervisor suppervisor不支持python3的版本安装1234yum install python-setuptoolswget supervisor-3.3.4.tar.gzcd supervisorpython setup.py install 成功安装后可以登陆python控制台输入import supervisor 查看是否能成功加载 创建配置文件(supervisord.conf） 使用root身份创建一个全局配置文件 12echo_supervisord_conf &gt; /etc/supervisord.confsupervisord -c /etc/supervisord.conf 修改配置文件(supervisord.conf） 如果修改了 /etc/supervisord.conf ,需要执行 添加 12345[program:shadowsocks]command=/usr/local/bin/sslocal -c /etc/shadowsocks.jsonautostart=trueautorestart=truenumprocs=1 12supervisorctl reloadsupervisorctl status 来重新加载配置文件，否则不会生效 supervisord 是启动supervisorsupervisorctl 是控制supervisord 4. 终端穿墙浏览器能穿墙就已经能满足绝大多数需求了，但是有的时候终端也必须穿墙，就比如Composer。关于终端穿墙，本人尝试了很多种方案，比如Privoxy、Proxychains和Polipo，最后选择了Privoxy。 为什么终端需要单独穿墙呢？难道Shadowsock不能“全局”代理么？这个问题当时困惑了我很久，最后一句话点醒了我。 Shadowsocks是一个使用SOCKS5（或者SOCK4之类）协议的代理，它只接受SOCKS5协议的流量，不接受HTTP或者HTTPS的流量。所以当你在Chrome上能穿墙的时候，是Proxy SwitchyOmega插件把HTTP和HTTPS流量转换成了SOCKS协议的流量，才实现了Shadowsocks的代理。而终端是没有这样的协议转换的，所以没法直接使用Shadowsock进行代理。 这时候就需要一个协议转换器，这里我用了Privoxy(我用privoxy没有成功！但是用polipo成功了)。 1~$ sudo apt-get install privoxy 12345yum install install autoconf automake libtoolwget http://www.privoxy.org/sf-download-mirror/Sources/3.0.26%20%28stable%29/privoxy-3.0.26-stable-src.tar.gztar -vxf privoxy-3.0.26-stable-src.tar.gz 安装前需要执行useradd privoxy创建一个用户privoxy，然后依次执行如下三条命令 123autoheader &amp;&amp; autoconf./configuremake &amp;&amp; make install 1less /usr/local/etc/privoxy/config 123先搜索关键字:listen-address找到listen-address 127.0.0.1:8118这一句，保证这一句没有注释，8118就是将来http代理要输入的端口。然后搜索forward-socks5t,将forward-socks5t / 127.0.0.1:1080 1~$ sudo /etc/init.d/privoxy restart 接着配置一下终端的环境，需要如下两句。 123export http_proxy=&quot;127.0.0.1:8118&quot;export https_proxy=&quot;127.0.0.1:8118&quot; 为了方便还是在/etc/rc.local中添加如下命令，注意在exit 0之前。 123sudo /etc/init.d/privoxy startservice privoxy start 在/etc/profile的末尾添加如下两句。 12export http_proxy=&quot;127.0.0.1:8118&quot;export https_proxy=&quot;127.0.0.1:8118&quot; 安装privoxy的参考原文在这里 1supervisorctl status 出现如下提示，即表示成功! 过设置alias简写来简化操作，每次要用的时候输入setproxy，不用了就unsetproxy。 12345alias setproxy=&quot;export ALL_PROXY=socks5://127.0.0.1:1080&quot;alias setproxy=&quot;export http_proxy=&quot;127.0.0.1:8118&quot;&quot;alias unsetproxy=&quot;unset ALL_PROXY&quot;alias ip=&quot;curl -i http://ip.cn&quot; 怎样让Centos开机时就让supervisor启动这些配置文件呢 1vi /etc/rc.d/rc.local 12345678#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff./usr/bin/supervisord -c /etc/supervisord.conftouch /var/lock/subsys/local 在rc.local里直接写 supervisord -c /etc/supervisord.conf 还不行，还得写全supervisord所在的路径 如果想撤销当前会话的http_proxy代理，使用 1unset http_proxy 如果想要更长久的设置代理，可以将 12export http_proxy=http://localhost:8123export https_proxy=http://localhost:8123 加入.bashrc或者.bash_profile文件 引申： 6.设置Git代理复杂一些的设置Git代理 12345678git clone https://android.googlesource.com/tools/repo --config http.proxy=localhost:8123Cloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 492 KiB/s, done.Resolving deltas: 100% (1956/1956), done. 其实这样还是比较复杂，因为需要记忆的东西比较多， 下面是一个更简单的实现 首先，在.bashrc或者.bash_profile文件加入这一句。 1gp=&quot; --config http.proxy=localhost:8123&quot; 然后 执行source操作，更新当前bash配置。 更简单的使用git的方法 12345678git clone https://android.googlesource.com/tools/repo $gpCloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 483 KiB/s, done.Resolving deltas: 100% (1956/1956), done. [在git终端mac终端加入代理原文引自这里]http://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/) 7.apt-get怎么使用代理服务器升级到Ubuntu10.04后，发现apt-get的代理设置有改变了，在9.10以前使用“http_proxy”环境变量就可以令apt-get使用代理. 然后在Ubuntu10.04下就无效了，看来apt-get已经被改成不使用这个环境变量了。 一阵郁闷后，最后我发现在“首选项”-&gt;“网络代理”那里，多了个“System-wide”按钮（我用的是英文环境，不知道中文被翻译成怎样，关闭窗口时也会提示你），在这里设置后，apt-get确实可以使用代理了。 但是我依然鄙视这种改进，因为我通常就是偶尔使用代理，更新几个被墙掉的仓库而已（如dropbox和tor），根本不想使用全局代理，本来用终端就能搞定的事，现在切换代理要点N次鼠标，真烦。 所以我研究了一下，发现那个代理设置修改了两个文件，一个是“/etc/environment”，这个是系统的环境变量，里面定义了“http_proxy”等代理环境变量。另一个是“/etc/apt/apt.conf”，这个就是apt的配置，内容如下 在/etc/apt/apt.conf中追加 123Acquire::http::proxy &quot;http://127.0.0.1:8123/&quot;;Acquire::ftp::proxy &quot;ftp://127.0.0.1:8123/&quot;;Acquire::https::proxy &quot;https://127.0.0.1:8123/&quot;; 很明显的代理设置代码，我看了下apt-get的手册，发现可以用“-c”选项来指定使用配置文件，也就是复制一份为“~/apt_proxy.conf”，然后“网络代理”那里重置回直接连接，以后使用 1sudo apt-get -c ~/apt_proxy.conf update 1sudo apt-get -c ~/apt_proxy.conf install mongodb apt-get 使用代理在的原文在这里 CENTOS 使用 SUPERVISOR 管理进程 centos下安装supervisor supervisor(一)基础篇 让终端走代理的几种方法 Centos 6 Supervisor 开机启动 CentOS6 Supervisor安装 linux 配置shadowsocks代理全局代理]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redmine安装步骤精编]]></title>
    <url>%2F2018%2F10%2F25%2Fredmine%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4%E7%B2%BE%E7%BC%96%2F</url>
    <content type="text"><![CDATA[运行环境：12345678910111213mysql 5.5.56mysql:root: ere20180625dphp 7.1.7服务器（CentOS6.8）redmine3.3.3.stable Ruby2.3.3 Rails4.2.7.1 下面的安装过程，假定已经安装好了LNMP环境 安装开发工具 命令： 1yum groupinstall &quot;Development tools&quot; 清理已安装过的 命令： 1yum erase ruby ruby-libs ruby-mode ruby-rdoc ruby-irb ruby-ri ruby-docs 安装依赖 命令： 1yum -y install zlib-devel curl-devel openssl-devel httpd-devel apr-devel apr-util-devel mysql-devel 安装RVM 安装公匙 1curl -sSL https://rvm.io/mpapis.asc | gpg --import 安装rvm 1curl -L https://get.rvm.io | bash -s stable 安装ruby 刚开始用命令yum install ruby安装，但是是2.0以下的版本。 http://www.ruby-lang.org/en/downloads/下载你需要的版本，把安装包上传到指定目录，并解压 wget –no-check-certificate https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.6.tar.gz 1234567命令：tar zxvf ruby-2.3.6.tar.gz；解压后，进入ruby-2.3.6目录下，依次执行下面命令命令：./configure命令：make命令：make install 查看ruby信息 命令： 1ruby -v 安装rails 4.2 1gem install rails -v=4.2 Ruby on Rails（官方简称为Rails，亦被简称为RoR），是一个使用Ruby语言写的开源Web應用框架，它是严格按照MVC结构开发的。 它努力使自身保持简单，来使实际的应用开发时的代码更少，使用最少的配置。 1gem install rails --no-document -v=&apos;4.2.7&apos; 下载redmine,解压并进入目录.安装管理ruby的包依赖的工具bundler 1gem install bundler #注意是在网站根目录下执行 安装redmine依赖的所有ruby包 1bundle install --without development test rmagick #完成redmine依赖包的安装 为Rails生成cookies秘钥 1rake generate_secret_token 安装redmine 创建redmine数据库,添加mysql用户,配置Redmine的database.yml 123456789101112SHOW databases;CREATE DATABASE redmine;use redmine;CREATE USER &apos;redmineAdmin&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;adminedd2018062532&apos;;grant all privileges ON redmine.* TO &apos;redmineAdmin&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;adminedd2018062532&apos;;mysqldump -u redmineAdmin -p adminedd2018062532 redmine &gt; redmine_backup0627.sql 123456789cp config/database.yml.example config/database.ymlcat database.yum #修改对应数据库连接信息production: adapter: mysql2 database: redmine host: localhost username: redmine password: &quot;my_password&quot; encoding: utf8 123RAILS_ENV=production bundle exec rake db:migrate #生成表结构,创建数据库结构RAILS_ENV=production bundle exec rake redmine:load_default_data # 初始化数据选择zh,生成缺省数据 在WEBrick服务上测试Redmine是否安装成功 1bundle exec rails server webrick -e production -b xx.xxx.xx.xx 配置Redmine在Nginx上运行 安装passenger 12gem install passengerpassenger-install-nginx-module 报错 1Can&apos;t connect to local MySQL server through socket &apos;/tmp/mysql.sock&apos; (2) 解决方法 在这里 Redmine Internal error bundle exec rails server webrick -e production -b 0.0.0.0 /sbin/iptables -I INPUT -p tcp –dport 3000 -j ACCEPT find / -name php.ini tail -f production.log 报错,这个报错很奇葩，会导致浏览的时候，报500错误。 12Error: unknown attribute &apos;issues_visibility&apos; for Role. Default configuration data was not loaded 解决方法： 我做了如下处理后得到解决： 把数据库中redmine库的字符集从原来设置的utf8mb4改为utf8； 把redmine库里自动生成的表全部清空，重新生成。这里用了个批理删除方法： 再运行下面命令重新初始化redmine库。 这次成功执行。 报错调试方法 原文传送 ERROR: Failed to build gem native extension 报错 12Don&apos;t run Bundler as root. Bundler can ask for sudo if it is needed, and installing your bundle as root will break this application for allnon-root users on this machine. 解决方法： 原文传送 运行 1bundle config --global silence_root_warning 1 12345678910111213DROP USER &apos;redmineAdmin&apos;@&apos;localhost&apos;;wget http://www.redmine.org/releases/redmine-3.3.3.tar.gzsource /etc/profile.d/rvm.shrvm yum install -y gcc-c++ patch readline readline-devel zlib zlib-devel libyaml-devel libffi-devel openssl-devel make bzip2 autoconf automake libtool bison iconv-develrvmsudo yum install -y gcc-c++ patch readline readline-devel zlib zlib-devel libyaml-devel libffi-devel openssl-devel make bzip2 autoconf automake libtool bison iconv-devel 新建用户redmine mkdir -p /home/redmine useradd -d /home/redmine -m redmine useradd -d /usr/lredmine -m lredmine passwd lredmine: 2lredmine625sd@ 给予redmine用户sudo权限：不然后面执行 bundle install会报错 could not locate gemfile redmine Gem::Ext::BuildError: ERROR: Failed to build gem native extension. gem install rmagick -v ‘2.16.0’ su lredmine /root/redmine-3.3.3 bundle install –without development test rmagick 替换 undle install命令。不然会报错，通不过。 ERROR: Failed to build gem native extension 大家都在用哪些Redmine插件呢? 推荐一些自己用过，而且觉得还不错的（全部支持3.3.x 版本）： 粘贴图片插件:Clipboard image paste [Redmine Banner plugin:全局或者 project 的banner，用来通知全体人员某些信息，挺方便] (https://github.com/akiko-pusu/redmine_banner) 点击打开大图 插件安装更新命令: 12rake redmine:plugins:migrate RAILS_ENV=production 大家都在用哪些Redmine插件呢? redmine-1.2.2安装截图粘贴插件 Redmine Checklists plugin: redmine_checklists-3_1_11-light Redmine Checklists plugin (Light version) 编辑器增强 Redmine CKEditor plugin会在每个issue下面增加一个check list 超好用，当任务至最小层次的时候，这个插件很好用。 ckeditor插件，高级功能插件 Uninstall plug Change the text formatting (Administration &gt; Settings &gt; General &gt; Text formatting) to textile Rollback the migration 1rake redmine:plugins:migrate NAME=redmine_ckeditor VERSION=0 RAILS_ENV=production Delete the plugin directory (plugins/redmine_ckeditor) 12[root@li1830-25 redmine-3.3.3]# rake -T emojirake emoji # Copy emoji to the Rails `public/images/emoji` directory 1234Could not find rmagick-2.16.0 in any of the sourcesRun `bundle install` to install missing gems. 解决方法 1yum install ImageMagick 12Gems in the groups development, test and rmagick were not installed.]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>redmine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA['Veeam虚拟机备份软件-设定与执行备份程序']]></title>
    <url>%2F2018%2F09%2F05%2FVeeam%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6-%E8%AE%BE%E5%AE%9A%E4%B8%8E%E6%89%A7%E8%A1%8C%E5%A4%87%E4%BB%BD%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Veeam 是一套可以用來排程備份 vmware 與 Hyper-V 虛擬環境 guest OS 的備份軟體。 Veeam 的安裝很簡單，幾乎都是下一步大法就完成了，除非你要自己指定所用的 SQL （在最後一張圖說明），底下就來看一下，如何的進行備份 Step 1：按「Backup Job」開始設定備份程序 你可以使用一個備份程序來備份全部的 Guest OS，也可以個別備份都是一個程序 Step 2：設定此程序備份名稱 Step 3：下一步 Step 4~6：按「Add」鈕，選擇要備份的 Guest VM(s) Step 7：選擇要排除的項目在備份時，你可以將某 Guest VM 內的某些 Disk 排除不做備份Step 8~12：選擇要排除的磁碟 他的缺點是不是到你指定的 Guest VM 去讓你選要排除的磁碟，而是用全表列的方式讓你自己設定 Step 13：選擇 Backup Proxy。你可以安裝多個 Backup proxy 來做備份的分流，避免全部擁擠在某網段或是某個備份空間的寫入速度。Step 14：選擇 Backup Repository 要儲存的備份空間。雖然安裝時已經有指定那個 Storage （repository）為預設，但第一次執行設定備份程序，這個項目還是有可能是錯的，記得要確認備份目的地。Step 15：設定備份保留的期間，預設14天。Step 16：進行進階的設定 進階的設定 選擇備份的類型 Reverse Incremental Backup 先做一次完整備份，每天作差異備份，然後將差異備份合併之前的完整備份，當作今日的備份，其好處是可以反推回幾天前的狀態 Active full backup 完整備份 每次的 active full bakcup 都從 source 完整複製一份 Synthetic full backup 沒勾「Create synthetic full backups periodically」，第一次做完整備份，往後每天就做差異性備份 打勾「Create synthetic full backups periodically」 作法同上方沒打勾「再」加上定期的合併之前的備份結果，於下圖右方「Create on: Saturday」時間作合併前一週期的備份與差異備份 打勾 Transform previous backup chains into rollbacks 把某次 Synthetic full backup 時間點前的差異性備份轉換為 rollback chain，讓還原可以「倒回」前幾天 Converts previous incremental backup chain into rollbacks for newly created full backup file. 依前面所講的各種 backup mode 來選擇適合你的備份模式 備份的壓縮比率 儲存的最佳化 設定備份完成後寄送的通知這點需配合 veeam 的 option 設定 啟用file system indexing 或 application-aware processing 設定定期執行的時間這樣預設是沒打勾的，如果你要開始排程、定期的執行此備份計畫，記得這裡要打勾並且設定時間執行順序上可以依循某個備份程序之後 最後的備份計畫說明這裡按下「Finish」並不會開始備份，僅是設定完備份計畫。 手動執行備份計畫點選到備份計畫的名稱，如下圖的 DNS1 或 NTP2 ，然後按下「Start」按鈕就會開始做了。 設定 veeam 系統事件的通知信左上角選單 -&gt; Options 在 E-mail Settings 標籤設定：寄件主機、寄件者、收件者與寄件主機詳細的帳號密碼設定。 等備份完成後，就會收到類似底下的備份執行結果。 ~ End 指定不同 SQL於安裝步驟時的畫面 原文传送门： 这里]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>veeam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[puppet自动化布署配置入门]]></title>
    <url>%2F2018%2F09%2F03%2Fpuppet%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B8%83%E7%BD%B2%E9%85%8D%E7%BD%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[本实例用到系统及软件版本 Centos6.8puppet3.8.7mysql5.1.73 1. 自动化运维都有哪些开源软件1234567初始化：Kickstart、Cobbler、 Rpmbuild/Xen、Kvm、Lxc、Docker/Openstack、Cloudstack、Opennebula、Eucalyplus 配置类工具：Chef、Puppet、Func、Cfengine 命令和控制类工具: Fabric、Salstack、Ansible、Capistrano、Pssh、Dsh、Expect 监控类工具：Cacti、Nagios、Zabbix、Ganglia 什么是Puppet? puppet是一种Linux、Unix平台的==集中配置管理系统==，使用ruby语言，可管理配置文件、用户、cron任务、软件包、系统服务等。puppet把这些系统实体称之为资源，puppet的设计目标是简化对这些资源的管理以及妥善处理资源间的依赖关系 Puppet都有哪些特性? 能管理多达40多种资源，如：file、user 、group、host、packeage、service、cron、exec、yumrepo等，适合整个软件生命周期的管理 可自动化重复任务、快速部署关键性应用及本地或者云端完成主动管理变更和快速扩展架构规模等 puppet的整个生命周期 供应（provisioning:包安装）—&gt;配置（configuration）—&gt;联动(orchestration)—&gt;报告(reporting) Puppet通常可以用来管理一台主机的整个生命周期：从初始化到安装、升级、维护以及最后将服务迁移并下架。 Puppet适用于哪些场合 初始化配置、修复、升级、审计 统一安装、配置管理软件 统一配置系统优化参数 定期检测服务是否运行 快速替换集群时设备的角色 大部分配置工具，如shell或者perl脚本，是命令式或者过程式的。它们描述的是事情应该怎么做而不是所需要的最终状态应该如何。 Puppet Server默认需要2G内存 Puppet社区版和企业版本功能上有什么差别 Puppet支持哪些系统 Puppet架构是怎样的 Puppet如何工作的 123456789101112131415161718Puppet的目录结构描述如下： |— puppet.conf # 主配置配置文件|— fileserver.conf #文件服务器配置文件|— auth.conf #认证配置文件 (只允许域内认证)|— autosign.conf #自动验证配置文件|— tagmail.conf # 邮件配置文件（将错误信息发送）|— manifests # 文件存储目录(puppet会先读取该目录的.pp文件site.pp)|— nodes| | | puppetclient.pp #puppet解析主配置文件所有的模块和节点都在此文件里include| |— site.pp # 定义puppet相关的变量和默认配置| |— modules.pp # 加载class类模块文件（include nginx）|— modules # 定义模块| —nginx # 以nginx为例| |— file| |— manifests| | |— init.pp #类的定义，类名必须与模块名相同| |—– templates # 模块配置目录，可以被模块的manifests引用| | |— nginx.erb #erb模板 Puppet组织结构是怎样的 如果是虚拟机或者物理机安装，一定要注意两台服务器时间要同步。 123yum install -y ntpntpdate ntp.api.bz 如果时间不同步，会导致如下错误。 1Failed to generate additional resources using &apos;eval_generate&apos;: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [CRL is not yet valid for /CN=linux-node1.com] 在安装puppet软件以前先设置主机名，因为生成证收的时候要把主机名写入证书，如果证书生成后再改主机名，puppet客户机就连不上主机了 12vi /etc/sysconfig/networkhostname 修改/etc/hosts 添加服务器和客户端的dns解析 123vi /etc/hosts192.168.0.177 master.*.*192.168.0.186 client1.*.* 客户端，服务端均需要安装epel-release，默认centos yum包管理器中没有puppet相关包，需要安装扩张包即epel-release。 Epel是企业版Linux附加软件包(Extra Packages for Enterprise Linux)的缩写，是一个由特别兴趣小组创建、维护并管理的，针对红帽企业版Linux(RHEL)及其衍生发行版(比如CentOS、Scientific Linux)的一个高质量附加软件包项目。 1234567891011121314根据操作系统版本配置Yum源，当前系统为Centos6x64，因此选择puppetlabs-release-6-11.noarch.rpm软件包rpm -Uvh http://yum.puppetlabs.com/el/6Server/products/x86_64/puppetlabs-release-6-11.noarch.rpmyum clean all yum install puppet-server # install会自动安装puppet所依赖的包（包括facter）puppet --version # 检查版本facter --version查看puppet版本信息yum info puppet-serveryum info puppet 第一次启动服务端建议采用 1puppet master --verbose --no-daemonize 方式启动，有助于测试和调试错误，如果采用后面这种方式，你可以看到启动的整个过程，启动过程会做一些初始化的工作，为master创建本地证书认证中心，证书和key。并打开socket等待client的连接。你可以在/etc/puppet/ssl目录看到相关的文件和目录 5、修改配置文件 1vim /etc/puppet/puppet.conf 在[master]节点最后添加： 1certname = master.*.* 如果没有[master]节点，手动添加上去。 保存退出 6、设置开机启动 1chkconfig puppetmaster on 7、启动服务 1# service start puppetmaster 服务器配置完毕！ centos6.8 的selinux是默认开启的。在这种状态下面，会报错，必须关闭！这个坑耽误了我大概2天时间 Firewall The Puppet Master listens on port 8140, so configure the firewall in such way that managed nodes can connect to the master. 123查看selinux的详细状态，如果为enable则表示为开启/usr/sbin/sestatus -v 永久性关闭（这样需要重启服务器后生效） 1sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/config 在客户端运行 1234567puppet agent --server=master --verbose --no-daemonize --debugpuppet agent --testpuppet agent --no-daemonize --verbose --debug --nooppuppet agent --server=master.*.* --no-daemonize --verbose --debug --noop –no-daemonize 使得puppet客户端运行在前台并输出日志到标准输出–verbose会使客户端输出详细的日志–debug参数能提供更加详细的输出，在解决问题时候非常有用 1puppet agent --server=master.*.* --no-daemonize --verbose --debug 二、客户端安装和配置 2.1 修改hostname 1hostname 2.2 修改/etc/hosts 12192.168.0.245 master.*.*192.168.0.247 client1.*.* 2.3 安装ruby 1yum -y install ruby ruby-libs 2.4 安装puppet 1yum install -y puppet 2.5 修改配置文件 1vim /etc/puppet/puppet.conf 在[agent]节点最后添加： 1server = master.*.* 保存退出 2.6 12345678设置开机启动# chkconfig puppet on启动服务# service puppet start 检查puppet是否启动 1netstat -nap | grep 8140 如果启动了还是连不上的话，应该是防火墙的问题.开启防火墙 123-I后面没有跟数字,所以是插入第一行的.iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 8140 -j ACCEPT 客户端配置完毕！ 3.0 添加认证 3.1 客户端执行 1# puppet agent -t 3.2 服务端执行,执行下面的命令查看是否有客户端的证书请求： 1# puppet cert list 出现请求过的认证信息后执行： 1# puppet cert sign client1.*.* 到处为止puppet服务端和客户端都能正常工作了 如果看到了客户端的证书请求，用下面的命令对所有证书请求签名： 1puppet cert -s -a or we can sign all awaiting certificates at once: 1puppet cert sign --all 签发证书之后，再次测试，已经从服务端获取到了文件。 查看所有签名的请求 1puppet cert list --all 您可能想从Puppet中删除主机，或重建主机，然后将其添加回Puppet。 在这种情况下，您将要从Puppet主控中撤销主机的证书。 要做到这一点，你可以用clean动作： 4.0 在服务器如何配置给客户端自动安全的签名 在/etc/puppet/puppet.conf [main]最后添加如下内容 1autosign = true 自动颁发证书是一种比较危险的操作，除非有很充分安全的需求，一般情况下不要这样操作。 runinterval选项用于设置每隔多久的时间进行自动更新，时间单位为秒，我们选择的是600秒 即时观察下messages系统日志，命令如下： 1tail -f /var/log/messages 5.0 puppet进阶用法 12192.168.0.245 master.*.* # puppet-master192.168.0.247 client1.*.* # puppet-client puppet是利用node(节点)来区分不同的客户机端，它并且给不同的客户机端分配不同的资源。 6.0 主清单文件 Puppet使用特定于域的语言来描述系统配置，这些描述保存到名为“manifests”的文件，其具有.pp文件扩展名。 默认的主清单文件位于您的Puppet主服务器上 1/etc/puppetlabs/code/environments/production/manifests/site.pp 主清单现在是空的，因此Puppet不会在代理节点上执行任何配置 如何执行主要清单 Puppet代理定期检查Puppet服务器（通常每30分钟）。 当它检入时，它将向主机发送关于自身的事实，并且拉出当前目录由主清单确定的与代理相关的资源及其期望状态的编译列表。代理节点然后将尝试进行适当的改变以实现其期望的状态。 只要Puppet主服务器正在运行并与代理节点通信，该周期将继续。 在特定代理节点上立即执行 1puppet agent --test 7.0 一次性清单 该puppet apply命令可以执行不相关的主清单清单上的需求。 它仅适用清单到您从应用节点。 这里是一个例子： 1puppet apply /path/to/your/manifest/init.pp 以这种方式运行清单很有用，如果你想在代理节点上测试一个新的清单，或者如果你只想运行清单一次（例如，将代理节点初始化到所需的状态） 8.0 ERB模板来自动配置Nginx虚拟主机 首先建立nginx模块 puppet模板主要用于文件，例如各种服务的配置文件，相同的服务，不同的”.erb”结尾的文件。 1mkdir -p /etc/puppet/modules/nginx/&#123;files,manifests,templates&#125; 通过define(定义),定义和类一样也属于资源容器，不过它的特点是能够在一台主机上多次赋值，此外它还能接受参数. 在nginx.conf.erb中，我们提前定义了两个变量$sitedomain和$port,使用的格式是”&lt;%=变量名%&gt;” 用如下命令来检测模板是否存在语法问题: 1erb -x -T &apos;-&apos; -P /etc/puppet/modules/nginx/templates/nginx.conf.erb | ruby -c 报错1： 1Warning: The package type&apos;s allow_virtual parameter will be changing its default value from false to true in a future release 解决方法： Add this to your site.pp 12345678if versioncmp($::puppetversion,&apos;3.6.1&apos;) &gt;= 0 &#123; $allow_virtual_packages = hiera(&apos;allow_virtual_packages&apos;,false) Package &#123; allow_virtual =&gt; $allow_virtual_packages, &#125;&#125; 报错2： 1Error: Could not run: Could not create PID file: /var/run/puppet/agent.pid 解决方案 Please check to see if you already have a daemonized puppet agent running when you execute the puppet agent command again. If you do, then you will not be able to run the agent again as a daemon. You can run it like so, even when the daemonized version is running: 1puppet agent -t --verbose 清除客户端 1rm -rf /var/lib/puppet/ssl/* 清楚服务端的证书 123puppet cert clean --allpuppet cert clean client1.*.* 参考文档: 原文在这里puppet3.0的命令变化 Puppet agent: Exiting; no certificate found and waitforcert is disabled - Solution Exiting; no certificate found and waitforcert is disabled puppet证书问题 puppet常见错误总结 在 CentOS 和 RHEL 上安装 Puppet 服务器和客户端 如何在主代理安装在CentOS 7安装Puppet 4 RedHat6.5 puppet配置服务端与客户端部署配置 Puppet的简要安装部署 初认识Puppet]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>puppet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos6配置防火墙iptables规则]]></title>
    <url>%2F2018%2F09%2F02%2FCentos6%E9%85%8D%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[应用场景centos服务器需要设置iptables防火墙规则，才能使用端口访问。目前只配置INPUT，OUTPUT和FORWORD都是ACCEPT的规则。 1. 检查iptables服务状态1service iptables status 1.1 如果没有安装的话可以直接yum安装 1yum install -y iptables 1.2 启动iptables 1service iptables start 2、配置规则常用添加端口规则 123456789101112131415161718192021222324252627#允许对外请求的返回包iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT#允许icmp包通过iptables -A INPUT -p icmp --icmp-type any -j ACCEPT#允许来自于lo接口的数据包，如果没有此规则，将不能通过127.0.0.1访问本地服务iptables -A INPUT -i lo -j ACCEPT#常用端口iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 21 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 23 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 3306 -j ACCEPTiptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j ACCEPT#过滤所有非以上规则的请求iptables -P INPUT DROP#如果要添加内网ip信任（接受其所有TCP请求）#注：(**.**.**.**)为IP,下同iptables -A INPUT -p tcp -s **.**.**.** -j ACCEPT#要封停一个IP，使用下面这条命令iptables -I INPUT -s **.**.**.** -j DROP#要解封一个IP，使用下面这条命令iptables -D INPUT -s **.**.**.** -j DROP 保存重启防火墙 12/etc/init.d/iptables saveservice iptables restart 2.1 其他方法： 12345678# 打开配置文件vim /etc/sysconfig/iptables# 加入如下语句:-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT# 保存（按Esc退出编辑）:wq# 重启防火墙，修改完成service iptables restart 4. 其他常用命令（不是一定要做）12345678#允许所有入栈规则iptables -P INPUT ACCEPT#清空默认所有规则iptables -F#清空自定义的所有规则iptables -X#计数器置0iptables -Z 5. 关闭端口常用删除端口规则 123456# 查看当前端口所在行数iptables -L -n --line-number# 删除指定序列的端口（下面是删除第五条端口）iptables -D INPUT 5# 确认是否已经删除，可以重新查看列表iptables -L -n --line-number 其他方法： 123456789# 打开配置文件vim /etc/sysconfig/iptables # 直接删除对应端口的那句 ！！！！# 保存（按Esc退出编辑）:wq# 重启防火墙，修改完成service iptables restart 6. 保存规则，重启iptables服务1234567891011#保存修改/etc/init.d/iptables save#查看规则是否添加成功vim /etc/sysconfig/iptables#添加到iptables服务自启动chkconfig iptables on#重启iptables服务service iptables restart 查看正在启用的端口情况，看上面有没有你刚加上的端口 1netstat -tl 查看所有添加进去的端口（包含启用的和未启用的端口） 1/etc/init.d/iptables status 原文传送 防火墙与 NAT 服务器]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奶爸如何用VPN连接work与life]]></title>
    <url>%2F2018%2F05%2F24%2F%E5%A5%B6%E7%88%B8%E5%A6%82%E4%BD%95%E7%94%A8VPN%E8%BF%9E%E6%8E%A5work%E4%B8%8Elife%2F</url>
    <content type="text"><![CDATA[00 应用场景笔者经常喜欢带着笔记本去星巴克学习工(zhuang)作(bi)。偶尔需要访问家里的网络，并在nas上面，下载一些资料。有时候也需要访问公司的网络，码一下代码。并且将Office上面的一些内容，做一个异地容灾同步的处理，同步到home 的nas上面去。偶尔需要处理一些突发的需求时，可以在家里面完成，并兼顾的小盆友。所以有了这样一个方案。 为了解决这个问题，打算使用VPN。对于VPN以前使用最多的是PPTP这个解决方案，但是PPTP相对于openvpn来说，没有openvpn安全，而且PPTP在linux下命令行支持不是很好，稳定性也不如openvpn。 所以最后就选择openvpn来搭建VPN。如下图： 隧道协议 有三种隧道协议：PPTP、L2TP、IPSec。 L2TP是第2层隧道协议，是一种工业标准Internet隧道协议。L2TP和PPTP都使用PPP协议对数据进行封装，然后添加独特的包头，之后在在网络上进行传输。PPTP只能在两端点之间建立单一隧道，L2TP支持两端之间使用多隧道，用户可以针对不同的服务质量创建不同的隧道。L2TP可以提供隧道验证，而PPTP不可以。当L2TP或PPTP结合IPSEC共同使用的时候，可以由IPSEC提供隧道验证。 IPSEC（IP Security）是一套协议包而不是一个单独的协议。IPSEC隧道模式是封装、路由与解封的过程。隧道将原始数据包封装在新的数据包内部，新的数据包可能会有新的寻址和路由信息，从而使其能够通过网络进行传输。封装后的数据包到达目的地后，会解除封装然后获得其中的数据内容。 使用VPN就是为了解决这样问题的。在以前要实现这样的需求，需要找运营商申请帧中继的服务，帧中继就像是找他们拉一条专有的网线，然后两边通过这条专有的线进行通信。但是帧中继是不是万能的，它有优点和缺点。 优点是申请多少带宽的服务通信时就一定有多少带宽，不会受到公网的网络阻塞；缺点是很贵，且不方便，试想如果到外国各地出差，如何解决它的流动性以及距离的问题。 VPN就像是把整个因特网虚拟成一个路由器，无论出差的作者身处何处，都能通过这个“路由器”来访问内网的机器。就像是随身带了一根无长度限制、无地理位置限制的网线，随时连接到内网中。而且，现在网速不断增速，加密技术也不断进步，使用VPN已经能够很好的实现上面的需求了。 从上面的描述中，可以总结出VPN的几个用途。 client-to-site，个人对公司(家庭)，就像上面的老板访问公司(家庭)内部服务器。是客户端对vpn服务器的隧道传输。 site-to-site，公司对家庭，是两边vpn服务器之间的隧道传输。 ssl vpn是使用SSL数字证书来实现数据加密的VPN。最典型的就是openvpn。它能用于client-to-site，也能用于site-to-site。由于openvpn是C/S架构的工具，所以要求使用vpn服务的客户端安装openvpn客户端程序。 IPSEC VPN主要用于site-to-site，即公司对公司的这种场景，两边公司都布置vpn服务器，两个vpn服务器之间进行隧道传输。常见的ipsec vpn的开源软件是openswan。 对于site-to-site的互联： 使用openvpn的实现是在两边都安装openvpn服务和openvpn的客户端，要和对端通信时使用本地的客户端拨号过去; 使用openswan的实现只需要两边安装openswan服务就可以，两边的通信是服务与服务的通信。 openvpn 的所有数据通信都基于一个单一的端口(默认是1194)，默认使用UDP协议，也可以使用且建议使用TCP协议。 openvpn 的核心是虚拟网卡。安装openvpn后会在主机上多出一个网卡，可以像其他的网卡一样进行配置。这个虚拟网卡可以接收和发送数据。 ==openvpn 提供了两种虚拟网络接口==：Tun和Tap。通过它们分别可以建立三层IP隧道和虚拟2层以太网，即分别在虚拟接口上分别实现 ==数据包== (tun)格式的传输和数据帧(tap)格式的传输。传输的数据可以通过压缩算法进行数据压缩后传输。 openvpn2.0 以后的版本都能实现一个进程管理多个并发的隧道。 openvpn 的身份验证方式支持预享密钥、第三方证书、用户名密码三种方式。 应用场景： 目标1： 搭建完成之后，可以在各地星巴克(外网)上直接输入家庭 (内网)地址进行访问。 目标2： Home与office网络可以进行互访. 01 openvpn 原理openvpn 通过使用公开密钥（非对称密钥，加密解密使用不同的key，一个称为Publice key，另外一个是Private key）对数据进行加密的。这种方式称为TLS加密 openvpn使用TLS加密的工作过程是，首先VPN Sevrver端和VPN Client端要有相同的CA证书，双方通过交换证书验证双方的合法性，用于决定是否建立VPN连接。 然后使用对方的CA证书，把自己目前使用的数据加密方法加密后发送给对方，由于使用的是对方CA证书加密，所以只有对方CA证书对应的Private key才能解密该数据，这样就保证了此密钥的安全性，并且此密钥是定期改变的，对于窃听者来说，可能还没有破解出此密钥，VPN通信双方可能就已经更换密钥了。 如果是对内网的某一个某个网站进行穿透，建议使用ngrok，这里是对整个网段的穿透。 02 OpenVPN Server和Client所用的环境 OpenVpn Sever使用的服务器（CentOS 6.8, 4.15.13-x86_64, ） OpenVPN 2.4.6 x86_64-redhat-linux-gnu easy-rsa-2.3.3_master.tar.gz (默认安装的easy-rsa-3.0.3没有找到vars) OpenVpn Client放在Openwrt(Pandorabox 16.10 stable, Openwrt 17.01)路由器上运行 （可选） windows 10 nginx-1.12.1 mysql-5.5.56 php-7.1.7 03 准备工作 a.安装依赖的软件包： 123bashyum install -y lzo lzo-devel openssl openssl-devel pam pam-devel yum install -y pkcs11-helper pkcs11-helper-devel b. 确认已经安装完成： 1rpm -qa lzolzo-devel openssl openssl-devel pam pam-devel pkcs11-helper pkcs11-helper-devel 04 安装OpenVPN(服务器端)登录服务器端，输入以下命令进行openvpn的安装 1yum install -y openvpn 或者是到openvpn的官方网站 下载源码 1wget https://swupdate.openvpn.org/community/releases/openvpn-2.4.1.tar.gz 然后按照下面的命令进行安装 12345tar -zxf openvpn-2.4.1.tar.gzcd openvpn-2.4.1./configuremakemake install 安装完成openvpn之后，输入以下命令就可看到版本信息 123456openvpn --versionOpenVPN 2.4.1 x86_64-redhat-linux-gnu [Fedora EPEL patched] [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] built on Apr 3 2017library versions: OpenSSL 1.0.1e-fips 11 Feb 2013, LZO 2.06Originally developed by James YonanCopyright (C) 2002-2017 OpenVPN Technologies, Inc. &lt;sales@openvpn.net&gt;...... 还有一种方式安装，就是编译安装 使用 rpmbuild 将源码包编译成rpm包来进行安装 1rpmbuild -tb openvpn-2.2.2.tar.gz 执行这条命令以后就会正常开始编译了，编译完成以后会在 1/root/rpmbuild/RPMS/x86_64 目录下生成 openvpn-2.2.2-1.x86_64.rpm 安装包 执行rpm -ivh openvpn-2.2.2-1.x86_64.rpm 以rpm包的方式安装： 05 配置 配置证书文件0501初始化 PKI1cd /usr/share/doc/openvpn-2.2.2/easy-rsa/2.0 进入到 /usr/share/doc/openvpn-2.2.2/easy-rsa/2.0 目录下，找到 vars 证书环境文件，修改以下几行 export 定义的参数值 123456bashexportKEY_COUNTRY=&quot;CN&quot; 所在的国家export KEY_PROVINCE=&quot;BJ&quot; 所在的省份exportKEY_CITY=&quot;Hangzhou&quot; 所在的城市exportKEY_ORG=&quot;aliyun&quot; 所属的组织 export KEY_EMAIL=my@test.com 邮件地址 上述参数的值可以自定义设置，对配置无影响。 0502 生成服务端的证书：清除并删除 keys 目录下的所有 key 1234bashln -s openssl-1.0.0.cnf openssl.cnf 做个软链接到openssl-1.0.0.cnf配置文件 source ./vars./clean-all 生成 CA 证书，刚刚已经在 vars 文件中配置了默认参数值，多次回车完成就可以 ： 1./build-ca 生成服务器证书，其中 ==aliyuntest== 是自定义的名字，一直回车，到最后会有两次交互，输入 y 确认，完成后会在 keys 目录下保存了 aliyuntest.key、aliyuntest.csr 和 aliyuntest.crt 三个文件。 1./build-key-server aliyuntest 0503 创建用户秘钥与证书1./build-key aliyunuser 创建用户名为 aliyunuser 的秘钥和证书，一直回车，到最后会有两次确认，只要按y确认即可。完成后，在 keys 目录下生成 1024 位 RSA 服务器密钥 aliyunuser.key、aliyunuser.crt 和 aliyunuser.csr 三个文件。 0504 生成 Diffie Hellman参数1./build-dh 执行了./build-dh后，会在 keys 目录下生成 dh 参数文件 dh1024.pem。该文件客户端验证的时候会用到。 0505 将 /usr/share/doc/openvpn-2.2.2/easy-rsa/2.0/keys 目录下的所有文件复制到 /etc/openvpn下：1cp -a /usr/share/doc/openvpn-2.2.2/easy-rsa/2.0/keys/* /etc/openvpn/ 0506 复制 openvpn 服务端配置文件 server.conf 到 /etc/openvpn/ 目录下：1cp -a /usr/share/doc/openvpn-2.2.2/sample-config-files/server.conf /etc/openvpn/ 06 配置OpenVPN Server1cd /etc/openvpn 0601 将server配置模板复制到/etc/openvpn下1cp /usr/share/doc/openvpn-2.4.1/sample/sample-config-files/server.conf . 0602 编辑server.conf1234567891011121314151617181920212223$ egrep -v &quot;^$|^#|^;&quot; server.conflocal 1.1.1.1 此处请填写用户自己的云服务器的公网IP地址port 1194proto udpdev tunca ca.crtcert aliyuntest.crt 此处crt以及下一行的key，请填写生成服务器端证书时用户自定义的名称key aliyuntest.key dh dh1024.pemserver 172.16.0.0 255.255.255.0ifconfig-pool-persist ipp.txtpush &quot;redirect-gateway def1 bypass-dhcp&quot;push &quot;dhcp-option DNS 223.5.5.5&quot; client-to-clientkeepalive 10 120comp-lzouser nobodygroup nobodypersist-keypersist-tunstatus openvpn-status.loglog openvpn.logverb 3 07 设置 iptables (（根据实际情况）)设置前请确保 iptables 已经开启，而且 /etc/sysconfig/iptables 文件已存在。然后开启转发： 修改以下内容： 1net.ipv4.ip_forward = 1 然后使内核参数生效： 1sysctl -p 添加 iptables 规则确保服务器可以转发数据包到vps内外网： 1iptables -t nat -A POSTROUTING -s 172.16.0.0/24 -j MASQUERADE 08 设置openvpn开机启动123systemctl -f enable openvpn@server.servicesystemctl start openvpn@server.servicesystemctl restart openvpn@server.service 通过 查看 1194 端口在监听，确保 openvpn 在运行中。 1netstat -ano | grep 1194 09 下载证书总共有三个文件 123ca.crtopenwrt.crtopenwrt.key 10 Windows PC 客户端的配置010 下载 openvpn 客户端011 安装：Windows系统下安装，按照默认设置安装完成。012 将云服务器中 /etc/openvpn/ 目录下的 aliyunuser.key、aliyunuser.crt 和 aliyunuser.csr 三个文件下载到需要连接 openvpn 的 Windows 客户端上（可以使用 ftp 工具下载）。保存路径为 openvpn 软件的安装路径下的 \OpenVPN\config 目录。 014 配置 client.opvn将 openvpn 安装路径下的 \OpenVPN\sample-config\ 目录中下的 client.opvn 复制到 openvpn 安装路径下的 \OpenVPN\config 目录，然后修配置文件中的如下参数； 12345bashproto udp 去掉前面的分号，采用与服务器端相同的udp协议 remote 1.1.1.1 1194 此处将1.1.1.1修改为用户的云服务器的公网IP地址，同时将该行前面的注释分号去掉cert aliyunuser.crt key aliyunuser.key 测试效果 另外一种方案，都相同，除了客户端的设置不同外，其他都一样 #11 Linux 客户端设置(Openwrt 17.01) site-to-site，公司对家庭，是两边vpn服务器之间的隧道传输 11.1 安装openvpn以及luci web配置界面还有中文 1234opkg updateopkg install openvpn-opensslopkg install luci-app-openvpnopkg install luci-i18n-openvpn-zh-cn 11.2 设置允许转发 默认情况下，防火墙是禁止转发的。所以登录路由器的web设置界面luci, 找到网络-防火墙-一般设置，可以看到转发是拒绝的，把转发设置成接受，然后点击保存并应用。 12 测试OpenVPN(客户端)能否正常运行（此步可以省略）12.1 client.conf 预先编辑好文件client.conf，然后传到/tmp目录，文件内容如下： 123456789101112131415161718192021222324252627client;dev tapdev tun;dev-node MyTap;proto tcpproto udpremote x.x.x.x 1194 # 这里修改为你的服务器ip;remote my-server-2 1194;remote-randomresolv-retry infinitenobind;user nobody;group nobodypersist-keypersist-tun;http-proxy-retry # retry on connection failures;http-proxy [proxy server] [proxy port #];mute-replay-warningsca ca.crtcert openwrt.crtkey openwrt.keyremote-cert-tls server;tls-auth ta.key 1cipher AES-256-CBCcomp-lzoverb 3;mute 20 测试运行，如果不成功会有提示 1openvpn --config client.conf 在服务器端输入以下命令测试 123ping 10.8.0.2 -c 4ping 192.168.1.1 -c 4ping 10.20.208.12 -c 4 如果能够正常运行，说明客户端和配置都没有问题 入坑： 1netstat -tlunp 1149 openvpn没有启动起来，找不到起动的日志文件 参考文档： 如何用通俗的语言解释 VPN 中 PPTP 与 L2TP 协议的联系与区别？ 使用OpenVPN对校园网内网穿透 云服务器 ECS Linux CentOS OpenVPN 配置概述 应用openvpn 用 OpenVPN 实现站对站 VPN 服务 优化openvpn性能实现两个局域网互连 Secure Communications with OpenVPN on CentOS 6 史上坑最少的openVPN搭建 CentOS6搭建OpenVPN服务器 How to setup OpenVPN on CentOS6 openvpn中server.conf和client.conf配置文件详解]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程ssh session 的管理工具]]></title>
    <url>%2F2018%2F02%2F04%2F%E8%BF%9C%E7%A8%8Bssh-session-%E7%9A%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[安装环境 ubuntu 14.04 tmux架构图： 为什么使用分屏工具在 C、C++ 开发过程中分屏是一项非常提高效率的功能，因为我们时常需要一边写代码 一边编译、测试。在脱离 IDE 的时代，通常我们需要开多个终端：一个终端编辑文件、 一个终端编译运行程序、一个终端查看相关的 man 文档…。多个终端管理起来其实是 非常不便的，因为你需要不停的在各个终端之间切换。终端分屏工具是解决这个问题 的绝佳办法，它可以把一个终端分成多个窗口，把所有需要做的事情放到一个终端里面， 让你不用再耗费大量的时间在不同的窗口之间切换。 工作中的痛点在使用SSH的环境下，避免网络不稳定，导致工作现场的丢失。想象以下场景， 你在执行一条命令的过程中，由于网络不稳定，SSH连接断开了。这个时候，你就不知道之前 的那条命令是否执行成功。如果此时你打开了很多文件，进入了较深层次的目录，由于网络 不稳定，SSH连接断开。重新连接以后，你又不得不重新打开那些文件，进入那个深层次的 目录。如果使用了tmux，重新连接以后，就可以直接回到原来的工作环境，不但提高了工作 效率，还降低了风险，增加了安全性。 What is a terminal multiplexer?It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal. 工作场景screen/tmux 是远程ssh session的管理工具。 可以在server端帮你保存工作现场和恢复工作现场。 最典型的应用场景就是，你每天下班关机器的时候，先保存现场(session)。 然后第二天上班的时候再登录上去恢复现场(session) ，可以一下子就进入到之前的工作状态， 比如当时正使用vim编写代码编写到第N行的状态。 为什么使用tmux：tmux比screen有更多的功能，能够保持你的工作环境连续性。例如tmux解决如下的问题： 1）下班后，你需要断开ssh或关闭电脑，你的ssh连接将丢失； 2）在公司打开的ssh，在家里也需要访问 tmux的使用问题：使用putty连接到远程的机器是，需要设置windows-&gt;Translation-&gt;Remote characters set 为UTF-8, 否则tmux的windows中pane的间隔线显示有问题。 在修改tmux的conf后，需要重启tmux服务， 此时需要 / : kill-server。 在多个panes的时候拷贝粘贴是个问题，需要 / [ 进入复制模式。 tmux使用C/S模型构建，主要包括以下单元模块： server服务器。输入tmux命令时就开启了一个服务器。 session会话。一个服务器可以包含多个会话 window窗口。一个会话可以包含多个窗口。 pane面板。一个窗口可以包含多个面板。 Tmux 能做什么Tmux 的功能非常非常的强大，使用它你可以： 在终端模拟器中虚拟出多个窗口 把一个窗口分成多个区域（panel）。 快速在各个区域中进行复制黏贴操作。 配置你喜欢的快捷键。 使用脚本控制 Tmux。 实现结对编程。 Tmux（”Terminal Multiplexer”的简称）可以让我们在单个屏幕的灵活布局下开出很多终端，我们就可以协作地使用它们。举个例子，在一个面板中，我们用Vim修改一些配置文件，在另一个面板，我们使用irssi聊天，而在其余的面板，可以跟踪一些日志。然后，我们还可以打开新的窗口来升级系统，再开一个新窗口来进行服务器的ssh连接。在这些窗口面板间浏览切换和创建它们一样简单。 安装Tmux和Vim一样属于字符终端软件，不需要任何GUI的支持，在远程登录时尤其有用。 screen，它是GNU软件，而Tmux是BSD的协议。 它们最主要的区别是Tmux支持Vi/Emacs风格的键盘映射，更好的接口和文档，以及更好的脚本控制。所以建议使用Tmux！ 123apt-get install autogen automake libevent-dev libncurses5-dev pkg-configsudo apt-get install tmux 编译安装 12345$ git clone https://github.com/tmux/tmux.git$ cd tmux$ sh autogen.sh$ ./configure &amp;&amp; makesudo make install 2.还有一种方法去官网下载最新版,编译安装 123456789解压之后 $ cd tmux $ ./configure &amp;&amp; make sudo make install export PATH=$PATH:/usr/local/bin/tmux source ~/.bashrc #生效，而不用重新登录 查看 echo $PATH命令查看PATH的值 PATH说简单点就是一个字符串变量，当输入命令的时候LINUX会去查找PATH里面记录的路径。比如在根目录/下可以输入命令ls,在/usr目录下也可以输入ls,但其实ls这个命令根本不在这个两个目录下，事实上当你输入命令的时候LINUX会去/bin,/usr/bin,/sbin等目录下面去找你此时输入的命令，而PATH的值恰恰就是/bin:/sbin:/usr/bin:……。其中的冒号使目录与目录之间隔开。 关于新增自定义路径：现在假设你新安装了一个命令在/usr/locar/new/bin下面，而你又想像ls一样在任何地方都使用这个命令，你就需要修改环境变量PATH了，准确的说就是给PATH增加一个值/usr/locar/new/bin。你只需要一行bash命令export PATH=$PATH:/usr/locar/new/bin。这条命令的意思太清楚不过了，使PATH自增:/usr/locar/new/bin,既PATH=PATH+”:/usr/locar/new/bin”;通常的做法是把这行bash命令写到/root/.bashrc的末尾，然后当你重新登陆LINUX的时候（应该是linux启动时就会执行这个文件），新的默认路径就添加进去了。当然这里你直接用source /root/.bashrc执行这个文件重新登陆了。你可以用echo $PATH命令查看PATH的值。 关于删除自定义路径：当某天你发现你新增的路径/usr/locar/new/bin已经没用了的话，你可以修改/root/.bashrc文件里面你新增的路径。或者你可以修改/etc/profile文件删除你不需要的路径使用Tmux的最好方式是使用会话的方式，这样你就可以以你想要的方式，将任务和应用组织到不同的会话中。如果你想改变一个会话，会话里面的任何工作都无须停止或者杀掉。让我们来看看这是怎么工作的。 关于PATH作用的详解，请参考这里 基本使用安装好后就可以启用一个Tmux Session了：（通过 tmux new -s myname 可以指定Session名） 1tmux new -s myname 在Tmux Session中，通过$可以重命名当前Session。其中指的是tmux的前缀键，所有tmux快捷键都需要先按前缀键。 它的默认值是Ctrl+b。 c可以创建新的窗口（Window）， %水平分割窗口（形成两个Pane），“垂直分割窗口。退出当前Session的快捷键是d。 然后在Bash中可以查看当前的tmux服务中有哪些Session： 1tmux ls 然后根据Session的名字可以再回去： 1tmux a -t myname (or at, or attach) 基本配置默认的是Ctrl+b，如果你觉得不好按可以调整为Ctrl+a，只需要在配置文件~/.tmux.conf中加入(tmux的配置文件是 ~/.tmux.conf，这个文件可能不存在，你可以自己新建。) 12unbind ^bset -g prefix &apos;C-a&apos; 为了能让Tmux动态载入配置而不是重启，我们设一个快捷键r来重新载入配置： 1bind r source-file ~/.tmux.conf \; display-message &quot;Config reloaded&quot; 注意，通过r重新载入配置并不等同于重启，只是增量地执行了配置文件中的所有命令而已。如果配置未生效，可以通过tmux kill-server来强行关闭Tmux。 如果你想知道当前tmux的设置，可通过tmux show -g来查看（该命令需要tmux正在运行）。 你可能会需要把这些设置导出为文件： 1tmux show -g &gt;&gt; current.tmux.conf 窗格切换可以把hjkl设置为切换窗格的快捷键： 123456789101112bind h select-pane -Lbind j select-pane -Dbind k select-pane -Ubind l select-pane -R``` &gt; 为什么使用 hjkl 键 这种操作方式看起来可能很别扭，不过如果你能够熟练地盲打，vim 和 tmux （配置成 vim 键风格）可以很容易让手指远离鼠标而只保持在键盘主键区（home row）进行操作。（译者注：home row 指的是键盘上的 “A、S、D、F、J、K、L、;” 这 8 个按键。） 这正是 hjkl 键的秘密：对于哪些盲打正确率高的人而言。 对于那些不习惯使用这些按键的人，可以先慢慢尝试几天。并先专注于打字的正确性，充分利用好你的十个手指。使用 hjkl 键的道理让我想起说服游戏初学者去使用 wasd 键而不使用方向键情况。 起初 wasd 的确会觉得不太直观，但这使得同时使用键盘以及鼠标操作变得更加容易。当适应这种操作方式之后，其优点是显而易见的。再给调整窗格大小设置快捷键： bind L resize-pane -L 10 # 向左扩展bind R resize-pane -R 10 # 向右扩展bind K resize-pane -U 5 # 向上扩展bind J resize-pane -D 5 # 向下扩展1我们发现当打开新窗格时Shell仍然在Home目录，可以设置为当前目录： bind ‘“‘ split-window -c ‘#{pane_current_path}’bind ‘%’ split-window -h -c ‘#{pane_current_path}’123456789101112131415161718## 会话- C-x s 以菜单的方式查看并选择会话- C-x :new-session 新建一个会话- C-x d 退出并保存会话- 终端运行 tmux attach 返回会话## 命名会话- tmux new -s session- tmux new -s session -d #在后台建立会话- tmux ls #列出会话- tmux attach -t session #进入某个会话## 分离会话（detach）之前已经说过，退出 tmux 可以使用 exit 命令或者 [Ctrl+d]12345678910111213141516组合键，退出 tmux 会把会话结束掉，就像平常关闭终端程序一样。但是在实际应用中，可能你并不希望这样，因为有些程序是要保持运行的，例如 rails 的测试服务、telnet连接远程服务器等等。 这时候分离会话就可以派上用场了，分离后的会话并不会把运行中的程序结束掉，而是会保持运行，你还可以稍后重新连接上这些会话。 ## 持久保存 Tmux 会话(sessio)如果机器重启，那么 Tmux会话就消失了，包括打开的各个窗口、窗格布局、以及其中跑的程序等所有东东。虽然已经有了一些工具可以简化 Tmux的会话创建过程，甚至我也写了脚本来做这方面的事情，但是毕竟我们使用 Tmux 会话是一个动态的过程，利用这些工具很难让消失的会话精确还原。要是能够把 Tmux 会话备份起来，那么恢复就容易多了。Tmux Resurrect 和 Tmux Continuum 这两个 Tmux 插件正是因此而生的。需要tmux所使用的版 本在1.9以上（tmux -V）[安装步骤在这里](https://linuxtoy.org/archives/tmux-resurrect-and-continuum.html)## tmux进阶1.细抠Session操作- 我们为前端开发环境和后端开发环境分别创建两个Session来独立管理，那么我们就可以灵活地在两个Session间穿梭，并且可以分别和前端、后端开发人员协同工作，下面我们看看相关的命令吧。 $ tmux 或 :new, 创建匿名Session$ tmux new -s mysession 或 :new -s mysession, 创建名为mysession的Session$ tmux kill-session -t mysession,关闭mysession会话$ tmux kill-session -a,关闭所有会话$ tmux ls,显示所有会话信息$ tmux a,附加到最近一个会话$ tmux a -t mysession,附加到会话mysession 123 2.多个panes输入同步 + :setw synchronize-panes12345678910- 这个功能在通过ssh维护多台服务器时十分有用## 拷贝在Tmux中通过[进入拷贝模式，按下&lt;space&gt;开始拷贝。然后用Vim/Emacs快捷键选择文本，按下&lt;Enter&gt;拷贝所选内容。然后通过]进行粘贴。&gt; 上述所有快捷键中，只有[和]需要先按下&lt;prefix&gt;。我们可以让上述拷贝快捷键符合Vi风格： bind Escape copy-modebind -t vi-copy v begin-selectionbind -t vi-copy y copy-selectionunbind pbind p pastebsetw -g mode-keys vi # Vi风格选择文本12345678这样，按下&lt;Escape&gt;进入拷贝模式，v进行选择，y拷贝所选内容，p进行粘贴。[原文在这里](http://harttle.com/2015/11/06/tmux-startup.html)## 快捷键Sessions :new new sessions list sessions$ name session123&gt; :new -s &lt;session-name&gt;可以指定新Session的名字Windows (tabs) c create windoww list windowsn next windowp previous windowf find window, name window&amp; kill window1Panes (splits) % vertical split“ horizontal split o swap panesq show pane numbersx kill pane break pane into window (e.g. to select text by mouse to copy) restore pane from window⍽ space - toggle between layouts q (Show pane numbers, when the numbers show up type the key to goto that pane) { (Move the current pane left) } (Move the current pane right) z toggle pane zoom1Misc d detacht big clock? list shortcuts: promp12345678910111213141516171819202122[分屏利器 Tmux](http://blog.guorongfei.com/2015/08/15/tmux-tutorial/)[如何使用Tmux提高终端环境下的效率](https://linux.cn/article-3952-1.html)[tmux+ubuntu 64 安装](http://www.jianshu.com/p/03971b5ed5da)[tmux的使用方法和个性化配置](http://mingxinglai.com/cn/2012/09/tmux/)[优雅地使用命令行：Tmux 终端复用](http://harttle.com/2015/11/06/tmux-startup.html)[使用tmux替代screen](http://www.opstool.com/article/253)[用 Tmux 和 Vim 打造 IDE](http://www.linuxidc.com/Linux/2015-06/119165.htm)[tmux入门 ： 3. 会话](http://www.tuicool.com/articles/jmeaY3)[为什么使用tmux](http://www.cnblogs.com/itech/archive/2012/12/17/2822170.html)## 常用按键这里需要说明一点的是，tmux的任何指令，都包含一个前缀，也就是说，你按了前缀(一组按键， 默认是Ctrl+b)以后，系统才知道你接下来的指令是发送给tmux的。 C-b ? 显示快捷键帮助C-b C-o 调换窗口位置，类似与vim 里的C-wC-b 空格键 采用下一个内置布局C-b ! 把当前窗口变为新窗口C-b “ 横向分隔窗口C-b % 纵向分隔窗口C-b q 显示分隔窗口的编号C-b o 跳到下一个分隔窗口C-b 上下键 上一个及下一个分隔窗口C-b C-方向键 调整分隔窗口大小C-b c 创建新窗口C-b 0~9 选择几号窗口C-b c 创建新窗口C-b n 选择下一个窗口C-b l 切换到最后使用的窗口C-b p 选择前一个窗口C-b w 以菜单方式显示及选择窗口C-b t 显示时钟C-b ; 切换到最后一个使用的面板C-b x 关闭面板C-b &amp; 关闭窗口C-b s 以菜单方式显示和选择会话C-b d 退出tumx，并保存当前会话，这时，tmux仍在后台运行，可以通过tmux attach进入 到指定的会话 12345678## 报错 protocol version mismatch (client 8, server 6) when trying to upgrade解决方法：[参考文档](http://unix.stackexchange.com/questions/122238/protocol-version-mismatch-client-8-server-6-when-trying-to-upgrade)&gt; This basically tells you, that you already have an (old) tmux-server running and the new tmux can&apos;t connect to it because they don&apos;t understand each other anymore. Exit all your existing tmux sessions and start a fresh one using the new version and everything should be fine. $ tmux attachprotocol version mismatch (client 7, server 6) $ pgrep tmux3429$ /proc/3429/exe attach $ kill -9 3429 然后就可以了 123## 补充 （2018-6.24德国对阵瑞典之战） 安装环境 CentOS release 6.8 (Final) 需要安装 libevent-2.0.22-stable.tar.gz tmux的安装版本 tmux-2.2.tar.gz 1234567891011121314151617yum install gcc kernel-devel make ncurses-develwget https://github.com/libevent/libevent/releases/download/release-2.0.22-stable/libevent-2.0.22-stable.tar.gztar -xf libevent-2.0.22-stable.tar.gzcd libevent-2.0.22-stable./configure --prefix=/usr/localmakemake installcd ..wget https://github.com/tmux/tmux/releases/download/2.2/tmux-2.2.tar.gztar -xf tmux-2.2.tar.gzcd tmux-2.2LDFLAGS=&quot;-L/usr/local/lib -Wl,-rpath=/usr/local/lib&quot; ./configure --prefix=/usr/localmakemake install 原文传送门]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[N2N组网 实现家里访与公司网络互访(精编版)]]></title>
    <url>%2F2018%2F02%2F03%2FN2N%E7%BB%84%E7%BD%91-%E5%AE%9E%E7%8E%B0%E5%AE%B6%E9%87%8C%E8%AE%BF%E4%B8%8E%E5%85%AC%E5%8F%B8%E7%BD%91%E7%BB%9C%E4%BA%92%E8%AE%BF-%E7%B2%BE%E7%BC%96%E7%89%88%2F</url>
    <content type="text"><![CDATA[N2N 是什么？N2N 是指通过 UDP 方式，建立虚拟局域网的一种轻量级 VPN，是全端口开放的 VPN。所以，如果某个网络禁用了 UDP，那么该网络下的设备就不适合使用本软件来加入这个虚拟局域网（用”blue’s port scanner”，选择UDP来扫描，扫出来的就是未被封的，正常情况下应该超级多）。 N2N 目前有几个版本？目前有三个版本，他们互不相容，必须分开独立使用。每一个版本都有两个主程序，一个是 edge （客户端），一个是 supernode（服务器端）。edge 和 supernode 必须成对使用，不能用 v1 的 edge 连接 v2 的 supernode。 推荐你使用官方新版 V2（2019-6-10 以后的）：速度最快、直连率高，最重要的是，官方正在更新，你也可以参与进来，帮助完善 123V1 ： https://github.com/meyerd/n2n/tree/master/n2n_v1 （版本号是 v.1.x.x）（使用 edge -h 可见，下同）V2 ： https://github.com/ntop/n2n ---------------------------------（版本号是 v.2.x.x）V2s： https://github.com/meyerd/n2n/tree/master/n2n_v2 （版本号是 v.2.1.0；V2s 是我们为便于区分，给它取的别名，它其实是一个德国网友写的 n2n v2 分支，非官方版本。曾经在三个版本中，它的直连率和稳定性是最高的） 最新N2N 最新版的安装方式123456sudo apt-get install subversion build-essential libssl-dev autoconfgit clone https://github.com/ntop/n2n.gitcd n2n./autogenmakesudo make install 建立 N2N 网络的前提条件？需要在所有的设备上运行一个 n2n 的客户端 edge（点对网、网对网的除外），并需要一个服务端 supernode（即中心节点）的帮助。 这个服务端 supernode 既可以是公用的（一个 supernode 可以同时支持很多 edge，同时支持多个虚拟局域网），也可以是自建的，但它必须要有一个外网 IP 以及对外开放的固定端口（别人通过这个 IP 和端口可以访问到它提供的服务）。而且所在的网络环境没有封 UDP 通讯（这个基本都没有封）。 n2n与openvpn对比 N2N可以用公众服务器， 所以可以不用外网IP 不用端口映射, OpenVPN服务器必需要一个外网端口 优点：不需要外网ip、不需要制做证书、设置简单 缺点：不支持tcp、不支持代理、不能从服务端推送路由表、安全性没openvpn好 n2n最大的优势在于： 开源，任何人都可以检查代码，看看是否有猫腻，而hamachi是闭源的，LogMeIn是否会截获密钥不得而知，一旦截获密钥，就可以对流经 hamachi服务器的数据包进行解码。 n2n的加解密过程由edge node实现，只有两端的用户知道协商好的共享密钥，super node无从知晓。 灵活性，n2n允许用户在Internet上自行创建super node，也可以利用任何一个公开的super node。 对于一个VPN而言，主要涉及封装和加解密两个步骤， edge node使用UDP协议进行封装，目的是为了更好的兼容防火墙的策略，因为很多防火墙禁用了非TCP/UDP协议禁用。加密算法则采用了twofish，好处开源、简便，处理速度快。 为了降低设计难度，n2n利用了tap/tun虚拟网卡，这样做得好处是一方面软件尺寸极小，一方面源码的依赖性极低，可以很容易移植到嵌入式设备中，目前有openwrt的版本，在未来的计划中，还将移植到Android和iPhone中。 有NAT和穿越防火墙的功能，即使n2n节点位于私网中，也能够访问，防火墙不再是在IP层的直接沟通和交流的障碍 如果两边都是内网，有个公网vps，可以使用ssh端口转发需要的端口给公网vps，再使用vps的ip:port远程使用但是有个缺陷是这始终是服务器中转连接，既然有公网vps，做成是n2n vpn，两边双内网直接连接，速度完爆vps中转，毕竟一个是中转，一个是直连。 有公网vps运行n2nvpn的supernode就成为超级节点，其它的电脑运行edge连接supernode成为边缘节点，边缘节点之间互相通信是靠超级节点帮助udp打洞来达到稳定直连 超级节点supernode 其实就是打洞节点 用户也可自己在vps上自己起一个 这样就可以把家里公司里的各种设备组建成一个私有环互相联通。 比较了n2n与openvpn的功能，n2n方便配置，使用简单。在同样的vps下面配置n2n和openvpn，测试的速度基本一样，也就是openvpn并不会比n2n的速度快。通过ping来测试，本人移动网络无公网ip，两者的速度均为到vps速度的2倍，也就是数据经过n2n和vpn中转后到的pc上面。 n2n国内的服务器速度较好，可以自己租用vps来搭建，可用的公共IP太少，好多公共IP都失效，要是大家能够一起搭建一服务器，那真是比较划算。 n2n是一个基于P2P协议的加密的二层专用网。加密使用开放协议部署在边缘节点，它使用用户定义的加密钥匙，由你自己控制安。各个n2n用户可以同时属于多个网络（或者社区）。它拥有在反向通信方向（如，从外部到内部）穿越NAT和防火墙的能力，因此可以到达n2n节点，即使运行在一个专用网中。防火墙不再是IP层面掌控通信的障碍。 n2n网络并不意味着它是独立的，它可以在n2n和非n2n网络间路由通信。 N2N的实现具有两个部分：supernode中心节点和edge边界节点， 边界节点通过中心节点找到对方，边界节点之间建立通信后，可以直接断开中心节点，实现点对点的加密通信。 简单说N2N就像QQ内网传文件的原理，即可以通过QQ服务器握手后直接对传，也可以通过QQ服务器中转传。 n2n设计的初衷，是为了建立连接以后，不再依靠中心节点的帮助，让连接的两端直接通讯，这本是一个很好的想法（然而，实际情况是，还是需要部分数据走中心节点，中心节点不可断）。而且，在我们国度大多数网络并不能很好工作，必须全部数据通过中心节点转发。 一个传统的 VPN（如 OpenVPN、PPTP）由一个 VPN 服务器和一个或多个连接到这台服务器的客户端组成。当任意两个 VPN 客户端彼此通信时，VPN 服务器需要中继它们之间的 VPN 数据流量。这样一个中心辐射型的 VPN 拓扑结构存在的问题是，当连接的客户端增多以后，VPN 服务器很容易成为一个性能上的瓶颈。 从某种意义上来说，中心化的 VPN 服务器也同样成为一个单点故障的来源，也就是当 VPN 服务器出现故障的时候，整个 VPN 都将无法被任何 VPN 客户端访问。 点对点 VPN（又称 P2P VPN）是另一个 VPN 模型，它能解决传统的基于服务器-客户端模型的 VPN 存在的这些问题。 一个 P2P VPN 中不再有一个中心的 VPN 服务器，任何拥有一个公开 IP 地址的节点都能引导其他节点进入 VPN。 当连接到一个 VPN 之后，每一个节点都能与 VPN 中的任何其他节点直接通信，而不需要经过一个中间的服务器节点。 当然任何节点出现故障时，VPN 中的剩余节点不会受到影响。节点中的延迟、带宽以及 VPN 扩展性在这样的设定中都有自然的提升，当你想要使用 VPN 进行多人游戏或者与许多朋友分享文件时，这都是十分理想的。 开源的 P2P VPN 实现已经有几个了，比如 Tinc、peerVPN，以及 n2n。 在本教程中，我将会展示如何在 Linux 上用 n2n 配置点对点 VPN。 n2n 是一个开源（GPLv3）软件，它允许你在用户间构建一个加密的 2/3 层点对点 VPN。 由 n2n 构建的 VPN 是“对 NAT 友好”的，也就是说，不同 NAT 路由器后方的两个用户可以通过 VPN 直接与对方通信。n2n 支持对称的 NAT 类型，这是 NAT 中限制最多的一种。因此，n2n 的 VPN 数据流量是用 UDP 封装的。较别的系统，n2n 实现的是终端到终端的加密通讯（端对端）. 假如想实现远程访问，访问家里的路由、电脑，等等设备，最简单的方式就是采用端口映射（端口转发），但是很多情况下我们没有路由的权限，这样根本就没有办法访问私网，因为受网关的保护。 本想通过sock请求来发送的，但是实现起来必须是内网主动发起，不能外网主动发起，可以保持长连接来通信，但是还是有很多的局限性。放弃采用。考虑过VPN，但是VPN配置还是比较复杂，不能做到随意使用，放弃。 这时N2N进入了我的视野，其实就是P2P协议，及点对点协议，实现UDP打洞。 n2n 是个很好玩的东西，假如你在家里面也有几台类似 nas 之类的东西，你在学校也有路由器之类的东西，那么你就可以将他们组成一个内网了，无论在哪里都可以访问到这几台设备了。只需要你在每一天设备上都设置好 edge ，部署在同一个 Supernode 上，并且设置相同的密钥和community就好了。 于我将要把一台树莓派放在学校内网挂校内的 PT ，并且这台树莓派又是通过路由器接入校园网的，那这样问题就来了。 假如我不在宿舍，但是我又想连上我的树莓派看看电影下载完了没有，那么我是没有办法连接上我的树莓派的。 如果用路由器进行端口映射的话，那么我必须在校园网的网络环境之中，并且我得知道我路由器的 IP ，由于校园网使用 DHCP 动态获取IP地址，因此我很难知道我路由器的IP。 如果我在校外的话，那么就基本上没有什么方法可以访问到我的树莓派了。 经过一轮折腾，我发现了 n2n 这个工具n2n 是一个开放源代码的2层跨越3层的VPN程序，该程序利用了点对点的架构来处理网络间的成员关系和路由。 不像大多数 VPN 程序那样， n2n 可以连接位于 NAT 路由器后面的计算机。这些连接在双方计算机都能连接的第三方计算机的帮助下建立起来。这台第三方的计算机，我们称之为supernode，他可以为 NAT 的计算机之间传输信息。 这是一个免费的开源软件，以GNU General Public License v3协议开源。 NAT(Network Address Translation) 模式（对外部网络单向可见） 外部网络对虚拟系统的网卡是可见的；虚拟系统的网卡对外部网络是不可见的。换句话说，NAT 模式可以起到【单向】防火墙的效果。这种模式用得最多。 NAT就是在局域网内部网络中使用内部地址，而当内部节点要与外部网络进行通讯时，就在网关（可以理解为出口，打个比方就像院子的门一样）处，将内部地址替换成公用地址，从而在外部公网（internet）上正常使用，NAT可以使多台计算机共享Internet连接，这一功能很好地解决了公共IP地址紧缺的问题。 NAT屏蔽了内部网络，所有内部网计算机对于公共网络来说是不可见的，而内部网计算机用户通常不会意识到NAT的存在。 N2N网络的架构图如下： 通讯信息流模型图示（多个局间通讯）如上图 N2N在需要操作系统设立虚拟网卡支持（如上图）商业版的hamachi SoftEther/VNN原理。用户需要登录到hamachi的服务器进行注册才能建立隧道，由于hamachi是一款闭源的商业软件，LogMeIn公司是否会截获用户的数据流就不得而知了，因此，在p2p VPN这个肥沃的市场中，软件公司要真的要好好想想商务模式，否则技术实现方案再好，用户不接受也是白搭。后面诞生的n2n就与hamachi有些类似.只不过，它是开源的. 第一部分：服务端即supernode端：准备工作：安装supernode与edge应用 源码直接从SVN上获取，里面包含了V1和V2两个版本，我编译的是V2版本 1234#svn co https://svn.ntop.org/svn/ntop/trunk/n2n#cd # cd n2n/n2n_v1#make#make install 编译安装后，多了两个应用：supernode和edge 12inet addr:112.74.108.206supernode -l 1000 1supernode -l 2051 -v &gt;/dev/null &amp; 参数-l是监听的端口，进程缺省是运行在后台的。 -v 参数输出详情的 服务器端命令是比较简单的, 但如果想要获得服务运行的信息, v1和v2是不同的, v2 中只有加入-f选项才能获得运行信息. 以下是v2版本, 创建一个可执行sh文件来背景运行supernode. 123#! /bin/bash/usr/bin/nohup /home/user/n2n/n2n_v2/build/supernode -l 1000 -v -f &gt; /home/user/supernode.log 2&gt;&amp;1 &amp; 1/usr/bin/nohup（不挂断的运行命令） 设置好这一步之后，要把端口加入到防火墙中 1iptables -A INPUT -p tcp -m tcp --dport 1050 -j ACCEPT 12-A INPUT -p tcp -m tcp --dport 1000 -j ACCEPT-A INPUT -p tcp -m tcp --dport 1050 -j ACCEPT 经过这两步操作就完成了supernode的操作 123456789-l Set UDP main listen port to-f Run in foreground.-v Increase verbosity. Can be used multiple times.-h This help message.root@iZ94r15fuj4Z:~/n2n# supernode -l 100009/Nov/2016 15:23:49 [supernode.c: 477]Supernode ready: listening on port 30037 [TCP/UDP] 运行 supernode 的时候可以带上-v 参数，来查看报错情况。 公司目前的客户端路由器为 NETGEAR-JNR3210(不技持openwrt) 3.配置Android的N2N客户端 配置完成后启动，可以看到和超级节点的注册信息，这时候手机也可以访问N2N网络的其他设备了，比如使用移动网络访问家里的路由器： 第二部分：Edge段12公司网段 172.25.0.0n2n IP:10.0.3.1 12家里网段 192.168.1.0 （注：公司与家里网段不能相同）n2n IP:10.0.3.2 也就是从家里访问172.25.0.0网络（从家里访问公司网络） 12345ipaddr 本地虚拟网卡的地址upsernode 超级节点的地址，可以是域名port 想要使用连接的端口community 对应的网络名称，同一个名称下才能互访 OfficeHomekey 用户定义的需要访问这边SSH使用的key Edge节点：用户PC机上安装的用于建立n2n网络的软件。几乎每个edge节点都会建立一个tun/tap设备，作为接入n2n网络的入口。 Supernode超级节点：它在edge节点间建立握手，或为位于防火墙之后的节点中转数据。它的基础作用是注册节点的网络路径，并为不能直通的节点做路由，能够直通的节点间通信，是P2P的。 ps:更改 community与key之后，客户端也要做相应的修改！同时路由器端的静态路由也要重新保存并应用刷新。否则的话通过局域网的ip无法访问。 重启（或关机重新开机）本地路由器后，与远程的局域网就断开了，解决办法， 进入管理页面的“网络-接口”，将n2n网络重新连接!如果没有这一步的话，也不能通过局域网的ip直连！ n2n设置 88.86.108.50 82是一个公用服务器（supernode），使用公用服务器的好处是可以不用有外网ip分组名、密钥随便填,但要与家里的n2n设置一样 网络设置 添加一个接口 n2n 公司openwrt路由器N2N设置 注意 以太网适配器 “edge0” 防火墙设置 如果没有防火墙这一步的设置的话，会出现一个问题。能远程访问路由，就是通过自己设置的n2n ip 10.30.168.1网关访问路由，但是不能访问10.68.0.0 网段的ip，但是ping不通，网关也ping不通？ 原因及解决方法：虚拟IP与真实IP是独立的，互相不能访问的，需要设置iptables. 添加zone n2n 第三部分 家里openwrt路由器N2N设置 分组名、密钥随便填,但要与公司的n2n设置一样 网络设置 添加一个接口 n2n 防火墙设置 添加zone n2n 添加静态路由表让172.25.0.0走10.0.3.1网关(公司) 静态路由表设置 测试 我网络中有人在看电影所以延时高，平是在25~40ms之间 如果要双向访问(从公司访问家里192.168.1.0网段)， 就在公司openwrt路由上添加静态路由 (公司openwrt上面添加的静态路由，要跟填家里openwrt上面的网段还有分配的虚拟n2nip) 反过来，如果要访问在家里访问公司的电脑，则要在家里的路由器上面添加静态路由值。而且值要是公司的网段，IPv4-网关要添分配给公司的虚拟n2n ip. 当双方都能ping 之后，客户端也必须要开启，并且让客户端上面n2n ip与路由器上面的保持一致！ 1234接口:n2n对象:192.168.1.0子网:255.255.255.0网关:192.168.3.2 原文链接：这里 原文名称：N2N VPN设置图文教程 测试情况：这是我照着此教程进行的测试情况图（本地以无线连接路由器；n2n拨号的IP是10.0.0.X）。远程主机是某openwrt系统，本地路由器是潘多拉 1024 版本（也是openwrt系统）。 需要注意的是： 作者使用的超级节点已经失效，请自己更改一个有效的超级节点； 设置过程中也许需要重启路由器，以使各步生效。 无法访问目标主机。 注意事项 重启（或关机重新开机）本地路由器后，与远程的局域网就断开了，解决办法. 进入管理页面的“网络-接口”，将n2n网络重新连接， 在 etc/rc.local 中添加如下代码（需对应自己的网络进行对应的修改）即可开机自动连接上远程的局域网。 123456killall -9 edge &amp;sleep 2edge -d n2n -a 10.10.10.2 -c test -k password -l 88.86.108.50:82 -M 1300 -r &amp;sleep 13route add -net 192.168.1.0/24 gw 10.10.10.1 &amp;iptables -t nat -A POSTROUTING -j MASQUERADE &amp; 官方N2N更新至2.6稳定版~1234567891011121314151617181920增加了在supernode上指定允许社区（组）的白名单的功能通过多播实现本地对等点发现Windows编译修复和说明MacOS编译修复和说明支持多个edge节点的systemd服务管理通过AES加密以提高安全性和吞吐量添加基准测试工具以提高加密吞吐量提高连接稳定性并建立P2P连接的机会删除密钥计划支持以简化加密代码整合n2n的meyerd分支中所做的更改实现P2P与supernode通信的数据包统计信息用哈希表替换对等链接列表，以在大型网络中更快地查找自动向用户n2n投递n2n版本改进添加对ARM64构建的支持在OpenWRT上构建n2n的说明和makefile文件更多选项来控制MTU，P2P连接，TOS和日志详细程度为n2n协议实现wireshark解剖器在tuntap_linux中删除对system（）的调用，改用netlink实现n2n-decode实用程序以解码流量并将其转储到PCAP 参考原文在这里 1.不要用同一个局域 网内的机器 试。同城 两个 局域网内机器 ping 4ms mtu 设置为1300 下载速度可以达到 80k/s ，基本跑满 1M带宽 N2N(vpn) Edge node（边界节点）与supernode（局端交换中心节点）配置手册 n2n内网穿透神器 树莓派通过 n2n 实现内网穿透 有玩N2N vpn的吗？就差一步成功了 N2N VPN设置图文教程 openwrt N2N VPN应用 n2n内网穿透神器 通过OpenWrt路由器和OpenVPN实现两地局域网互联 在内网组建n2n vpn 远程管理openwrt ssh远程控制 超级supernode资源共享：N2N VPN 超级节点共享 用n2n在Linux上配置一个非常实用免费的VPN解决方案 N2N 新手向导及最新信息（2019-12-05 更新) 参考文档：n2n搭建手记-1-V1参考文档：n2n搭建手记-2-V2]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>n2n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssr,ss被墙后linode与vultr如何自己解决！]]></title>
    <url>%2F2018%2F01%2F26%2Fssr-ss%E8%A2%AB%E5%A2%99%E5%90%8Elinode%E4%B8%8Evultr%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E8%A7%A3%E5%86%B3%EF%BC%81%2F</url>
    <content type="text"><![CDATA[2017.1.25 最近没有什么新鲜事，两件事： 一）处于安全考虑，Linode 在执行一次大规模的系统升级来完成漏洞的防范。这个升级是分多个阶段来完成，很多人看到自己的后台当中有一个 phase 1 complete，这个就是代表第一阶段已经完成了。注意它的升级是自动进行的，所以你不需要进行任何的操作。 二）ssr 在23号到24号之间，大面积被墙，ip接连被封。博主使用的老东京机房的 ip 也不幸中招。 在国内知名的主机论坛逛了一会，发现 vps 板块里铺天盖地的ip被封的帖子，都在骂娘。 注意：并不是 linode 的 ip 容易被墙，而是任何一个 vps hosting 的服务商的 ip 运行 SSR，ip 都可能被墙。 目前没什么好的办法，只能考虑换用一个新ip了。你ps的架构有两种，一种是OpenVZ，还有一种是kvm。OpenVZ的架构下，如果你想要换ip，只能通过发工单的形式让客服帮你更换ip。kvm的架构下，如 Linode，你可以直接 remove 掉，然后重新添加一个Linode，这样就自动重新分配了一个新的 ip 地址。 不过，重新分配的 ip 也不一定就能用，不少也是别人被墙以后换掉的 ip，所以还是要耐心测试有没有能用的。跟以前一样的步骤，配置好系统以后，启动(Boot)。启动完之后再 ping 一下新分配的 ip 地址，如果 ping 得通即可，ping 不通，remove 掉重新添加。。你可以连续添加很多个 linode，依次配置，启动。。然后看看哪个 ip 能用，直到找到能用到。其他全部 remove。 当然，如果你特别不走运新添加的十几个 linode 忙活了三四十分钟发现所有的 ip 都不能用，那么建议你换个机房的。国内多数人较多选用日本和美国的vps 来FQ，你可以选择新加坡机房的试试。按道理说，新加坡机房的 ip 很多都是可以用并且未被墙过的，速度上慢一点点没事，能用就行。 提醒：如果你的Linode上有建站的话，建议还是先备份把网站文件和数据库保存到本地再remove。其实我还是建议建站和 FQ 分开，比如建站用虚拟主机，FQ 用VPS。或者建站用一个VPS，FQ用一个VPS。费用肯定会比网站和梯子都放在一个 VPS 高，但是保险，省心。 源文引自这里 在Vultr购买VPS服务器后，会得到一个随机分配的IP地址，由于你懂的原因，部分IP在国内是被屏蔽的，所以购机后第一步，最好先PING下分配到的IP，如果你不幸正好得到一个这样的地址，那肯定需要再换一个。 那如何免费更换呢？ 本文分两种情况说下Vultr换IP的问题 我们刚建好VPS服务器时，如果对当前的IP不满意，这时换IP不需要保留数据。 VPS服务器已经使用了一段时间，里面有数据需要保留，那么就需要在换IP的同时保留数据。 第一种情况下，首先，登录后台管理界面，点击建好的VPS名称,进入下图界面： 点击上图中右上角的垃圾筒标志，删除建好的系统，然后点击加号，重新建立一个新的，这时会重新分配一个IP，PING一下看能否PING通，能的话就大功告成啦。如果对再次分配的IP不满意，我们可以重复以上步骤，直到换到满意的IP为止。 而第二种情况下，要在保留数据的情况下换IP，就要用到Vultr的快照（Snapshots）功能了，先点击上面图片中上方的“Snapshots”，进入后按提示添加系统快照，这个功能很实用，并且目前是免费的。 添加完成后就可以删除当前的系统了，然后点+号重新建立系统。在接下来新建菜单中第二项，选择如下图的Snapshot，找到刚才建立的快照选择。 其它步骤和新建时一样，这样，新建的VPS服务器既换了IP，又直接恢复了原来的全部数据，避免了换机房导致的数据丢失，节省大家的时间和精力，非常方便！ 小提示：换IP说是免费，其实每建立一个新系统，会自动按0.01美元重新开始计费，所以理论上换一个IP需要0.01美元，这点钱基本可以说是免费了。当然了也不建议频繁换IP，0.01的多了，也是一大笔哦。 原文引自这里]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linode+vultr上SSR服务优化-开启bbr]]></title>
    <url>%2F2018%2F01%2F25%2Fvultr%E4%B8%8ASSR%E6%9C%8D%E5%8A%A1%E4%BC%98%E5%8C%96-%E5%BC%80%E5%90%AFbbr%2F</url>
    <content type="text"><![CDATA[现在linode已经不提供因GFW原因，造成的ip被锁定之后，而提供免费更换IP的服务了。所以得想替代方案了。vultr与樱花都进入了视线。vultr的购买方便。以后有机会再测试 樱花吧。 什么是BBR? BBR是来自于谷歌的TCP拥塞控制算法，它可以让我们绕过拥塞的流量管道，实现加速。Google 开源了其 TCP BBR 拥塞控制算法，并提交到了 Linux 内核，从 4.9 开始，Linux 内核已经用上了该算法。根据以往的传统，Google 总是先在自家的生产环境上线运用后，才会将代码开源，此次也不例外。根据实地测试，在部署了最新版内核并开启了 TCP BBR 的机器上，网速甚至可以提升好几个数量级。 什么情况下搭建bbr魔改全部需要加速的地方，比如以下几种情况（但不限于）： 搭建ss，实现科学上网。但是网速较慢，无法观看高清视频 博客搭建在国外的服务器上，访问速度较慢 升级内核版本如果系统是 64 位，则下载 amd64 的 linux-image 中含有 generic 这个 deb 包；如果系统是 32 位，则下载 i386 的 linux-image 中含有 generic 这个 deb 包；安装的命令如下（以最新版的 64 位 4.12.4 举例而已，请替换为下载好的 deb 包）： TCP-BBR需要VPS升级内核版本到4.9.0 1weget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.12.7/linux-image-4.12.7-041207-generic_4.12.7-041207.201708160856_amd64.deb 1dpkg -i linux-image-4.12.7-041207-generic_4.12.7-041207.201708160856_amd64.deb 安装完成后，再执行命令： 1/usr/sbin/update-grub 重启 vps ss服务建好后，部分朋友在使用过程中可能会觉得速度不够快，这就需要对服务器作进一步的优化了，即开启bbr（谷歌提供的新的TCP拥塞控制算法，其目的就是要尽量跑满带宽，并且尽量不要有排队的情况）。先上一张优化后油管可顺畅观看2K视频的图片。 确认是否已经开启bbr，如果返回结果中没有显示bbr即表示未开启。 1lsmod | grep bbr 依次执行下记命令，可每次复制一行后右键粘贴至命令窗口中。 123456modprobe tcp_bbrecho &quot;tcp_bbr&quot; &gt;&gt; /etc/modules-load.d/modules.confecho &quot;net.core.default_qdisc=fq&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv4.tcp_congestion_control=bbr&quot; &gt;&gt; /etc/sysctl.confsysctl -p 继续输入命令，如果结果都有 bbr, 则证明你的内核已开启 bbr。 12sysctl net.ipv4.tcp_available_congestion_controlsysctl net.ipv4.tcp_congestion_control 再输入之前的命令验证一下看看是否已经开启成功，这次有显示bbr字样了。 1lsmod | grep bbr 2018.6.24 补充 在 CentOS release 6.8 (Final) 系统上安装 BBR加速 如果是Linode VPS的CentOS 6系统，安装BBR前修改内核为GRUB（Legacy） 如何在Linode VPS安装BBR 123uname -r #查看内核版本，是否含4.10lsmod | grep bbr #查看BBR是否启动，返回值有 tcp_bbr表示已启动 参考文档： 原文引自这里:开启bbr 一键安装最新内核并开启 BBR 脚本 vultr全面教呈 几种vps的比较：linode，vultr，樱花 目前能代购到最好的日本VPS]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虞美人]]></title>
    <url>%2F2018%2F01%2F15%2F%E8%99%9E%E7%BE%8E%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[春花秋月何时了，往事知多少。小楼昨夜又东风，故国不堪回首月明中。雕栏玉砌应犹在，只是朱颜改。问君能有几多愁，恰似一江春水向东流。 今日行路听《五代十国.后唐篇》。作者言，古代有二段可歌可泣的爱情故事《西楚霸王项羽与虞姬》、《后唐国主李煜与小周后》。项羽自不必言，生是人杰，死亦鬼雄！李煜在政治上毫无建树，不但失国，而且老婆小周后与他一同降宋后，第三年被宋太宗赵光义看上，并被骗入宫中遭多次侮辱。宋人画《熙陵幸小周后图》就是描述赵光义这段风流债。1127年，靖康之难，金人减宋,赵光义后人的妻女亦被金人凌辱, 真是因果循环，一点不差！ 小周后为了家人，一再隐忍。李煜不是不知道这些，而是无能为力，在传闻里，小周后受辱后常常是破口大骂，觉得自己的夫君无能，此刻李煜除了写词写诗以外，没有别的办法排遣，想到国破家亡，自己的女人也保护不了，在极度压抑苦闷的心境下,于是有这了这首《虞美人》，词中流露出不加掩饰的故国之思，据说是促使宋太宗下令毒死李煜的原因之一。那么，它等于是李煜的催命词了。李煜被毒杀,时年42岁，死状甚惨，小周后悲痛欲绝，不久也随之死去。 李煜亡国后，所写的词凄凉悲壮，意境深远，通俗生动，接近口语，为词史上承前启后的大宗师。被誉为词圣。想到年，除了硬记，没有结合时代的背景去理解，今日反聆听这一段，不禁悲从中来！ 历史没有如果，项羽如果选择了投降，而不死自刎，下场也好不到哪里去。看自己的对手韩信下场就知了！]]></content>
      <categories>
        <category>书评</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Blade Runner 2049]]></title>
    <url>%2F2018%2F01%2F02%2FBlade-Runner-2049%2F</url>
    <content type="text"><![CDATA[睡太早，醒来睡意全无，零晨3点花了两个小时，将美区一刀未剪福利版的【blade runner 2049】看完，感谢字幕组！ 这是一部反无托邦式的电影！根据小说改编而来，信息量巨多，场景，镜头，充满了美感。 电影中的一样超现实的想法，相信若干年后终将成为现实，未来回到家想吃的食物已经做好，开门的哪一刻喜欢的背景音乐响起，一个全息投影出来的虚拟女主(joy)出现在你面前，通过扫描你的面部微表情，感知你的情绪。然后不断变化花样，用各种方式撩你。她永不知疲倦，不会耍情绪。 joy会不断的学习，所以每个属于自己的joy，不会重复。因为世界上找不到两个一模一样的人。joy的数据库（大脑）就是主人投射在现实世界的需求，而定制打造出来的。 维基上面是这样子解释反乌托邦（dystopia）社会的主要特征： 表面看来是公平有序、没有贫困和纷争的理想社会，实际是受到全方位管控只有自由的外表，人的尊严和人性受到否定。 肃清。领导者用宣传对国民洗脑，把自己的体制说成理想社会，反抗者被强制制裁并且排除在社会之外。 剥夺表达的自由。将所谓对社会有害的出版物禁止或没收。 社会不公。在社会承认的市民阶层以下，有不被当人的贫困阶级和贱民存在，事实上是贫富两极的社会。 为了根除市民社会的贫困，用社会体制将极端贫困者强制隔离。 生活在社会体制内的市民阶级，由体制根据血统DNA之类进行管控。 生育管制。为强制进行人口调整，市民的家族计划、恋爱、性行为、妊娠、产子等都由社会管控。 通过愚民政策，以上负面资讯被完全遮蔽，或这些弊端被市民阶层视为理所当然并自然而然予以接受。]]></content>
      <categories>
        <category>影评</category>
      </categories>
      <tags>
        <tag>2049</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[v2ray的模块化]]></title>
    <url>%2F2017%2F12%2F30%2Fv2ray%E7%9A%84%E6%A8%A1%E5%9D%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[V2Ray 的介绍中有这么一句话：”V2Ray 是一个模块化的代理软件包”。在推广的过程中，看上去大家对模块化的概念不太了解，不清楚这是一个什么样的东西。 模块化，简单来说就是乐高积木，每一块积木都有统一的接口（有例外，这里忽略），积木和积木之间想拼就拼，想拆就拆，很方便。V2Ray 也是这样，V2Ray 中的每一个功能都可以简单地添加或移除。 举个例子来说，一些用户尝试在路由器中运行 V2Ray，路由器的 ROM 通常只有 8M 或 16M，而 V2Ray 的程序文件就达到了 7M (v3.4 ARM)，这样就很难塞进路由器了。但其实根据使用情况，在路由器中不会用到 V2Ray 的所有功能，有一些功能不是必要的，比如 Shadowsocks，我们可以把它从 V2Ray 中拿走，以便减少可执行文件的体积。 V2Ray 中所有的模块列表如下 （文件连接） 123456789101112131415161718192021222324252627282930import ( _ &quot;v2ray.com/core/app/dispatcher/impl&quot; _ &quot;v2ray.com/core/app/dns&quot; _ &quot;v2ray.com/core/app/log&quot; _ &quot;v2ray.com/core/app/policy/manager&quot; _ &quot;v2ray.com/core/app/proxyman/inbound&quot; _ &quot;v2ray.com/core/app/proxyman/outbound&quot; _ &quot;v2ray.com/core/app/router&quot; _ &quot;v2ray.com/core/proxy/blackhole&quot; _ &quot;v2ray.com/core/proxy/dokodemo&quot; _ &quot;v2ray.com/core/proxy/freedom&quot; _ &quot;v2ray.com/core/proxy/http&quot; _ &quot;v2ray.com/core/proxy/shadowsocks&quot; _ &quot;v2ray.com/core/proxy/socks&quot; _ &quot;v2ray.com/core/proxy/vmess/inbound&quot; _ &quot;v2ray.com/core/proxy/vmess/outbound&quot; _ &quot;v2ray.com/core/transport/internet/kcp&quot; _ &quot;v2ray.com/core/transport/internet/tcp&quot; _ &quot;v2ray.com/core/transport/internet/tls&quot; _ &quot;v2ray.com/core/transport/internet/udp&quot; _ &quot;v2ray.com/core/transport/internet/websocket&quot; _ &quot;v2ray.com/core/transport/internet/headers/http&quot; _ &quot;v2ray.com/core/transport/internet/headers/noop&quot; _ &quot;v2ray.com/core/transport/internet/headers/srtp&quot; _ &quot;v2ray.com/core/transport/internet/headers/utp&quot; _ &quot;v2ray.com/core/transport/internet/headers/wechat&quot;) 具体每个模块这里就不细说了，从它们的名字应该可以大致猜出是做什么的。假如根据使用场景，这些模块我们不需要： DNS（不加载的时候自动使用 localhost）； Blackhole、SOCKS、HTTP、Shadowsocks、VMess Inbound； Websocket、KCP 以及相关伪装； 把上述的模块删除之后，我们仅使用透明代理加上 VMess 进行代理，以及 Freedom 做分流。示例： 12345678910111213141516import ( _ &quot;v2ray.com/core/app/dispatcher/impl&quot; _ &quot;v2ray.com/core/app/log&quot; _ &quot;v2ray.com/core/app/policy/manager&quot; _ &quot;v2ray.com/core/app/proxyman/inbound&quot; _ &quot;v2ray.com/core/app/proxyman/outbound&quot; _ &quot;v2ray.com/core/app/router&quot; _ &quot;v2ray.com/core/proxy/dokodemo&quot; _ &quot;v2ray.com/core/proxy/freedom&quot; _ &quot;v2ray.com/core/proxy/vmess/outbound&quot; _ &quot;v2ray.com/core/transport/internet/tcp&quot; _ &quot;v2ray.com/core/transport/internet/tls&quot; _ &quot;v2ray.com/core/transport/internet/udp&quot;) 对，只需要修改这一个文件就可以了，其它的都不用动。 重新编译一下，在 ARM 上获得新的程序文件，体积仅为 4M，几乎减半。这样就能很方便地集成进路由器的系统了。 编译 V2Ray 需要一些 Golang 的知识，并不是很难，步骤如下： 安装最新的 Golang SDK. 下载 V2Ray 源码： go get -u v2ray.com/core/… go get -u v2ray.com/ext/… 安装 V2Ray 编译工具：go install v2ray.com/ext/tools/build/vbuild 编译 V2Ray：$GOPATH/bin/vbuild -os=linux -arch=arm 顺便介绍一下如何在路由器中绕过 v2ctl 程序的限制。 准备好你的配置文件 config.json 在 PC 上把它转换成 Protobuf 格式：v2ctl config &lt; config.json &gt; config.pb 在路由器运行 V2Ray：v2ray -config=config.pb 在仅使用 Protobuf 配置文件的时候，V2Ray 是不依赖 v2ctl 以及其它数据文件的，只需要 v2ray 一个文件即可运行。是不是很方便呢。 现在你对 V2Ray 的强大之处是不是更仰慕了呢？ 原文引自：这里]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>v2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[v2ray设计理念]]></title>
    <url>%2F2017%2F12%2F22%2Fv2ray%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[首先，V2Ray 是什么？ V2Ray 是一个网络代理工具。什么是代理？代理是当 A 和 B 想说话但不能直接对话时，要找 C 来传话，这时 C 就是代理。那么在这个过程中，C 有哪些职责？很简单，把话传到，把话传对，并且不泄露给除 A、B 之外的人。 看似这个描述很简单，把它应用到代理软件中，可以表述为下面的内容： 开放网络连接，使得 A 和 B 可以间接地进行正常通信； 通信内容不能被第三方知晓； 网络连接要稳定，不能间歇或永久地失效； 之所以没有写成一二三条，是因为上述三条的优先级，在不同的人看来是不一样的。 大多数人的选择应该会是“正常通信”。毕竟翻墙工具诞生的意义就是让用户访问被墙的网站，比如 Google 被墙了，我架设一个 Shadowsocks 服务器就又可以访问 Google 。至于我在 Google 上搜索了什么，都是一些人畜无害的内容，我想别人也没兴趣知道。只要这个工具能让我爽快地访问 Google，它就是一个好工具。这种情况我们称为“速度优先”。 另外一些人可能就比较注重私密性，不管我在搜索一些什么内容，可能也就是一些网红八卦，但我就是不想让别人知道，被别人知道了我就不爽。这种情况我们称为“隐私优先”。 还有一些人对服务器的稳定性有需求，平时我不怎么翻墙，而一旦需要的时候，就一定要可以及时翻出去。几百万的大单，谈不成这个月就要喝西北风了。这些人对翻墙服务（或服务器）的稳定性要求很高，服务不能断线。而断线的情况有很多种，比如翻墙协议被识别了，或者服务器被探测了，都可以造成翻墙服务失效。这种情况我们称为“稳定优先”。 上述的三种情况在翻墙工具中都有相应的功能，比如“速度优先”可能的问题是 ISP 的 QoS 限制，应对方式是流量混淆。 又比如“隐私优先”可能需要工具提供完美的加密方式。 而“稳定优先”需要翻墙服务器不被探测，翻墙协议需要减少特征。 正是由于这些纷繁复杂的功能，引起了大量的讨论。观众不明就里，而开发人员则忙于推销自己的产品。有些问题呢，就越讨论越看不懂了。 从 V2Ray 的角度来看，这是一个不可调和的矛盾。每个人的需求不同，适合自己的才是最好的。不能因为开发人员觉得一项功能好，就强加给用户。 用户用着没效果，也不会感谢开发者。 举个例子，Shadowsocks 最近的一次协议升级，使用 AEAD 算法取代了 OTA，提供了更好的加密特性。这个升级在“速度优先”的用户看来是完全没有意义的。因为 AEAD 只是加强了数据完整性，对于 QoS 和可被探测性没有任何改进。 也就是说，在客观环境不变的情况下，升级到 AEAD 不会在翻墙速度上有任何提升。于是很多人就会问，到底要不要升级 AEAD，升完对我有什么好处。由于惰性，在没有明显优势的情况下，AEAD 的推广有着很大的阻力。 V2Ray 则尝试从另一个角度来解决这样的问题：提供很多功能，让用户根据自己的需求自由组合。虽然各种组合并不一定能满足所有人的需求，但总比只有一个选择来得好。 对于上述的三个需求，V2Ray 分别提供了不同的特性： 速度优先：VMess 协议、mKCP 协议、Mux.Cool 协议，内部路由分流 隐私优先：TLS (完整实现，非伪装) 稳定优先：HTTP 伪装、BT 伪装等，以及 Mux.Cool 协议。 V2Ray 在设计时已考虑到多个功能相互组合的使用，比如你可以使用 VMess + TLS 来达到隐私和速度的双重保证，或者使用 VMess + Mux 来提升速度和稳定性。 也考虑到不同的用户的需求不同，V2Ray 的所有功能都可以选择打开或关闭，不会把任何一个功能强加给用户。不同的配置可以达到不同的效果，而在 V2Ray 中，你只需要一个配置文件就可以应对这些不同的需求。 当然 V2Ray 的功能远不止上述这些，如果你现在的翻墙工具已不能满足你的日常需求，何不试一下新的轮子呢？ 原文引自这里 v2ray官网]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>v2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用scp在Linux主机之间进行定时备份]]></title>
    <url>%2F2017%2F12%2F17%2F%E4%BD%BF%E7%94%A8scp%E5%9C%A8Linux%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E8%BF%9B%E8%A1%8C%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[如果只是小部分备份，比如只是备份MySQL数据库，就没有必要使用rsync软件备份了。而之前的lftp增量备份似乎也不适合这种备份。 在两台Linux主机，而且都是ssh权限，那就建议使用scp命令备份了，这种备份速度快且是加密传输，安全性高。 运行环境 本地linux(10.1.1.1)主机 远程linux主机(10.1.1.2) 添加ssh公匙scp在建立连接中是需要密码认证的，所以我们第一步就是添加ssh公匙。 ssh终端运行: 1ssh-keygen -t rsa 结果如下: 12345678910Generating public/private rsa key pair. Enter file in which to save the key (/home/.username/ssh/id_rsa):#回车 Enter passphrase (empty for no passphrase):#回车 Enter same passphrase again:#回车 Your identification has been saved in /home/.username /.ssh/id_rsa. Your public key has been saved in /home/.username /.ssh/id_rsa.pub. The key fingerprint is: 38:25:c1:4d:5d:d3:89:bb:46:67:bf:52:af:c3:17:0c username@localhost Generating RSA keys: Key generation complete. 会在用户目录~/.ssh/产生两个文件，id_rsa，id_rsa.pub 配置远程主机第二是把本地(10.1.1.1)主机上的 1id_rsa.pub文件拷贝到远程linux(10.1.1.2)主机的root用户主目录下的.ssh目录下,并且改名为authorized_keys 即： 1scp /root/.ssh/id_rsa.pub root@10.1.1.2:/root/.ssh/authorized_keys 这样在本地linux(10.1.1.1)主机上使用scp命令复制文件到远程linux主机(10.1.1.2)上将不提示输入密码了，直接复制了。 反之亦然！ 使用scp命令我们在管理服务器或vps的时候，经常要上传和下载数据。 比如当我们需要把数据从一个服务器搬到另一个服务器的时候，通常是从第一个服务器下载数据到我们电脑，再到ftp工具上传下载好的数据到远程服务器。 但这样即浪费时间，又浪费精力。我们何不先在第一台服务器打包好，就直接传输数据到另一台服务器呢？ 而Linux scp命令则刚好能实现两台服务器之间传输数据的作用。 什么是scp? scp就是secure copy，是用来进行远程文件拷贝的。数据传输使用 ssh，并且和ssh 使用相同的认证方式，提供相同的安全保证 。与rcp 不同的是，scp 在需要进行验证时会要求你输入密码或口令。 scp的用法从 本地 复制到 远程 命令基本格式： 1scp [可选参数] 本地文件名 远程用户名@远程地址:远程文件或目录 复制文件例子： 12scp /home/backup.zip root@www.example.com:/home/others/backupscp /home/backup.zip root@www.example.com:/home/others/otherbackup.zip 第一个是本地文件backup.zip发送到远程backup目录下。 第二个是本地文件backup.zip发送到远程otherbackup.zip文件。 复制目录例子： 1scp -r /home/backup root@www.example.com:/home/others/ 复制本地目录backup到远程目录others 从 远程 复制到 本地命令基本格式： 1scp [可选参数] 远程用户名@远程地址:远程文件或目录 本地文件名 例子： 12scp root@www.example.com:/home/others/bakcup.zip /home/newbackup.zipscp -r root@www.example.com:/home/backup /home/other/ 第一个是下载远程文件backup.zip到本地文件newbackup.zip。 第二个是下载远程目录bakcup到本地目录other。 scp可选参数： 参数 解释 -v 和大多数 linux 命令中的 -v 意思一样 , 用来显示进度 . 可以用来查看连接 , 认证 , 或是配置错误 -C 使能压缩选项 -P 选择端口 . 注意 -p 已经被 rcp 使用 -4 强行使用 IPV4 地址 -6 强行使用 IPV6 地址 把命令放入脚本，如以下的脚本事例（请根据自己的具体情况修改）1234567#!/bin/shbackpath=/var/ftp/backup/date=`date +%y%m%d`site=sitenametar zcf $&#123;backpath&#125;$&#123;site&#125;&quot;-&quot;$&#123;date&#125;.tar.gz /var/www/$&#123;site&#125;scp $&#123;backpath&#125;$&#123;site&#125;&quot;-&quot;$&#123;date&#125;.tar.gz root@backupserver:/var/backupfind $&#123;backpath&#125; -mtime +30 -exec rm &#123;&#125; \; 使用crontab命令定时运行脚本每个操作系统都有它的自动定时启动程序的功能，Windows有它的任务计划，而Linux对应的功能是crontab. crontab 简介crontab命令常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。 该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。 该词来源于希腊语 chronos(χρόνος)，原意是时间。 通常，crontab储存的指令被守护进程激活， crond常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。 crontab用法crontab的格式如下面： 1f1 f2 f3 f4 f5 program 其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执行程式的路径。 当 f1 为 时表示每分钟都要执行 program，f2 为 时表示每小时都要执行程式，其余类推 当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其余类推 当 f1 为 /n 时表示每 n 分钟个时间间隔执行一次，f2 为 /n 表示每 n 小时个时间间隔执行一次，其余类推 当 f1 为 a, b, c,… 时表示第 a, b, c,… 分钟要执行，f2 为 a, b, c,… 时表示第 a, b, c…个小时要执行，其余类推 管理员登录SSH,输入命令crontab -e编辑crontab文件，根据上面的格式输入并保存。 crontab例子 每月每天每小时的第 0 分钟执行一次 /bin/ls : 10 * * * * /bin/ls 在 12 月内, 每天的早上 6 点到 12 点中，每隔 20 分钟执行一次 /usr/bin/backup : 1*/20 6-12 * 12 * /usr/bin/backup 周一到周五每天下午 5:00 寄一封信给 alex@domain.name : 10 17 * * 1-5 mail -s &quot;hi&quot; alex@domain.name &lt; /tmp/maildata 每月每天的午夜 0 点 20 分, 2 点 20 分, 4 点 20 分….执行 echo “haha” 120 0-23/2 * * * echo &quot;haha&quot; 晚上11点到早上8点之间每两个小时，早上8点 10 23-7/2，8 * * * date 在hp unix,中，每20分钟执行一次，表示为：0,20,40 而不能采用*/n方式，否则出现语法错误 crontab用法其实很容易掌握，懂得使用crontab，对网站和服务器维护起到很大的帮助，比如定时备份，定时优化服务器等。 参考文档： 使用SCP命令 crontab详解]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>scp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync服务器架设(数据同步文件增量备份)]]></title>
    <url>%2F2017%2F12%2F17%2Frsync%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E8%AE%BE-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E6%96%87%E4%BB%B6%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[如果你是一位运维工程师，你很可能会面对几十台、几百台甚至上千台服务器，除了批量操作外，环境同步、数据同步也是必不可少的技能。 说到“同步”，不得不提的利器就是rsync，今天就来说说我从这个工具中看到的同步的艺术。 我们在使用服务器发布我们的网站的时候，通常要考虑到文件的备份，而文件的备份比较高效的备份是增加备份，rsync软件就是这样的一个工具。 为了实现多个服务器负载均衡，我们需要这几个服务器之间进行数据同步，而rsync软件也能胜任. 下面我们来介绍如何架设rsync服务器来达到文件增量备份和数据同步的功能。 什么是rsyncrsync 是一个快速增量文件传输工具，它可以用于在同一主机备份内部的备分，我们还可以把它作为不同主机网络备份工具之用。 本文主要讲述的是如何自架rsync服务器，以实现文件传输、备份和镜像。相对tar和wget来说，rsync 也有其自身的优点，比如速度快、安全、高效。 rsync的安装我们可以执行以下命令安装 1sudo apt-get install rsync rsync服务器的配置文件rsyncd.conf下面我们将涉及到三个文件 rsyncd.conf，rsyncd.secrets 和rsyncd.motd。 12345rsyncd.conf 是rsync服务器主要配置文件。rsyncd.secrets是登录rsync服务器的密码文件。rsyncd.motd是定义rysnc 服务器信息的，也就是用户登录信息。 下面我们分别建立这三个文件。 1mkdir /etc/rsyncd 注：在/etc目录下创建一个rsyncd的目录，我们用来存放rsyncd.conf 和rsyncd.secrets文件； 1touch /etc/rsyncd/rsyncd.conf 注：创建rsyncd.conf ，这是rsync服务器的配置文件； 1touch /etc/rsyncd/rsyncd.secrets 注：创建rsyncd.secrets ，这是用户密码文件； 1chmod 600 /etc/rsyncd/rsyncd.secrets 注：为了密码的安全性，我们把权限设为600； 1touch /etc/rsyncd/rsyncd.motd 注：创建rsyncd.motd文件，这是定义服务器信息的文件。 下一就是我们修改 rsyncd.conf 和rsyncd.secrets 和rsyncd.motd 文件的时候了。 rsyncd.conf文件内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Minimal configuration file for rsync daemon# See rsync(1) and rsyncd.conf(5) man pages for help # This line is required by the /etc/init.d/rsyncd scriptpid file = /var/run/rsyncd.pid port = 873address = 192.168.1.171 #uid = nobody#gid = nobody uid = root gid = root use chroot = yes read only = no #limit access to private LANshosts allow=192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0 hosts deny=* max connections = 5motd file = /etc/rsyncd/rsyncd.motd #This will give you a separate log file#log file = /var/log/rsync.log #This will log every file transferred - up to 85,000+ per user, per sync#transfer logging = yes log format = %t %a %m %f %bsyslog facility = local3timeout = 300 [linuxsirhome] path = /home list=yesignore errorsauth users = linuxsirsecrets file = /etc/rsyncd/rsyncd.secrets comment = linuxsir home exclude = beinan/ samba/ [beinan]path = /optlist=noignore errorscomment = optdir auth users = beinansecrets file = /etc/rsyncd/rsyncd.secrets 密码文件：/etc/rsyncd/rsyncd.secrets的内容格式； 123用户名:密码linuxsir:222222beinan:333333 注：linuxsir是系统用户，这里的密码值得注意，为了安全，你不能把系统用户的密码写在这里。比如你的系统用户 linuxsir 密码是 abcdefg ，为了安全，你可以让rsync 中的linuxsir 为 222222 。 这和samba的用户认证的密码原理是差不多的； rsyncd.motd 文件; 它是定义rysnc 服务器信息的，也就是用户登录信息。比如让用户知道这个服务器是谁提供的等；类似ftp服务器登录时，我们所看到的 linuxsir.org ftp ……。 当然这在全局定义变量时，并不是必须的，你可以用#号注掉，或删除； 我在这里写了一个 rsyncd.motd的内容为： 123++++++++++++++++++++++++++++ linuxsir.org rsync 2002-2007 ++++++++++++++++++++++++++++ rsyncd.conf文件代码说明 注：告诉进程写到 /var/run/rsyncd.pid 文件中； 1pid file = /var/run/rsyncd.pid 注：指定运行端口，默认是873，您可以自己指定； 1port = 873 注：指定服务器IP地址； 1address = 192.168.1.171 注：服务器端传输文件时，要发哪个用户和用户组来执行，默认是nobody。如果用nobody用户和用户组，可能遇到权限问题，有些文件从服务器上拉不下来。所以我就偷懒，为了方便，用了root 。不过您可以在定义要同步的目录时定义的模块中指定用户来解决权限的问题。 1use chroot = yes 用chroot，在传输文件之前，服务器守护程序在将chroot 到文件系统中的目录中，这样做的好处是可能保护系统被安装漏洞侵袭的可能。缺点是需要超级用户权限。另外对符号链接文件，将会排除在外。 也就是说，你在rsync服务器上，如果有符号链接，你在备份服务器上运行客户端的同步数据时，只会把符号链接名同步下来，并不会同步符号链接的内容；这个需要自己来尝试； 1read only = yes 注：read only 是只读选择，也就是说，不让客户端上传文件到服务器上。还有一个 write only选项，自己尝试是做什么用的吧； 12#limit access to private LANshosts allow=192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0 注：在您可以指定单个IP，也可以指定整个网段，能提高安全性。格式是ip 与ip 之间、ip和网段之间、网段和网段之间要用空格隔开； 1max connections = 5 注：客户端最多连接数； 1motd file = /etc/rsyncd/rsyncd.motd 注：motd file 是定义服务器信息的，要自己写 rsyncd.motd 文件内容。当用户登录时会看到这个信息。 1log file = /var/log/rsync.log 注：rsync 服务器的日志； 1transfer logging = yes 注：这是传输文件的日志； 1[linuxsirhome] 注：模块，它为我们提供了一个链接的名字，链接到哪呢，在本模块中，链接到了/home目录；要用[name] 形式； 1path = /home 注：指定文件目录所在位置，这是必须指定的； 1auth users = linuxsir 注：认证用户是linuxsir ，是必须在 服务器上存在的用户； 1list=yes 注：list 意思是把rsync 服务器上提供同步数据的目录在服务器上模块是否显示列出来。默认是yes 。如果你不想列出来，就no ；如果是no是比较安全的，至少别人不知道你的服务器上提供了哪些目录。你自己知道就行了； 1ignore errors 注：忽略IO错误，详细的请查文档； 1secrets file = /etc/rsyncd/rsyncd.secrets 注：密码存在哪个文件； 1comment = linuxsir home data 注：注释可以自己定义，写什么都行，写点相关的内容就行； 1exclude = beinan/ samba/ 注：exclude 是排除的意思，也就是说，要把/home目录下的beinan和samba 排除在外； beinan/和samba/目录之间有空格分开 ； 启动rsync 服务器及防火墙的设置 启动rsync服务器 启动rsync 服务器相当简单，–daemon 是让rsync 以服务器模式运行； 1/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf rsync服务器和防火墙 Linux 防火墙是用iptables，所以我们至少在服务器端要让你所定义的rsync 服务器端口通过，客户端上也应该让通过。 1iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 873 -j ACCEPT 查看一下防火墙是不是打开了 873端口； 1iptables -L 通过rsync客户端来同步数据1rsync -avzP linuxsir@linuxsir.org::linuxsirhome linuxsirhome Password: 这里要输入linuxsir的密码，是服务器端提供的，在前面的例子中，我们用的是 222222，输入的密码并不显示出来；输好后就回车； 注： 这个命令的意思就是说，用linuxsir 用户登录到服务器上，把linuxsirhome数据，同步到本地目录linuxsirhome上。当然本地的目录是可以你自己定义的，比如linuxsir也是可以的；当你在客户端上，当前操作的目录下没有linuxsirhome这个目录时，系统会自动为你创建一个；当存在linuxsirhome这个目录中，你要注意它的写权限。 说明： 123456789-a 参数，相当于-rlptgoD，-r 是递归 -l 是链接文件，意思是拷贝链接文件；-p 表示保持文件原有权限；-t 保持文件原有时间；-g 保持文件原有用户组；-o 保持文件原有属主；-D 相当于块设备文件；-z 传输时压缩；-P 传输进度；-v 传输时的进度等信息，和-P有点关系，自己试试。可以看文档； 1rsync -avzP --delete linuxsir@linuxsir.org::linuxsirhome linuxsirhome 123这回我们引入一个 –delete 选项，表示客户端上的数据要与服务器端完全一致，如果 linuxsirhome目录中有服务器上不存在的文件，则删除。最终目的是让linuxsirhome目录上的数据完全与服务器上保持一致；用的时候要小心点，最好不要把已经有重要数所据的目录，当做本地更新目录，否则会把你的数据全部删除； 1rsync -avzP --delete --password-file=rsync.password linuxsir@linuxsir.org::linuxsirhome linuxsirhome 1这次我们加了一个选项 –password-file=rsync.password ，这是当我们以linuxsir用户登录rsync服务器同步数据时，密码将读取 rsync.password 这个文件。这个文件内容只是linuxsir用户的密码。我们要如下做； 1234touch rsync.passwordchmod 600 rsync.passwordecho &quot;222222&quot;&gt; rsync.passwordrsync -avzP --delete --password-file=rsync.password linuxsir@linuxsir.org::linuxsirhome linuxsirhome 注： 这样就不需要密码了；其实这是比较重要的，因为服务器通过crond 计划任务还是有必要的； 让rsync 客户端自动与服务器同步数据编辑crontab 1crontab -e 加入如下代码 110 0 * * * rsync -avzP --delete --password-file=rsync.password linuxsir@linuxsir.org::linuxsirhome linuxsirhome 表示每天0点10分执行后面的命令。 参考文档：原文引自这里 rsync同步的艺术]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[inotify+rsync实现数据实时同步]]></title>
    <url>%2F2017%2F12%2F17%2Finotify-rsync%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[数据实时同步介绍什么是实时同步：如何实现实时同步A. 要利用监控服务（inotify），监控同步数据服务器目录中信息的变化 B. 发现目录中数据产生变化，就利用rsync服务推送到备份服务器上 实现实时同步的方法inotify+rsync 方式实现数据同步 sersync 方式实现实时数据同步 实时同步原理介绍 inotify+rsync 方式实现数据同步Inotify简介Inotify是一种强大的，细粒度的。异步的文件系统事件监控机制，linux内核从2.6.13起，加入了Inotify支持，通过Inotify可以监控文件系统中添加、删除，修改、移动等各种事件,利用这个内核接口，第三方软件就可以监控文件系统下文件的各种变化情况，而 inotify-tools 正是实施这样监控的软件。 国人周洋在金山公司也开发了类似的实时同步软件sersync。 提示信息： sersync软件实际上就是在 inotify软件基础上进行开发的，功能要更加强大些 ，多了定时重传机制，过滤机制了提供接口做 CDN，支持多线程橾作。 Inotify实际是一种事件驱动机制，它为应用程序监控文件系统事件提供了实时响应事件的机制，而无须通过诸如cron等的轮询机制来获取事件。 cron等机制不仅无法做到实时性，而且消耗大量系统资源。相比之下，inotify基于事件驱动，可以做到对事件处理的实时响应，也没有轮询造成的系统资源消耗，是非常自然的事件通知接口，也与自然世界事件机制相符合。 inotify的实现有几款软件： inotify-tools，sersync，lrsyncd inotify+rsync使用方式inotify 对同步数据目录信息的监控 rsync 完成对数据信息的实时同步 利用脚本进行结合 部署inotify软件的前提需要2.6.13以后内核版本才能支持inotify软件。2.6.13内核之后版本，在没有安装inotify软件之前，应该有这三个文件。 123456789[root@backup ~]# ll /proc/sys/fs/inotify/total 0-rw-r--r-- 1 root root 0 Oct 17 10:12 max_queued_events-rw-r--r-- 1 root root 0 Oct 17 10:12 max_user_instances-rw-r--r-- 1 root root 0 Oct 17 10:12 max_user_watches 三个重要文件的说明 【服务优化】 可以将三个文件的数值调大，监听更大的范围 inotify软件介绍及参数说明两种安装方式1234- yum install -y inotify-tools- 手工编译安装 手工编译安装方式需要到github上进行下载软件包 inotify软件的参考资料链接： 1https://github.com/rvoicilas/inotify-tools/wiki inotify主要安装的两个软件inotifywait： （主要） 在被监控的文件或目录上等待特定文件系统事件（open close delete等）发生，执行后处于阻塞状态，适合在shell脚本中使用 inotifywatch： 收集被监控的文件系统使用的统计数据，指文件系统事件发生的次数统计。 说明：在实时实时同步的时候，主要是利用inotifywait对目录进行监控 inotifywait命令参数说明 -e[参数] 可以指定的事件类型inotifywait监控中的事件测试 创建事件 12345[root@nfs01 data]# touch test2.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e create17-10-17 11:19 /data/test2.txt 事件信息: CREATE 删除事件 12345[root@nfs01 data]# \rm -f test1.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e delete17-10-17 11:28 /data/test1.txt 事件信息: DELETE 修改事件 12345[root@nfs01 data]# echo &quot;132&quot; &gt; test.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e close_write17-10-17 11:30 /data/test.txt 事件信息: CLOSE_WRITE,CLOSE 移动事件 moved_to 12345[root@nfs01 data]# mv /etc/hosts .[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_to17-10-17 11:33 /data/hosts 事件信息: MOVED_TO 移动事件 moved_from 12345[root@nfs01 data]# mv ./hosts /tmp/[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_from17-10-17 11:34 /data/hosts 事件信息: MOVED_FROM 修改输出的日期格式123[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d/%m/%y %H:%M&quot; --format &quot;%T %w%f&quot;17/10/17 11:12 /data/test1.txt 对inotifywait命令的测试对inotifywait命令测试的说明： 需要打开两个连接窗口 窗口运行inotifywait 窗口对文件夹进行操作，可在一窗口中查看出inotifywait的监控记录 创建文件的逻辑1234567891011121314151617[root@nfs01 ~]# inotifywait /dataSetting up watches.Watches established./data/ CREATE test1.txt/data/ OPEN test1.txt/data/ ATTRIB test1.txt/data/ CLOSE_WRITE,CLOSE test1.txt创建文件，inotifywait显示创建文件的过程↑[root@nfs01 data]# touch test1.txt 创建目录逻辑12345[root@nfs01 data]# mkdir testdir[root@nfs01 ~]#/data/ CREATE,ISDIR testdir 监控子目录下的文件123456789[root@nfs01 data]# touch testdir/test01.txt[root@nfs01 ~]# inotifywait -mrq /data/data/testdir/ OPEN test01.txt/data/testdir/ ATTRIB test01.txt/data/testdir/ CLOSE_WRITE,CLOSE test01.txt sed命令修改逻辑123456789101112131415161718192021222324252627[root@nfs01 data]# sed &apos;s#132#123#g&apos; test.txt -i[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_from /data/test.txt 事件信息: OPEN /data/sedDh5R8v 事件信息: CREATE /data/sedDh5R8v 事件信息: OPEN /data/test.txt 事件信息: ACCESS /data/sedDh5R8v 事件信息: MODIFY /data/sedDh5R8v 事件信息: ATTRIB /data/sedDh5R8v 事件信息: ATTRIB /data/test.txt 事件信息: CLOSE_NOWRITE,CLOSE /data/sedDh5R8v 事件信息: CLOSE_WRITE,CLOSE /data/sedDh5R8v 事件信息: MOVED_FROM /data/test.txt 事件信息: MOVED_TO 实时同步命令参数示意图 inotify+rsync实时同步服务部署第一个里程碑：部署rsync服务rsync服务端部署 软件是否存在 123[root@backup ~]# rpm -qa |grep rsyncrsync-3.0.6-12.el6.x86_64 需求：查询到某个命令非常有用。但是不知道属于哪个软件包 123yum provides rysncprovides Find what package provides the given value 进行软件服务配置 1234567891011121314151617181920212223242526272829303132333435363738394041[root@backup ~]# vim /etc/rsyncd.confuid = rsyncgid = rsyncuse chroot = nomax connections = 200timeout = 300pid file = /var/run/rsyncd.pidlock file = /var/run/rsync.locklog file = /var/log/rsyncd.logignore errorsread only = falselist = falsehosts allow = 172.16.1.0/24auth users = rsync_backupsecrets file = /etc/rsync.password[backup]comment = &quot;backup dir by oldboy&quot;path = /backup[nfsbackup]comment = &quot;nfsbackup dir by hzs&quot;path = /nfsbackup 创建rsync管理用户 1[root@backup ~]# useradd -s /sbin/nologin -M rsync 创建数据备份储存目录,目录修改属主 123[root@backup ~]# mkdir /nfsbackup/[root@backup ~]# chown -R rsync.rsync /nfsbackup/ 创建认证用户密码文件并进行授权600 123echo &quot;rsync_backup:oldboy123&quot; &gt;&gt;/etc/rsync.passwordchmod 600 /etc/rsync.password 启动rsync服务 1rsync --daemon 至此服务端配置完成 12345[root@backup ~]# ps -ef |grep rsyncroot 2076 1 0 17:05 ? 00:00:00 rsync --daemonroot 2163 1817 0 17:38 pts/1 00:00:00 grep --color=auto rsync rsync客户端配置 软件是否存在 123[root@backup ~]# rpm -qa |grep rsyncrsync-3.0.6-12.el6.x86_64 创建安全认证文件，并进行修改权限600 123echo &quot;oldboy123&quot; &gt;&gt;/etc/rsync.passwordchmod 600 /etc/rsync.password 测试数据传输 1234567891011[root@nfs01 sersync]# rsync -avz /data rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.passwordsending incremental file listdata/data/.hzsdata/.tar.gzdata/.txt 第二个里程碑：部署inotify服务安装inotify软件两种安装方式 123- yum install -y inotify-tools- 手工编译安装 注： 手工编译安装方式需要到github上进行下载软件包 inotify软件的参考资料链接： 1https://github.com/rvoicilas/inotify-tools/wiki 查看inotify安装上的两个命令 inotifywait inotifywatch 12345[root@nfs01 ~]# rpm -ql inotify-tools/usr/bin/inotifywait #主要/usr/bin/inotifywatch inotifywait和inotifywatch的作用 inotifywait : 在被监控的文件或目录上等待特定文件系统事件（open close delete等）发生,执行后处于阻塞状态，适合在shell脚本中使用 inotifywatch :收集被监控的文件系统使用的统计数据,指文件系统事件发生的次数统计。 第三个里程碑：编写脚本，实现rsync+inotify软件功能结合 rsync服务命令 1rsync -avz --delete /data/ rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.password inotify服务命令： 1inotifywait -mrq /data -format &quot;%w%f&quot; -e create,delete,move_to,close_write 编写脚本 12345678[root@nfs01 sersync]# vim /server/scripts/inotify.sh#!/bin/bashinotifywait -mrq /data --format &quot;%w%f&quot; -e create,delete,moved_to,close_write|\while read linedo rsync -az --delete /data/ rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.passworddone 脚本说明： for循环会定义一个条件，当条件不满足时停止循环 while循环：只要条件满足就一直循环下去 对脚本进行优化 12345678910111213141516#!/bin/bashPath=/databackup_Server=172.16.1.41/usr/bin/inotifywait -mrq --format &apos;%w%f&apos; -e create,close_write,delete /data | while read line do if [ -f $line ];then rsync -az $line --delete rsync_backup@$backup_Server::nfsbackup --password-file=/etc/rsync.password else cd $Path &amp;&amp;\ rsync -az ./ --delete rsync_backup@$backup_Server::nfsbackup --password-file=/etc/rsync.password fidone 第四个里程碑：测试编写的脚本让脚本在后台运行 在/data 目录先创建6个文件 123[root@nfs01 data]# sh /server/scripts/inotify.sh &amp;[root@nfs01 data]# touch &#123;1..6&#125;.txt 在backup服务器上，已经时候同步过去了6个文件。 123456789101112131415root@backup ~]# ll /nfsbackup/total 8-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 1.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 2.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 3.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 4.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 5.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 6.txt 利用while循环语句编写的脚本停止方法（kill）ctrl+z暂停程序运行，kill -9杀死 不要暂停程序，直接利用杀手三剑客进行杀进程 说明： kill三个杀手不是万能的，在进程暂停时，无法杀死；kill -9 （危险） 查看后台都要哪些程序在运行123[root@nfs01 data]# jobs[1]+ Running sh /server/scripts/inotify.sh &amp; fg将后台的程序调到前台来123[root@nfs01 data]# fg 1sh /server/scripts/inotify.sh 进程的前台和后台运行方法：123fg -- 前台bg -- 后台 脚本后台运行方法1234501. sh inotify.sh &amp;02. nohup sh inotify.sh &amp;03. screen实现脚本程序后台运行 1sh /server/scripts/inotify.sh &amp; nohup1nohup sh inotify.sh &amp; 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile详解]]></title>
    <url>%2F2017%2F12%2F16%2FDockerfile%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[基本结构Dockerfile 由一行行命令语句组成，并且支持以# 开头的注释行。 一般的，Dockerfile 分为四部分： 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令。 例如 1234567891011121314# This dockerfile uses the ubuntu image# VERSION 2 - EDITION 1# Author: docker_user# Command format: Instruction [arguments / command] ..# Base image to use, this must be set as the first lineFROM ubuntu# Maintainer: docker_user &lt;docker_user at email.com&gt; (@docker_user)MAINTAINER docker_user docker_user@email.com# Commands to update the imageRUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ raring main universe&quot; &gt;&gt; /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y nginxRUN echo &quot;\ndaemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf# Commands when creating a new containerCMD /usr/sbin/nginx 其中，一开始必须指明所基于的镜像名称，接下来推荐说明维护者信息。 后面则是镜像操作指令，例如RUN 指令， RUN 指令将对镜像执行跟随的命令。每运行一条RUN 指令，镜像添加新的一层，并提交。 最后是CMD 指令，来指定运行容器时的操作命令。 下面是一个更复杂的例子 12345678910111213141516171819202122232425262728293031# Nginx## VERSION 0.0.1FROM ubuntuMAINTAINER Victor Vieux &lt;victor@docker.com&gt;RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server# Firefox over VNC## VERSION 0.3FROM ubuntu# Install vnc, xvfb in order to create a &apos;fake&apos; display and firefoxRUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefoxRUN mkdir /.vnc# Setup a passwordRUN x11vnc -storepasswd 1234 ~/.vnc/passwd# Autostart firefox (might not be the best way, but it does the trick)RUN bash -c &apos;echo &quot;firefox&quot; &gt;&gt; /.bashrc&apos;EXPOSE 5900CMD [&quot;x11vnc&quot;, &quot;-forever&quot;, &quot;-usepw&quot;, &quot;-create&quot;]# Multiple images example## VERSION 0.1FROM ubuntuRUN echo foo &gt; bar# Will output something like ===&gt; 907ad6c2736fFROM ubuntuRUN echo moo &gt; oink# Will output something like ===&gt; 695d7793cbe4# You᾿ll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with# /oink. 指令指令的一般格式为 INSTRUCTION arguments ，指令包括FROM 、MAINTAINER 、RUN 等。 FROM 格式为FROM 或 FROM : 。 第一条指令必须为FROM 指令。并且，如果在同一个Dockerfile中创建多个镜像时，可以使用多个FROM 指令（每个镜像一次）。 MAINTAINER 格式为 MAINTAINER ，指定维护者信息。 RUN 格式为 RUN 或 RUN [“executable”, “param1”, “param2”] 。 前者将在 shell 终端中运行命令，即/bin/sh -c ；后者则使用exec 执行。指定使用其它终端可以通过第二种方式实现，例如RUN [“/bin/bash”, “-c”, “echo hello”] 。 每条RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\ 来换行。 CMD 支持三种格式: CMD [“executable”,”param1”,”param2”] 使用exec 执行，推荐方式； CMD command param1 param2 在/bin/sh 中执行，提供给需要交互的应用； CMD [“param1”,”param2”] 提供给ENTRYPOINT 的默认参数； 指定启动容器时执行的命令，每个 Dockerfile 只能有一条CMD 命令。如果指定了多条命令，只有最后一条会被执行。 EXPOSE 格式为EXPOSE […]告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。 ENV 格式为ENV 。 指定一个环境变量，会被后续RUN 指令使用，并在容器运行时保持。 例如 1234ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH ADD 格式为ADD 该命令将复制指定的 到容器中的 。 其中 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）。 COPY 格式为COPY 。 复制本地主机的 （为 Dockerfile 所在目录的相对路径）到容器中的 。 当使用本地目录为源目录时，推荐使用 COPY 。 ENTRYPOINT 两种格式： ENTRYPOINT [“executable”, “param1”, “param2”] ENTRYPOINT command param1 param2 （shell中执行）。 配置容器启动后执行的命令，并且不可被docker run 提供的参数覆盖。每个 Dockerfile 中只能有一个ENTRYPOINT ，当指定多个时，只有最后一个起效。 VOLUME 格式为VOLUME [“/data”] 。创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 USER 格式为USER daemon 。指定运行容器时的用户名或 UID，后续的RUN 也会使用指定用户。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如： RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 。要临时获取管理员权限可以使用gosu ，而不推荐sudo 。 WORKDIR 格式为WORKDIR /path/to/workdir 。为后续的RUN 、CMD 、ENTRYPOINT 指令配置工作目录。可以使用多个WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 则最终路径为/a/b/c 。 ONBUILD 格式为ONBUILD [INSTRUCTION] 。配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。例如，Dockerfile 使用如下的内容创建了镜像image-A 。 1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] 如果基于 image-A 创建新的镜像时，新的Dockerfile中使用FROM image-A 指定基础镜像时，会自动执行ONBUILD 指令内容，等价于在后面添加了两条指令。 1234FROM image-A#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src 使用ONBUILD 指令的镜像，推荐在标签中注明，例如ruby:1.9-onbuild 。 创建镜像编写完成 Dockerfile 之后，可以通过docker build 命令来创建镜像。 基本的格式为 ==docker build== [选项] 路径，该命令将读取指定路径下（包括子目录）的 Dockerfile，并将该路径下所有内容发送给 Docker 服务端，由服务端来创建镜像。因此一般建议放置 Dockerfile 的目录为空目录。也可以通过.dockerignore 文件（每一行添加一条匹配模式）来让 Docker 忽略路径下的目录和文件。要指定镜像的标签信息，可以通过-t 选项， 例如 1$ sudo docker build -t myrepo/myapp /tmp/test1/]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu14.04更换内核以安装锐速]]></title>
    <url>%2F2017%2F12%2F16%2FUbuntu14-04%E6%9B%B4%E6%8D%A2%E5%86%85%E6%A0%B8%E4%BB%A5%E5%AE%89%E8%A3%85%E9%94%90%E9%80%9F%2F</url>
    <content type="text"><![CDATA[首先，将现有vps系统更新过最新版本。 123apt-get updateapt-get upgrade 命令 1uname -a 检查当前Kernel版本。结果举例： 1Linux localhost 4.0.4-x86_64-linode57 #1 SMP Thu May 21 11:01:47 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux 接下来是安装grub2组件： 123456789Ubuntu:apt-get update apt-get install linux-image-virtual grub2Debian:apt-get install linux-image-amd64 grub2 注意！远程窗口会显示Grub安装界面，要求你选择grub安装位置，你直接选择不安装即可。 查看默认系统内核版本： ls /boot/vmlinuz* 接下来我们配置grub启动参数： 1vi /etc/default/grub 将grub配置文件修改以下参数： 12345GRUB_TIMEOUT=10GRUB_CMDLINE_LINUX=&quot;console=ttyS0,19200n8&quot;GRUB_DISABLE_LINUX_UUID=trueGRUB_SERIAL_COMMAND=&quot;serial -speed=19200 -unit=0 -word=8 -parity=no -stop=1&quot;GRUB_TERMINAL=serial 更新bootloader： Debian 8 &amp; Ubuntu 15.04： 更新系统引导文件 1update-grub 点击Linode后台面板Dashboard，点击Edit按钮： 在Kernel下拉菜单，选择GRUB 2启动： 重启vps后，再次输入uname -a可显示内核版本号。 举例：Linux li63-119.members.linode.com 3.10.0-229.4.2.el7.x86_64 至此，你已摆脱了Linode官方默认的内核，可随意安装任意版本号的Kernel，然后修改grub菜单指定启动选项。 Linode官网 www.linode.com 如果是用Linode竞争对手的产品，无论是Vultr和Digitalocean都可轻松更换内核，比linode方便很多，而且默认直接支持hybla阻塞算法，有良好的加速效果。 参考文档 https://since1989.org/linode/centos-ubuntu-kernel-linux-grub2.html 第二步 破解版锐速最新更新 =======2016年8月7日更新===========： 新增了以下支持的内核，欢迎大家测试，有问题及时反馈： 12345CentOS-6.8：2.6.32-642.el7.x86_64CentOS-7.2：3.10.0-327.el7.x86_64CentOS：4.4.0-x86_64-linode63Ubuntu_14.04：4.2.0-35-genericDebian_8：3.16.0-4-amd64 首先保证你的服务器或VPS是64位系统，锐速不支持任何ubuntu 14.04的32位系统 安装linux-image-4.2.0-35-generic内核文件： 1~# apt-get install linux-image-4.2.0-35-generic 复制代码 查看当前安装的内核： 1~# dpkg -l|grep linux-image 复制代码 这里会返回刚才装的linux-image-4.2.0-35-generic内核和之前服务器上安装的内核，我们要做的就是卸载以前安装的内核. 卸载第3步中看到的其他内核： 1~# apt-get purge linux-image-3.13.0-110-generic linux-image-3.13.0-95-generic 复制代码 这里的xx是第3步中看到的当前服务器或VPS上安装的其他内核，注意如果当前服务器安装的不是最新的内核，卸载的同时会给服务器安装最新内核；为了能让服务器使用锐速支持的3.13.0-24-generic内核，我们还要再执行一次这个命令，把安装的最新内核卸载掉。 更新grub系统引导文件： #~ sudo update-grub复制代码 重启系统： #~ sudo reboot复制代码 重启之后使用 #~ uname -r复制代码就可以看到服务器已经使用锐速支持的3.13.0-24-generic内核了，这时候就可以去安装锐速了 参考文档 第三步安装锐速 (一键安装) 锐速破解版安装方法： wget -N –no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.sh vi /serverspeeder/etc/config 锐速破解版卸载方法： chattr -i /serverspeeder/etc/apx* &amp;&amp; /serverspeeder/bin/serverSpeeder.sh uninstall -f 查看锐速启动状态/serverspeeder/bin/serverSpeeder.sh status 第二种方式 锐速手动安装 1wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/test/serverspeeder-v.sh &amp;&amp; bash serverspeeder-v.sh Ubuntu 14.04 4.2.0-35-generic x64 3.11.20.4 serverspeeder_42035 第三种安装方式 锐速 (lotServer) 一键安装新增 lotServer 安装，建议首先尝试使用，安装失败再尝试其他方式有部分使用者可能存在91yun的锐速破解后存在断流问题，可以尝试锐速 (lotServer)参考 lotServer是锐速的母公司，面向企业用户。大家比较熟悉的锐速其实是lotServer的马甲首先查一下，是否有支持所采用的系统的内核存在看这里安装 参考文档： Ubuntu14.04更换内核以安装锐速]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网精编]]></title>
    <url>%2F2017%2F12%2F16%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E7%B2%BE%E7%BC%96%2F</url>
    <content type="text"><![CDATA[前景 谁比较需要单独购买VPS科学上网？ 只是翻墙简单浏览网页，就不必要再单独花费人民币了，蓝灯、和各个网站的免费ss账户，是你的主菜。 偶尔看看视频、不想画太多精力在寻找免费资源上。可以直接购买现成的ss/vpn账户，在G+相应的社区可以找到很多比较靠谱的提供商。 常年泡视频网站又受不了国内视频网站乱七八糟、24h使用google服务、对稳定性和流量有一定要求、愿意使用一点时间来折腾VPS。 搜索文档，更新软件源、折腾个人网站又不想备案的、折腾开源的 这部分大神谁会来看这个啊，早自己分分钟写个轮子了 注意，本教程 不是 图文并茂的面向小白的教程，窝希望你能够有足够的 Linux 操作经验再来看这篇教程。至少你需要熟悉 ssh 连接，熟悉 Web 环境的配置，最好可以看得懂一些代码。 前端搭建搭建环境 Linux ubuntu 4.2.0-35-generic #40~14.04.1-Ubuntu x86_64 GNU/Linux (ubuntu14.04)mysql Ver 14.14 Distrib 5.5.48 (Enable InnoDB: y)nginx-1.10.0mysql-5.5.48php-7.0.7安装lnmp 1.3用的是SSR(该款的子菜单shadowsocks是单用户)管理SSR的前端是sspanel 步骤开始： [x] linode更换Linux内核通过锐速加速 请参看这篇博文 ubuntu16.04不需要装锐速，只需要更新linux内核即可。 整合ss-panel目前主要是支持了 VPN 自动开户，自动销毁，流量计入面板的总流量，以及弄了个公告系统，还有和 DirectAdmin 对接，还有可以支持 PAC 等方式的接入。 添加一个虚拟主机，同时创建数据库。 1lnmp vhost add 配置好nginx运行环境之后，就是下载mod ss-panel 123456cd /home/wwwroot/ss.panelyum install git -ygit clone https://github.com/glzjin/ss-panel-v3-mod.git tmp -b new_master &amp;&amp; mv tmp/.git . &amp;&amp; rm -rf tmp &amp;&amp; git reset --hardchown -R root:root *chmod -R 777 *chown -R www:www storage 修改完了之后，到网站目录下进行一些修改 1234[root@vultr vhost]# cd /home/wwwroot/ss.panel/[root@vultr ss.panel]# chattr -i .user.ini[root@vultr ss.panel]# mv .user.ini public[root@vultr ss.panel]# cd public 然后就是重新添加回权限。 1chattr +i .user.ini OK，重启一下 nginx 1service nginx restart 编辑完后重载你的 Web 服务器，然后访问你的站点……于是你得到了一个 500 Internal Server Error（如果你没开启 display_errors 可能看不到详细报错）： 1234Warning: require(/home/wwwroot/ss.prinzeugen.net/vendor/autoload.php): failed to open stream: No such file or directory in /home/wwwroot/ss.prinzeugen.net/bootstrap.php on line 18Fatal error: require(): Failed opening required &apos;/home/wwwroot/ss.prinzeugen.net/vendor/autoload.php&apos; (include_path=&apos;.:/usr/local/php/lib/php&apos;) in /home/wwwroot/ss.prinzeugen.net/bootstrap.php on line 18 这是我们还未安装 ss-panel 所需的依赖库导致的。遂安装之： 12$ curl -sS https://getcomposer.org/installer | php$ php composer.phar install 等待它安装完毕后接着进行配置： 12345cp .env.example .env有的版本的配置文件是在config/.config.php.example 将 .env.example (.config.php.example ) 复制一份重命名为 .env (.config.php)，自行修改其中的数据库等信息。 12345678910# database 数据库配置db_driver = &apos;mysql&apos;db_host = &apos;localhost&apos;db_port = &apos;3306&apos;db_database = &apos;ss-panel&apos;db_username = &apos;ss-panel&apos;db_password = &apos;secret&apos;db_charset = &apos;utf8&apos;db_collation = &apos;utf8_general_ci&apos;db_prefix = &apos;&apos; 数据库的创建我就不多说了，建站的一般都玩过数据库吧？将根目录下的 db.sql 导入到数据库中即可。其他配置自行修改。 你还需要修改 .env 中的 muKey 字段，修改为任意字符串（最好只包含 ASCII 字符），下面配置后端的时候我们需要使用到这个 muKey： 1muKey = &apos;api_key_just_for_test&apos; 接下来，确保 storage 目录可写入（否则 Smarty 会报错）： 1$ chown -R www storage 现在访问你的站点，就可以看到 ss-panel 的首页啦： 1$ php xcat createAdmin 重置流量 1php xcat resetTraffic Auth Driver 认证设置ss-panel v3 配置说明，请根据说明合理选择密码加密方式，认证方式等 ss-panel v3支持多种存储用户认证信息的方式： file 使用文件存储sessions。 redis 使用Redis存储，推荐此方式。 推荐使用redis 计划任务接下来需要对服务器进行计划任务的设置, 执行 crontab -e命令, 添加以下五段 123456730 22 * * * php /home/wwwroot/站点文件夹/xcat sendDiaryMail */1 * * * * php /home/wwwroot/站点文件夹/xcat synclogin*/1 * * * * php /home/wwwroot/站点文件夹/xcat syncvpn0 0 * * * php -n /home/wwwroot/站点文件夹/xcat dailyjob*/1 * * * * php /home/wwwroot/站点文件夹/xcat checkjob */1 * * * * php -n /home/wwwroot/站点文件夹/xcat syncnas 重启Crontab /etc/init.d/cron restart 查看防火墙规则1234查看已添加的iptables规则iptables -L -n --line-numbers删除已添加的iptables规则iptables -D INPUT line-numbers 规则保存1234# Ubuntuiptables-save &gt; /etc/iptables.rules# CentOSservice iptables save 部署并配置 shadowsocks-manyuser(后端)连接上 shadowsocks 试试看能不能翻墙了？八成不能。 虽然你成功的把 servers.py 跑起来了，但还可能有各种神奇的错误阻止你翻出伟大的墙。 首先国际惯例查看连接： 1$ netstat -anp | grep 你的端口 正常的话，应该是这样的： 如果没有来自你的 IP 的 TCP 连接的话，那八成就是防火墙的锅了，执行 iptables 放行你的端口： 12$ iptables -I INPUT -p tcp -m tcp --dport 你的端口 -j ACCEPT$ iptables-save 3.运行SSR 1234567运行的话, 有几种方式 python server.py 用于调错的 ./run.sh 无日志后台运行 ./logrun.sh 有日志后台运行 supervisord 这句运行代码主要用于调试，关闭ssh后ss后端自动关闭，所以正式使用请使用下面的脚本运行！如果需要停止请按Ctrl+C键终止程序。这时可查看有运行情况，检查有没有错误。如果服务端没有错误，而连接不上，需要检查iptables或firewall(ubuntu)的防火墙配置 ss-panel 新注册的用户所分配的端口均为其 id-1 的用户的端口号 + 1。比如说你把 admin 用户（uid 为1）的端口改为 12450（ss-panel 中不能改，去数据库改），那么后面注册的新用户的端口就会是 12451, 12452 这样递增的。 所以如果你要开放注册，就要这样配置你的 iptables： 123# 注意是半角冒号，意为允许 12450 及以上的端口# 也可以指定 12450:15550 这样的范围$ iptables -I INPUT -p tcp -m tcp --dport 12450: -j ACCEPT 现在再连接 shadowsocks 就应该可以看到 TCP 连接信息了。并且你可以在 ss-mu 后端的输出信息中看到详细的连接日志： 使用supervisor监控ss-manyuser后端运行安装 supervisor （用的是上面安装过的 pip）： 12345# 先安装 pip 包管理器$ sudo apt-get install python-pip # For Debian/Ubuntu$ sudo yum install python-pip # For CentOS$ pip install supervisor 创建 supervisor 配置文件 12# 输出至 supervisor 的默认配置路径$ echo_supervisord_conf &gt; /etc/supervisord.conf 运行 supervisor 服务 1$ supervisord 配置 supervisor 以监控 ss-manyuser 运行 1$ vim /etc/supervisord.conf 在文件尾部（当然也可以新建配置文件，不过这样比较方便）添加如下内容并酌情修改： 12345[program:ss-manyuser]command = python /root/shadowsocks-py-mu/shadowsocks/servers.pyuser = rootautostart = trueautorestart = true 其中 command 里的目录请自行修改为你的 servers.py 所在的绝对路径。 重启 supervisor 服务以加载配置 1$ killall -HUP supervisord 查看 shadowsocks-manyuser 是否已经运行： 12$ ps -ef | grep servers.pyroot 952 739 0 15:40 ? 00:00:00 python /root/shadowsocks-rm/shadowsocks/servers.py 可以通过以下命令管理 shadowsock-manyuser 的状态 1$ supervisorctl &#123;start|stop|restart&#125; ss-manyuser ss-panel 的多节点配置其实多节点也没咋玄乎，说白了就是多个后端共用一个前端而已。而且我们的后端是使用 Mu API 来与前端进行交互的，所以多节点的配置就更简单了：只要把所有后端的 config.py 中的 API_URL 和 API_PASS 都改成一样即可（记得 API_ENABLED = True）。 Linux chattr命令Linux chattr命令用于改变文件属性。这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下8种模式： 用chattr命令防止系统中某个关键文件被修改： 1chattr +i /etc/resolv.conf 1lsattr /etc/resolv.conf 会显示如下属性 1----i-------- /etc/resolv.conf 让某个文件只能往里面追加数据，但不能删除，适用于各种日志文件： Step 4 123456789Nginx Config example:if you download ss-panel on path /home/www/ss-panelroot /home/www/ss-panel/public;location / &#123; try_files $uri $uri/ /index.php$is_args$args;&#125; 1chattr +a /var/log/messages 参考文档在这里 报错： 1234&gt; PHP Warning: require(/home/wwwroot/mod-ss-panel/vendor/autoload.php): failed to open stream: No such file or directory in /home/wwwroot/mod-ss-panel/bootstrap.php on line 17PHP Fatal error: require(): Failed opening required &apos;/home/wwwroot/mod-ss-panel/vendor/autoload.php&apos; (include_path=&apos;.:/usr/local/php/lib/php&apos;) in /home/wwwroot/mod-ss-panel/bootstrap.php on line 17&gt; Failed opening required &apos;vendor/autoload.php&apos; after update 解决方法 1Try to run &quot;composer install&quot; in the project root, since the master branch does not automatically include all dependencies (opposing to stable releases). 原文在这里 报错 1ss-panel-v3-mod No input file specified 解决方法 更改php.ini首先php.ini的配置中把;cgi.fix_pathinfo=0 改为cgi.fix_pathinfo=1 原文在这里 mod版官方wiki 报错 1ImportError: No module named cymysql 解决方法 安装cymysql 1pip install cymysql 原文在这里 报错 1The server quit without updating PID file (/var/run/mysqld/mysqld.pid). 解决方法 /usr/local/var/目录下面也确实生成了mysql目录，但是还是无法启动MySql，每个目录该给的权限都给了，所有者以及所有组都改成mysql了，但是还是不行，网上各种方法也都试了，最后无果，就在CSDN上面提出了问题，有个热心网友回答了我的问题，他给了一个链接，其实那个链接的文章我看过了，但是看到删除my.cnf的时候我突然想到/etc/mysql/下面也有个my.cnf文件，于是删除了那个文件，启动MySql服务，成功了！ 原文在这里 参考文档： 可能是最好的ss-panel部署教程 安装ss-go-mu版本和ss-panel v3 ShadowsocksR多用户版服务端安装教程 shadowsocks-py-mu 搭建 sspanel v3 魔改版记录 ShadowsocksR 多用户版安装教程]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived + 反向代理 + http服务构建实用性Web站点]]></title>
    <url>%2F2017%2F12%2F13%2Fkeepalived-%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86-http%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA%E5%AE%9E%E7%94%A8%E6%80%A7Web%E7%AB%99%E7%82%B9%2F</url>
    <content type="text"><![CDATA[反向代理又有什么用呢？既然有反向代理，那就肯定有正向代理，先说下正向代理。在我们上网的时候，有时候我们不是直接访问某个网站，而是通过某个代理来访问某个网站，然后，再由该代理将结果返回给我们。 这里，这个代理就是正向代理。也就是说在客户端使用的代理叫做正向代理。而反向代理正好相反。 在服务器端，有些服务器出于安全及负载等考虑，不是让客户端直接访问，而是在服务器端，设置一个代理，将所有用户的请求都先发至该代理，然后，通过代理向服务器端请求相应的资源，然后由该代理向客户端回应。这里的代理就是反向代理。 在网络中正向代理、反向代理如下图所示： 本文主要介绍如何结合keepalived、反向代理和http服务三者一块工作，共同构建一个高效的、实用性的web服务站点。 通过一个图，来介绍下如何构建此架构，以及列举出一些常用的提供反向代理和http服务的技术都有哪些。 从上图可以看到，这里，我将keepalived和反向代理放在同一个主机上实现，后端是提供http服务的主机，采用类似于LVS的NAT模型来实现此架构。 本博客，仅介绍和演示两种提供反向代理及http服务的技术，分别为：nginx、haproxy和apache、nginx。 在前面的博客中，我已经介绍过，keepalived主要用来实现对我们的路由实现高可用，以免当只有一个路由时，因该路由故障导致内网无法与外部通信的问题。这里相关的理论知识就不在介绍了。在LNMP相关博文中，我已经介绍过，nginx除了可以提供http服务之外，还有一个重要的功能，那就是可用来做反向代理，这里，正是将nginx作为反向代理来用，同时也结合nginx提供的http服务一起工作。nginx由于其低资源占用，在处理客户端请求时，一万个并发请求也仅占用2.5M内存空间，因此，可以想象为何众多网站采用nginx做代理来实现了吧。haproxy是专用来提供反向代理的服务软件。 待会在细细介绍haproxy。在LAMP中，已经介绍过apache了，因此，这里也不再过多介绍，仅演示实验。我会在后面的博客中专门介绍apache做代理，后端采用tomcat提供http服务的相关博客，敬请关注。 先介绍下haproxy什么是haproxy呢？先来看下百度百科是如何介绍haproxy的吧。 HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案。HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在时下的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。 HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。多进程或多线程模型受内存限制 、系统调度器限制以及无处不在的锁限制，很少能处理数千并发连接。事件驱动模型因为在有更好的资源和时间管理的用户端(User-Space) 实现所有这些任务，所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。这就是为什么他们必须进行优化以 使每个CPU时间片(Cycle)做更多的工作。 HAProxy是免费、极速且可靠的用于为TCP和基于HTTP应用程序提供高可用、负载均衡和代理服务的解决方案，尤其适用于高负载且需要持久连接或7层处理机制的web站点。 演示实验包括以下两个： 12一、keepalived + nginx反向代理 + apache服务构建Web站点二、keepalived + haproxy反向代理 + apache服务构建Web站点 此处，keepalived用来提供虚拟路由的功能，采用两台主机来实现此功能；nginx反向代理功能与keepalived一起在同一主机上实现。在提供反向代理功能方面，还有另外一个也可以实现反向代理的功能，那就是haproxy，关于haproxy的反向代理功能的实现，会在后面的博客中介绍，敬请关注，这里主要介绍nginx的反向代理功能的实现。 apache用来提供http服务，后端也可以是一个基于LAMP架构的服务器。在上一遍博客中，提供的一张图片显示，apache的市场份额依然占据着领导地位，其强大的市场份额一时半会还没有哪个能够超越，因此，在企业中，基于LAMP架构来提供http服务还是很多的。这里我们就不演示基于LAMP架构来实现此三者的组合了，有兴趣的话读者可参考我的另一篇博客中介绍的方法自行构建一个LAMP，然后加上nginx的反向代理功能，再加上keepalived来提供虚拟路由功能，将三者结合起来一起工作。 实验环境： 本次所示实验都在虚拟机上实现。 123456789系统：RHEL5.8；内核：linux-2.6.18-308.el5；基于keepalived的主从模型实现，后端采用类似于LVS的NAT模型实现。各IP分配如下所示： VIP: 172.16.32.5 DIP: 172.16.32.30 对应主机名为：node1 172.16.32.31 对应主机名为：node2 RIP: 172.16.32.32 对应主机名为：node3 172.16.32.33 对应主机名为：node4 实现集群的前提： 1231、时间同步2、双机互信3、主机名解析 以上三个前提在前面的博客中我已经介绍过如何实现了，这里就不在介绍了，不知道的请参考我以前的博客。现在开始演示我们的实验。 一、keepalived + nginx反向代理 + apache提供的http服务构建Web站点 先看下实现模型图： 在上图中，keepalived和nginx反向代理在同一个主机上实现，后端是apache提供的http服务。 首先，安装好yum源，然后先去提供http服务的两台后端主机node3和node4上安装apache，以提供http服务。 以下操作需在node3和node4上都进行： 123456rpm -q httpd #查看是否安装此软件包，如果没有配置好yum源后安装此软件包yum -y install httpd #安装该软件包vim /var/www/html/index.html #编辑该文件，添加如下两行信息，以提供主页&lt;h1&gt;http://lq2419.blog.51cto.com/&lt;/h1&gt;&lt;h2&gt;Apache node3, IP: 172.16.32.32&lt;/h2&gt; #在node4主机上，修改&lt;h2&gt;Apache node4, IP: 172.16.32.33&lt;/h2&gt;service httpd start #启动httpd服务 在我们的物理机上进行测试，显示效果如下所示： 下面是实现的日志，这里进贴出其中一个主机的日志。 1234567891011tail /var/log/httpd/access_log #在node3查看日志信息，查看显示访问的IP，因为是通过我们的物理机直接访问，所以显示的是物理机IP172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot; 修改httpd的配置文件，对日志显示格式进行修改。 123vim /etc/httpd/conf/httpd.conf #修改该文件LogFormat &quot;%&#123;X-Real-IP&#125;i %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot;&quot; combined #找到该项，将第一个%h修改%&#123;X-Real-IP&#125;i，用于当使用代理访问时显示客户端IP，而不是代理IPservice httpd restart #重启httpd服务 以上修改需在两台后端服务器主机上都进行。为了看效果，你也可以先不修改后端服务器httpd配置文件中日志格式的显示情况，先启动代理服务，使用物理机访问代理主机，然后在服务器主机上查看访问日志，看显示的客户端IP是否正常，是代理IP还是物理机IP。然后再来修改此选项。这里不再演示。 后端的两台http服务器已配置完毕。接着去前端的两台代理主机node1和node2上进行相关的配置。 以下操作在需在node1和node2上都执行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144yum -y install pcre-develtar xf nginx-1.4.1.tar.gzcd nginx-1.4.1./configure \ #执行该命令，各参数相关含义前面博客中已经介绍过，这里就不再介绍了 --prefix=/usr \ --sbin-path=/usr/sbin/nginx \ --conf-path=/etc/nginx/nginx.conf \ --error-log-path=/var/log/nginx/error.log \ --http-log-path=/var/log/nginx/access.log \ --pid-path=/var/run/nginx/nginx.pid \ --lock-path=/var/lock/nginx.lock \ --user=nginx \ --group=nginx \ --with-http_ssl_module \ --with-http_flv_module \ --with-http_stub_status_module \ --with-http_gzip_static_module \ --http-client-body-temp-path=/var/tmp/nginx/client/ \ --http-proxy-temp-path=/var/tmp/nginx/proxy/ \ --http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ \ --http-uwsgi-temp-path=/var/tmp/nginx/uwsgi \ --http-scgi-temp-path=/var/tmp/nginx/scgi \ --with-pcre \ --with-file-aio提供SysV风格的服务脚本：vim /etc/init.d/nginx#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15# description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0nginx=&quot;/usr/sbin/nginx&quot;prog=$(basename $nginx)NGINX_CONF_FILE=&quot;/etc/nginx/nginx.conf&quot;[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginxmake_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125;start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; configtest || return $? stop sleep 1 start&#125;reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125;force_reload() &#123; restart&#125;configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125;rh_status() &#123; status $prog&#125;rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125;case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac赋予其执行权限：chmod +x /etc/init.d/nginxservice httpd stop #如果http服务启动，需先关闭chkconfig httpd off #关闭开机自启动chkconfig --add nginx #添加到服务列表chkconfig nginx on #开机自启动chkconfig --list nginxservice nginx start #启动服务 到此两台主机上的nginx就算是安装好了，我们只需添加相关代理的配置即可。现在直接配置我们的代理，使其能够工作。 123456789101112131415161718vim /etc/nginx/nginx.conf #编辑此配置文件，添加与代理相关的配置 upstream webserver &#123; #在http段，添加新的上下文 server 172.16.32.32 weight=1 max_fails=2 fail_timeout=2; #定义后端服务器，权重为1，最大尝试失败次数为2，失败时两次尝试的超时时长为2 server 172.16.32.33 weight=1 max_fails=2 fail_timeout=2; server 127.0.0.1 backup; #定义当上边两个都down机后，启用本机的服务，只要有一个还提供服务，就不启用此主机的服务 &#125; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; #删除根下所有的默认配置，添加如下两行 proxy_pass http://webserver; #将所有访问根的都转发的webserver proxy_set_header X-Real-IP $remote_addr; #设置转发的头部，此项可实现让后端服务器记录客户端IP，否则记录的是代理的IP，但后端服务器的记录格式也需要相应修改 &#125;同样，将上边配置好的文件发给另一台代理主机：scp /etc/nginx/nginx.conf node2:/etc/nginx #将此配置文件传给另一个代理主机，假如你没有实现主机名解析，请写成相应的IP地址service nginx reload #重新载入nginx服务 知识点补充： upstream模块可定义一个新的上下文，它包含了一组宝岛upstream服务器，这些服务器可能被赋予了不同的权重、不同的类型甚至可以基于维护等原因被标记为down。 1234567891011upstream模块常用的指令有：ip_hash：基于客户端IP地址完成请求的分发，它可以保证来自于同一个客户端的请求始终被转发至同一个upstream服务器；keepalive：每个worker进程为发送到upstream服务器的连接所缓存的个数；least_conn：最少连接调度算法；server：定义一个upstream服务器的地址，还可包括一系列可选参数，如： weight：权重； max_fails：最大失败连接次数，失败连接的超时时长由fail_timeout指定； fail_timeout：等待请求的目标服务器发送响应的时长； backup：用于fallback的目的，所有服务均故障时才启动此服务器； down：手动标记其不再处理任何请求；upstream模块的负载均衡算法主要有三种，轮调(round-robin)、ip哈希(ip_hash)和最少连接(least_conn)三种。默认为轮调。 现在在我们的物理机上在测试下，这次我们在IE浏览器上测试，因为谷歌浏览器有缓存，刷新时看不出来效果。进行测试，看我们的代理能否工作，显示效果如下所示： 同样，启动另一个代理主机的nginx服务，看能否实现代理功能，这里不再演示。 最后去安装keepalived。keepalived我们采用安装rpm包的方式，不在使用源码包，首先，请下载keepalived的相关rpm包，这里使用的是keepalived-1.2.7-5.el5.i386.rpm。yum -y –nogpgcheck localinstall keepalived-1.2.7-5.el5.i386.rpm 因前面已经介绍过keepalived配置文件各参数含义，这里不再介绍，直接配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108vim /etc/keepalived/keepalived.conf #修改其配置文件global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_script chk_nginx &#123; #检查nginx服务是否存在 script &quot;killall -0 nginx&quot; interval 2 weight -2 fall 2 rise 1 &#125;vrrp_script chk_schedown &#123; #用于手动控制keepalived的主从模型 script &quot;[[ -f /etc/keepalived/down ]] &amp;&amp; exit 1 || exit 0&quot; interval 2 weight -2&#125;vrrp_instance VI_1 &#123; state MASTER #设置其为master，在node2主机上设置该项为backup interface eth0 virtual_router_id 232 #设置虚拟路由组ID priority 101 #设置优先级，在node2主机上设置该项为100，因为是backup advert_int 1 authentication &#123; auth_type PASS auth_pass langdu &#125; virtual_ipaddress &#123; 172.16.32.5/16 dev eth0 label eth0:0 &#125; track_script &#123; chk_nginx chk_schedown &#125; notify_master &quot;/etc/keepalived/notify.sh master&quot; #根据检查结果不同，向同一脚本传递不同的参数 notify_backup &quot;/etc/keepalived/notify.sh backup&quot; notify_fault &quot;/etc/keepalived/notify.sh fault&quot;&#125;健康检查脚本如下所示：vim /etc/keepalived/notify.sh #健康检查脚本，在node2主机上也添加次脚本#!/bin/bash# Author: MageEdu &lt;linuxedu@foxmail.com&gt;# description: An example of notify script#vip=172.16.32.5contact=&apos;root@localhost&apos;Notify() &#123; mailsubject=&quot;`hostname` to be $1: $vip floating&quot; mailbody=&quot;`date &apos;+%F %H:%M:%S&apos;`: vrrp transition, `hostname` changed to be $1&quot; echo $mailbody | mail -s &quot;$mailsubject&quot; $contact&#125;case &quot;$1&quot; in master) notify master /etc/rc.d/init.d/haproxy start exit 0 ;; backup) notify backup /etc/rc.d/init.d/haproxy restart exit 0 ;; fault) notify fault exit 0 ;; *) echo &apos;Usage: `basename $0` &#123;master|backup|fault&#125;&apos; exit 1 ;;esacservice keepalived start #启动我们的keepalived服务先看下主节点上的日志信息tail /var/log/messages #查看node1节点上的日志信息May 25 19:37:03 node1 Keepalived_vrrp[3822]: VRRP_Script(chk_schedown) succeededMay 25 19:37:04 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 25 19:37:05 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) setting protocol VIPs.May 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 19:37:06 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 added #添加虚拟IPMay 25 19:37:06 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:11 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5ifconfig #查看node1上各端口的IP配置eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AF inet addr:172.16.32.30 Bcast:172.16.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:655564 errors:7 dropped:0 overruns:0 frame:0 TX packets:66292 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:76104124 (72.5 MiB) TX bytes:9021082 (8.6 MiB) Interrupt:59 Base address:0x2000eth0:0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AF #添加有虚拟IP inet addr:172.16.32.5 Bcast:0.0.0.0 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 Interrupt:59 Base address:0x2000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:2472 errors:0 dropped:0 overruns:0 frame:0 TX packets:2472 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:227776 (222.4 KiB) TX bytes:227776 (222.4 KiB) 为了查看效果，我们依然在IE浏览器上查看。 我们知道，配置keepalived就是想实现地址漂移的，那IP到底会不会漂移呢，现在我们去试一下。在keepalived的相关配置文件中，提到，只需在/etc/keepalived目录下创建一个down文件即可实现手动漂移。现在我们创建个文件，尝试一下。 在master主机上，这里是node1，请查看你的哪台主机是master，创建一个down文件。 1234567891011121314151617181920212223touch /etc/keepalived/down #创建该文件，用以实现手动实现IP地址漂移tail /var/log/messages #查看node1的日志信息May 25 19:37:06 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:06 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:11 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 20:41:02 node1 Keepalived_vrrp[3822]: VRRP_Script(chk_schedown) failedMay 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Received higher prio advertMay 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Entering BACKUP STATE #进入backup状态May 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) removing protocol VIPs.May 25 20:41:03 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 removed #实现IP漂移May 25 20:41:03 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 removed现在去查看下node2主机上的日志信息，看是否是master。tail /var/log/messagesMay 25 19:37:15 node1 Keepalived_vrrp[17601]: Netlink reflector reports IP 172.16.32.5 removedMay 25 20:41:15 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 25 20:41:16 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) setting protocol VIPs.May 25 20:41:17 node1 Keepalived_healthcheckers[17600]: Netlink reflector reports IP 172.16.32.5 added #添加虚拟IPMay 25 20:41:17 node1 avahi-daemon[3375]: Registering new address record for 172.16.32.5 on eth0.May 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 20:41:17 node1 Keepalived_vrrp[17601]: Netlink reflector reports IP 172.16.32.5 addedMay 25 20:41:22 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5 实现IP漂移回来就不在演示了，可自己删除创建的文件，然后查看日志信息。从上面两台主机的日志信息，我们已经看到，可以实现IP地址的漂移，并且在物理机上刷新界面显示也正常。 到此，keepalived + nginx反向代理 + apache构建Web站点我们已经实现。 keepalived + haproxy反向代理 + apache提供的http服务构建Web站点 先看构建的模型图： 还是在我们刚才的主机上进行。但是有一点不好的是，rhel5.8上其yum源里没有自带的haproxy安装包，所以，这里我们采用下载源码包，自己编译安装的方式安装haproxy，本人也是第一次源码编译安装，以前是在rhel6.4上采用rpm包安装的，所以，在源码编译安装的过程中，肯定会遇到各种问题，如果有必要我会将遇到的问题一并贴出来，与大家分享，也好解决当你遇到同样问题时不知如何办才好。好了，废话不多说，开始安装我们的haproxy吧。 在node1和node2主机上关机keepalived和nginx服务。 123456789service keepalived stop #关闭keepalived服务service nginx stop #关闭nginx服务chkconfig nginx off #关闭开机自启动然后去下载我们的haproxy，这里采用haproxy-1.4.22版本，下载地址为http://haproxy.1wt.eu/#downtar xf haproxy-1.4.22.tar.gzcd haproxy-1.4.22uname -r #确定自己的内核版本是多少，这里我的内核是linux-2.6.18-308.el5，所以下边使用linux26make TARGET=linux26 PREFIX=/usr/local/ make install PARFIX=/usr/local/ 注：进入该haproxy-1.4.22目录后，你会发现这里并没有我们常见的configure文件，所以不需要执行./configure命令。TARGET用于指定内核版本，PREFIX用于指定安装文件路径，虽然指定了文件路径，但在该目录下并不会生成该文件，此处还有一点需要注意，后边两个选项一定要写成大写，本人试了，小写会报错。 不管你信不信，反正我是信了。要不你试试. 为haproxy提供配置文件和服务脚本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798mkdir /etc/haproxy #创建该目录，用来存放我们的配置文件cp examples/haproxy.cfg /etc/haproxy/ #复制该文件当我们的配置文件，但是里边好多需要修改提供SysV风格服务脚本：vim /etc/init.d/haproxy #编辑该文件，添加服务脚本#!/bin/sh## haproxy## chkconfig: - 85 15# description: HAProxy is a free, very fast and reliable solution \# offering high availability, load balancing, and \# proxying for TCP and HTTP-based applications# processname: haproxy# config: /etc/haproxy/haproxy.cfg# pidfile: /var/run/haproxy.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0exec=&quot;/usr/local/sbin/haproxy&quot; #在执行完make install之后，会出现此选项，我们可以直接执行haproxy命令prog=$(basename $exec) #说白了，其实就是让取haproxy，不信，待会启动报错时你就会发现了[ -e /etc/sysconfig/$prog ] &amp;&amp; . /etc/sysconfig/$proglockfile=/var/lock/subsys/haproxy #锁文件check() &#123; $exec -c -V -f /etc/$prog/$prog.cfg #知道为何创建刚才的那个haproxy目录了，如果不创建此目录，这里就需要修改了。假如你没有创建刚才的目录，请相应修改服务脚本中的选项&#125;start() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi echo -n $&quot;Starting $prog: &quot; # start it up here, usually something like &quot;daemon $exec&quot; daemon $exec -D -f /etc/$prog/$prog.cfg -p /var/run/$prog.pid retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $&quot;Stopping $prog: &quot; # stop it here, often &quot;killproc $prog&quot; killproc $prog retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi stop start&#125;reload() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi echo -n $&quot;Reloading $prog: &quot; $exec -D -f /etc/$prog/$prog.cfg -p /var/run/$prog.pid -sf $(cat /var/run/$prog.pid) retval=$? echo return $retval&#125;force_reload() &#123; restart&#125;fdr_status() &#123; status $prog&#125;case &quot;$1&quot; in start|stop|restart|reload) $1 ;; force-reload) force_reload ;; check) check ;; status) fdr_status ;; condrestart|try-restart) [ ! -f $lockfile ] || restart ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|try-restart|reload|force-reload&#125;&quot; exit 2esac 此服务脚本是参考rhel6.4上的haproxy的服务脚本，稍加改动而成的。假如你不想源码编译安装haproxy的话，也可以在rhel6.4上进行此实验。但是rhel6.4上有个地方需要修改，待会再说。 123chmod +x /etc/init.d/haproxychkconfig --add haproxychkconfig haproxy on 先不要启动haproxy服务，先去修改配置文件，将其修改成我们需要的。先介绍下haproxy配置文件中的选项含义 haproxy的配置文件中的选项可分为两个类： 12“global”配置段，用于设定全局配置参数；proxy相关配置段，如“defaults”、“listen”、“frontend”和“backend”； 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于keepalived(主从+双主) + LVS(DR模型) + DNS实现http高可用集群服务]]></title>
    <url>%2F2017%2F12%2F13%2F%E5%9F%BA%E4%BA%8Ekeepalived-%E4%B8%BB%E4%BB%8E-%E5%8F%8C%E4%B8%BB-LVS-DR%E6%A8%A1%E5%9E%8B-DNS%E5%AE%9E%E7%8E%B0http%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[什么是keepalived呢？keepalived是实现高可用的一种轻量级的技术手段，主要用来防止单点故障(单点故障是指一旦某一点出现故障就会导致整个系统架构的不可用)的发生。之所以说keepalived是轻量级的，是相对于corosync + ldirectord来说的。 keepalived也可以实现高可用集群，而且配置起来比corosync + ldirectord简单方便很多，keepalived与corosync的工作机制相差很多。 corosync + ldirectord实现的功能虽然强大，但配置起来比较麻烦，而keepalived功能虽然简单，但配置起来比较容易。 也就是说keepalived可实现corosync + ldirectord实现的功能，只不过前者没有后者功能强大而已。这里主要介绍keepalived。 在介绍keepalived之前，不得不先介绍下一个协议——VRRP。之所以要介绍这个协议，是因为==VRRP协议是keepalived实现的基础==。 下面先来一块看下这个这协议是干吗用的吧。 如上图所示，通常，同一网段内的所有主机都设置一条相同的、以网关为下一跳的缺省路由。 主机发往其他网段的报文将通过缺省路由发往网关，再由网关进行转发，从而实现主机与外部网络的通信。 当网关发生故障时，本网段内所有以网关为缺省路由的主机将无法与外部网络通信，仅能实现内部主机间通信。 缺省路由为用户的配置操作提供了方便，但是对缺省网关设备提出了很高的稳定性要求。 增加出口网关是提高系统可靠性的常见方法，此时如何在多个出口之间进行选路就成为需要解决的问题。而VRRP正好解决了此问题。 VRRP：Virtual Router Redundancy Protocol，虚拟路由冗余协议。 VRRP说白了就是实现地址漂移的，是一种容错协议，在提高可靠性的同时，简化了主机的配置。 该协议能够实现将可以承担网关功能的一组路由器加入到备份组中，形成一台虚拟路由器，由VRRP的选举机制决定哪台路由器承担转发任务，局域网内的主机只需将虚拟路由器配置为缺省网关。 在VRRP协议出现之前，为了不让单个路由器成为本地与外部通信的瓶颈，我们需要有多个路由，在此种模式下，我们内部的主机就需要将自己的网关指向不同的路由器，这样的配置对我们的网关管理员来说是很麻烦的，且不容易实现。 在VRRP协议出现后，为了不让单个路由器成为本地与外部通信的瓶颈，我们仍需要有多个路由，但可以使用同一个缺省网关，我们只需将内部主机指定一个缺省网关即可。VRRP协议会根据优先级来选择一个正常的路由作为主路由器实现与外部的通信，而其他路由则作为备份路由不参与转发。 在此模式下，多个路由器组成虚拟路由器组，物理上是多个路由器组成，但在逻辑上却是表现为只有一个路由。 效果如下图所示： 在上图中，Router A、Router B和Router C组成一个虚拟路由器。 各虚拟路由器都有自己的IP地址。局域网内的主机将虚拟路由器设置为缺省网关。 Router A、Router B和Router C中优先级最高的路由器作为Master路由器，承担网关的功能。其余两台路由器作为Backup路由器。当master路由器出故障后，backup路由器会根据优先级重新选举新的master路由器承担网关功能。 Master 路由器周期性地发送VRRP 报文，在虚拟路由器中公布其配置信息（优先级等）和工作状况。Backup路由器通过接收到VRRP 报文的情况来判断Master 路由器是否工作正常。 VRRP根据优先级来确定备份组中每台路由器的角色（Master 路由器或Backup 路由器）。 优先级越高，则越有可能成为Master 路由器。VRRP优先级的可配置的取值范围为1 到254。 为了防止非法用户构造报文攻击备份组，VRRP通过在VRRP报文中增加认证字的方式，验证接收到的VRRP报文。 VRRP提供了两种认证方式： simple：简单字符认证。发送VRRP 报文的路由器将认证字填入到VRRP 报文中，而收到VRRP 报文的路由器会将收到的VRRP 报文中的认证字和本地配置的认证字进行比较。如果认证字相同，则认为接收到的报文是真实、合法的VRRP 报文；否则认为接收到的报文是一个非法报文。 md5：MD5 认证。发送VRRP 报文的路由器利用认证字和MD5 算法对VRRP 报文进行摘要运算，运算结果保存在Authentication Header（认证头）中。收到VRRP报文的路由器会利用认证字和MD5 算法进行同样的运算，并将运算结果与认证头的内容进行比较。如果相同，则认为接收到的报文是真实、合法的VRRP 报文；否则认为接收到的报文是一个非法报文。 在有多个路由器组成的虚拟路由中，当我们的内部主机很多时，如果所有主机都使用同一个master路由，会使得其他路由器很清闲，很浪费资源，我们期望我们本地的内部主机平分到各个路由器上，即让我们的内部主机的缺省网关指向不同的路由，从而减轻因只有一个master路由而造成网络带宽拥堵的负担。 这就是负载分担VRRP。但这个如何实现呢？先看下面的配置效果图： 在此情况下，同一台路由器同时加入多个VRRP备份组，在不同备份组中有不同的优先级，从而实现负载分担。 在上图中，有三个备份组存在： 备份组1：对应虚拟路由器1。Router A作为Master路由器，Router B和Router C作为Backup路由器。备份组2：对应虚拟路由器2。Router B作为Master路由器，Router A和Router C作为Backup路由器。备份组3：对应虚拟路由器3。Router C作为Master路由器，Router A和Router B作为Backup路由器。 为了实现业务流量在Router A、Router B和RouterC之间进行负载分担，需要将局域网内的主机的缺省网关分别设置为虚拟路由器1、2和3。在配置优先级时，需要确保三个备份组中各路由器的VRRP优先级形成交叉对应。 为了便于理解，这里给出一张表，我们假定有三个路由设备Router A、B、C和三台主机Host A、B、C，列举有在不同的虚拟路由组中，各路由器所具有的VRRP优先级如下表所示。 从上表可以看到，各路由分别属于不同的虚拟路由组。 对路由器A来说，因在虚拟路由组1中Router A的优先级高于另外两个，因此，Router A 作为 Master 路由器，Router B 和Router C 作为 Backup路由器； 同样，对路由器B来说，因在虚拟路由器组2中Router B的优先级高于另外两个，因此，Router B 作为 Master 路由器，Router A 和Router C 作为 Backup路由器； 对路由器C来说，因在虚拟路由器组3中Router C的优先级高于另外两个，因此，Router C 作为 Master 路由器，Router A 和Router B 作为 Backup路由器。 对不同的主机来说，一旦其master路由器出故障后，会在另外正常的路由器中根据优先级重新选定master路由。 如这里假定Host A的默认网关指向Router A，即Host A指向虚拟路由器组1的默认网关，对主机A来说，如果其master路由出现故障，即Router A出现故障，则会从另外两个正常的备份虚拟路由中根据各自的优先级选取高优先级的作为新的master路由，这里就是选取Router B作为其master路由来完成网关功能。 假如想了解更多关于VRRP协议相关的信息请查阅相关资料，这里不再过多介绍。 在文章开始我就提到，keepalived实现的基础就是基于VRRP协议，上边介绍了那么多关于VRRP协议的相关知识，不知道你是否已经猜到keepalived与VRRP协议到底有什么关系。 keepalived设计之初就是为LVS提供高可用集群的，下面给出一个keepalived官方给的设计图： 从上图可以看出，keepalived包括三个组件：IO复用组件、内存管理组件和控制组件。 在核心组件中IPVS wrapper就是负责生成ipvs规则的，所以说keepalived设计之初就是为ipvs也就是LVS提供高可用集群功能的。 早期的keepalived实现的共能很简单，一般只需三个功能：一个是将IP地址转移到其他节点上，一个就是在另一个主机上生成ipvs规则，最后一个就是健康状况检查。那如何实现IP地址漂移呢，或者说如何实现IP地址转移的呢？这就需要借助于VRRP协议实现了。 keepalived通过软件的方式在其内部模拟实现VRRP协议，然后借助于VRRP协议实现IP地址漂移。现在知道为什么花那么多篇幅介绍VRRP协议了吧。 在下面配置keepalived的时候，还会用到VRRP协议的相关知识，因此一定要清楚的理解VRRP的工作机制，否则在下面配置keepalived的时候你可能会眼花缭乱。 从上边的设计图我们就可以看到，keepalived的出生就是为lvs提供高可用集群服务的，因此，采用keepalived为lvs提供高可用集群服务，配置起来比较简单方便。 在上边我一直强调keepalived设计之初是为lvs提供高可用集群服务的，现在假如我只想通过keepalived为某个服务，不再为lvs高可用服务，比如说通过两台主机为web服务提供高可用服务，该怎么办呢，或者说能否实现呢？ 上边提到，keepalived实现的功能主要有三个，既然我们不为lvs提供高可用服务了，那生成ipvs规则和健康状况检查就不需要了，仅需要提供IP地址转移即可。因此，采用keepalived来实现IP地址转移，我们仍然可以实现为web服务提供高可用服务。 但又遇到另一个问题，我们进实现了IP地址漂移，那我们的web服务怎么办呢，如何实现高可用呢，不能进实现IP地址转移啊，那样的话只能说是IP地址高可用，不能说是web服务高可用？其实现在的keepalived还有其他额外的功能。 keepalived可以通过调用外部脚本的功能，来监控外部其他资源。在keepalived的配置文件中，一般都会有包含如下三行信息： 123vrrp_script &quot;killall -0 SERVICE_NAME&quot;notify-master &quot;script&quot;notify-backup &quot;script&quot; 第一行仅仅实现改变优先级，当IP实现漂移后，在另一个节点上检查是否有该服务，如果有该服务，尝试杀死该服务时会返回正确结果，如果没有该服务会返回错误结果，然后根据返回的结果来修改自身的优先级； 第二行表示，如果是主节点，执行相应的脚本，启动相应的服务； 第三行表示，如果是从节点，执行相应的脚本，停止相应的服务。但是这个脚本需要自己写，这是keepalived中比较麻烦的一个问题。 在下面keepalived的配置文件中会详细介绍如何调用外部脚本来监控其他外部资源。这里不再具体阐述，读者仅仅有个印象即可。枯燥的理论知识终于介绍完毕，你是不是有种解脱的感觉。好吧，下面跟着我一块来配置keepalived吧。 实验场景：在VMware上安装RedHat5.8，内核为Linux-2.6.18，这里模拟实现http服务的高可用，采用两台主机做keepalived高可用，另外两台做http高可用服务集群，安装的系统均为RedHat5.8，内核为Linux-2.6.18。 123keepalived高可用主机IP：172.16.32.30和172.16.32.31http服务高可用主机IP：172.16.32.32和172.16.32.33VIP采用172.16.32.5 这里采用LVS的DR模型来实现集群服务。所以keepalived只需一个网卡即可。假如你想做NAT模型，请添加第二块网卡。LVS的相关理论知识会在以后的博客中介绍。 关闭四台主机的selinux。在命令行界面执行setenforce 0可关闭selinux，否则会对我们的测试有影响。 关闭iptables防火墙。 各虚拟机及主机名和IP对应关系如下所示： 123456虚拟机 主机名 IP地址 HA1 node1.langdu.com 172.16.32.30 HA2 node2.langdu.com 172.16.32.31 HA3 node3.langdu.com 172.16.32.32 HA4 node4.langdu.com 172.16.32.33 知识点补充： 集群(Cluster)类型： 123LB：Load Balancing，负载均衡集群，以提高服务的并发能力为着眼点HA：High Available，高可用集群，以提高服务可用性为着眼点HP(HPC)：High Performance，高性能集群，以提高服务系统处理性能为着眼点 对于高可用集群来说，为避免集群分裂，一个高可用集群至少要有3个节点，或奇数个节点。这里因硬件的限制，我仅用了两个节点，在实际使用中最好使用奇数个节点。既然是高可用集群了，那可用性如何计算呢？这里给出一个计算公式：可用性=在线时间/(在线时间+故障处理时间)可能用到的各IP简写： 1234CIP：Client IPVIP：virtual IPDIP：Director IPRIP：realserver IP LVS类型： 1234LVS-NAT：地址转换LVS-DR：直接路由LVS-TUN：隧道 各类型需遵循的法则： LVS-NAT： 1234567集群节点跟director必须在同一个IP网络中 RIP地址通常是私有地址，仅用于各集群节点间通信 director位于client和reals erver之间，并负责处理进出的所有通信 realserver必须将网关指向DIP 支持端口映射 realserver可以使用任意OS 较大规模应用场景中，director易成为系统瓶颈 LVS-DR： 123456此模型下VIP地址配置在realserver的网卡别名上，通常情况下是隐藏的。 集群节点跟director必须在同一个物理网络中 RIP可以使用公网地址，实现便捷的远程管理和监控 director仅负责处理入站请求，响应报文则由realserver直接发往客户端 realserver不能将网关指向DIP 不支持端口映射 LVS-TUN 123456集群节点可以跨越互联网 RIP必须是公网IP地址 director仅负责处理入站请求，响应报文则由realserver直接发往客户端 realserver不能将网关指向director 只有支持隧道功能的OS才能用于realserver 不支持端口映射 Director调度策略： 静态调度 123456rr：轮叫，又称轮询 wrr：Weight rr，加权轮询 sh：source hashing，源地址hash 实现会话绑定：session affinity ssession sharing：会话共享 dh：destination hash：目标地址hash 动态调度 1234567891011lc：最少连接 active*256+inactive 谁的小，挑谁 wlc：加权最少连接 (active*256+inactive)/weight 谁的小，挑谁 sed：最短期望延迟 (active+1)*256/weight nq：never queue，永不排队 LBLC：基于本地的最少连接 LBLCR：基于本地的带复制功能的最少连接 安装完ipvsadm后默认方法：wlc LVS-DR模型： 123456789kernel parameter：arp_announce：定义将自己的地址向外通告时的通告级别 0：将本机任何接口上的任何地方向外通告(默认为0) 1：试图仅想目标网络通告与其网络匹配的地址 2：仅将与本地接口上地址匹配的网络进行通告arp_ignore：定义接受到ARP请求时的响应级别 0：只要本地配置的有相应地址，就给予响应(默认为0) 1：仅在请求的目标地址配置请求到达的接口上的时候，才给予响应 集群相关基础性知识基本就这些了。还有其他的这里就暂不介绍了，相关更多知识会在以后的集群博客中介绍。这里知识帮助理解。 实现集群的前提： 1231、时间同步，这里采用cron任务计划实现；2、主机名解析，最好通过配置/etc/hosts实现，不要使用DNS来实现，这里通过配置/etc/hosts来实现；3、双机互信。 首先，解决时间同步的问题： 1234service ntpd stop #在四台主机上关闭本机上时间同步的服务，因我们常挂起虚拟机，各虚拟机时间可能不相同，因此我们不使用系统时间同步的服务ntpdate 172.16.0.1#在各主机上执行该命令，同步时间与该地址的时间相同，该地址最好可以上网，这里是与本地网关服务器的时间同步crontab -e #在各主机上使用该命令添加cron任务，每5分钟同步一下时间，同步后不管输出什么信息都送到/dev/null中，否则，你会每5分钟收到一封邮件*/5****/sbin/ntpdate 172.16.0.1&amp;&gt; /dev/null 然后，配置主机名解析：我们先来修改各主机名： 123hostname node1.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.30的主机，修改其主机名HOSTNAME=node1.langdu.com #修改主机名 同样在其他主机上也修改主机名。 1234567891011121314hostname node2.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.31的主机，修改其主机名HOSTNAME=node2.langdu.com #修改主机名hostname node3.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.32的主机，修改其主机名HOSTNAME=node3.langdu.com #修改主机名hostname node4.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.33的主机，修改其主机名HOSTNAME=node4.langdu.com #修改主机名vim /etc/hosts #在IP为172.16.32.30的主机上，编辑该文件，添加如下四行信息172.16.32.30node1.langdu.com node1172.16.32.31node2.langdu.com node2172.16.32.32node3.langdu.com node3172.16.32.33node4.langdu.com node4 在node1主机上使用scp命令将该文件传给其他三台主机： 123scp /etc/hosts 172.16.32.31:/etc/#执行该命令时，需输入各主机的密码，可能有点麻烦scp /etc/hosts 172.16.32.32:/etc/scp /etc/hosts 172.16.32.33:/etc/ 最后，实现双机互信：因我们这里是两台主机做keepalived，两台主机做http，因此，我们将前两台实现双机互信，后两台实现双机互信，即node1和node2双机互信，node3和node4双机互信。首先在node1上： 12ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.31#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 接着在node2上： 12ssh-keygen -t rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.30#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 这样，以后在node1和node2之间通信时，我们就不需要输入密码了ssh node2 ‘ifconfig’#在node1主机上，使用该命令，查看下是否已实现双机互信同样在另外两台主机上也需要执行上述命令在node3主机上，执行如下命令： 12ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.33#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 接着在node4上： 12ssh-keygen -t rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.32#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 准备工作已经完毕，现在来安装我们的keepalived软件包。这里我们通过安装rpm包来实现，有兴趣的读者也可以从网上下载源码包，自己编译安装。本rpm是经过源码编译制作的rpm包，相关的脚本及示例配置文件都已制作进来，所以，直接在本地安装即可。假如你是从网上下载的rpm包，里边是没有示例配置文件和相关脚本的，需要自己写。 首先准备好我们的yum源，解决依赖关系时有可能用到里边的相关rpm包。在node1上，使用如下命令完成安装，同样在node2上也需要安装该软件包，因node1和node2实现keepalived高可用。 12345678910yum -y --nogpgcheck localinstall keepalived-1.2.7-5.el5.i386.rpm #因rpm我们以下载至本地，所以采用本地安装，别忘了在另一台主机上也安装该软件包rpm -ql keepalived #使用该命令查看安装的rpm生成哪些文件，这里只贴出生成的部分文件/etc/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.haproxy_example #配置示例文件，网上下载的rpm包安装后没有该文件/etc/keepalived/notify.sh #脚本文件，同样网上下载安装的也没有该脚本文件/etc/rc.d/init.d/keepalived/etc/sysconfig/keepalived/usr/bin/genhash/usr/sbin/keepalived 一起来看下配置文件里的内容： 1234/etc/keepalived/keepalived.conf配置文件中，可分为三个部分：global_defs #全局配置部分vrrp_instance VI_1 #vrrp实例，用来定义虚拟路由组virtual_server 192.168.200.100 443 #虚拟服务部分，用来定义LVS相关配置的 各部分配置的各参数含义： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253global_defs &#123;notification_email &#123; #通知邮件收件人acassen@firewall.locfailover@firewall.locsysadmin@firewall.loc&#125;notification_email_from Alexandre.Cassen@firewall.loc #定义通知邮件的来源smtp_server 192.168.200.1smtp_connect_timeout 30router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; #vrrp实例部分state MASTER #定义初始状态下虚拟路由状态interface eth0 #定义配置在哪个端口上virtual_router_id 51#虚拟路由组ID号priority 100#优先级advert_int 1#每隔1秒进行通告authentication &#123; #实现认证auth_type PASS #采用字符认证auth_pass 1111#认证需要的字符串，最好是随机生成的，两边主机需一样&#125;virtual_ipaddress &#123; #在对应接口上配置虚拟IP，可根据需要进行添加或删除192.168.200.16192.168.200.17192.168.200.18&#125;&#125;virtual_server 192.168.200.100443&#123; #虚拟服务部分，虚拟IP和端口分别是多少，delay_loop 6#获取服务时的等待时间lb_algo rr #集群调度策略，默认为轮询，可自行修改lb_kind NAT #集群转发方式nat_mask 255.255.255.0#虚拟IP的掩码persistence_timeout 50#集群持久连接超时时长，不想支持持久连接可去掉该项protocol TCP #协议为TCPsorry_server 192.168.200.2001358#定义当real_server都down之后，该怎么办real_server 192.168.201.100443&#123; #真实后台服务器IP及端口号，当有多个RIP时可出现多次weight 1#该服务器的权重SSL_GET &#123; #采用SSL进行健康检查，还有其他方式也可实现健康检查url &#123;path /#通过443端口到指定路径下获取相关服务digest ff20ad2481f97b1754ef3e12ecd3a9cc #摘要码&#125;url &#123;path /mrtg/status_code 200#状态码，访问正常时状态码为200&#125;connect_timeout 3#定义多长时间检查一次nb_get_retry 3#检查不健康后，重试次数delay_before_retry 3#多长时间重试一次&#125;&#125;&#125; keepalived配置文件主要内容已经介绍完毕。 想要了解更多keepalived配置文件的信息可使用该命令来查看： 1man keepalived.conf 接下来我们先去配置好http高可用服务两台主机，然后再回来配置keepalived。 既然是LVS的DR模型，那我们先来配置另外两台主机，来实现http高可用集群。 首先，打开另外两外两台提供http服务的虚拟机，这里是HA3和HA4。 为了方便你也可以改为RS1和RS2。先在HA3上执行如下命令： 1234567rpm -q httpd #确保已经安装过httpd软件包，如果没有请自行安装该软件包echo &quot;&lt;h1&gt;RS1.langdu.com&lt;/h1&gt;&quot; &gt; /var/www/html/index.html #为http提供主界面service httpd start #启动服务同样，在HA4虚拟机上也执行同样的命令，但主界面要换成相应的命令。rpm -q httpd #确保已经安装过httpd软件包，如果没有请自行安装该软件包echo &quot;&lt;h1&gt;RS2.langdu.com&lt;/h1&gt;&quot; &gt; /var/www/html/index.html #为http提供主界面service httpd start #启动服务 然后在我们的物理机上尝试访问这两个IP，看是否可以访问。 在DR模型中，只添加个http服务还不行，还有许多需要修改，貌似不是很简单，好吧，为了节约时间，这里我们通过一个脚本实现修改各个数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bash## Script to start LVS DR real server.# chkconfig: - 90 10# description: LVS DR real server#. /etc/rc.d/init.d/functionsVIP=172.16.32.5#定义VIPhost=`/bin/hostname`case &quot;$1&quot;instart)# Start LVS-DR real server on this machine./sbin/ifconfig lo down/sbin/ifconfig lo upecho 1&gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2&gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho 1&gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 2&gt; /proc/sys/net/ipv4/conf/all/arp_announce/sbin/ifconfig lo:0$VIP broadcast $VIP netmask 255.255.255.255up/sbin/route add -host $VIP dev lo:0;;stop)# Stop LVS-DR real server loopback device(s)./sbin/ifconfig lo:0downecho 0&gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho 0&gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho 0&gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 0&gt; /proc/sys/net/ipv4/conf/all/arp_announce;;status)# Status of LVS-DR real server.islothere=`/sbin/ifconfig lo:0| grep $VIP`isrothere=`netstat -rn | grep &quot;lo:0&quot;| grep $VIP`if[ ! &quot;$islothere&quot;-o ! &quot;isrothere&quot;];then# Either the route or the lo:0 device# not found.echo &quot;LVS-DR real server Stopped.&quot;elseecho &quot;LVS-DR real server Running.&quot;fi;;*)# Invalid entry.echo &quot;$0: Usage: $0 &#123;start|status|stop&#125;&quot;exit 1;;esac 记得该脚本需要在另一个主机上也要执行一下。 两台主机上都执行过上边的脚本后，验证下各参数是否已经修改：下面几个命令均在node3主机上执行，可在node3上使用ssh node4 ‘COMMAND’来验证下node4上各参数是否已修改 123456789101112131415161718192021222324252627282930ifconfig #在node3上执行该命令，查看是否有VIP，使用ssh node4 &apos;ifconfig&apos;命令查看node4上是否也有VIPeth0 Link encap:Ethernet HWaddr 00:0C:29:7F:8F:44inet addr:172.16.32.33Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:162748errors:0dropped:0overruns:0frame:0TX packets:2368errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:26502458(25.2MiB) TX bytes:200681(195.9KiB)Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b)lo:0Link encap:Local Loopbackinet addr:172.16.32.5Mask:255.255.255.255UP LOOPBACK RUNNING MTU:16436Metric:1route -n #查看是否有配置的VIP特定路由Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.32.50.0.0.0255.255.255.255UH 000lo169.254.0.00.0.0.0255.255.0.0U 000eth0172.16.0.00.0.0.0255.255.0.0U 000eth00.0.0.0172.16.0.10.0.0.0UG 000eth0cat /proc/sys/net/ipv4/conf/all/arp_ignore1cat /proc/sys/net/ipv4/conf/all/arp_announce2 到此，我们的两台RealServer都以配置完毕，而且其http服务也已正常工作。 现在去编辑我们的keepalived的配置文件，并修改成我们所需要的。 先在node1主机上进行修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137vim /etc/keepalived/keepalived.confglobal_defs &#123;notification_email &#123;root@localhost #有通告信息时将邮件发给管理员&#125;notification_email_from keepalived@localhost #通告邮件来自哪里smtp_server 127.0.0.1smtp_connect_timeout 30router_id LVS_DEVEL&#125;vrrp_script chk_httpd &#123; #定义该vrrp脚本，上边应提到keepalived就是靠这些脚本来实现其相关功能的script &quot;killall -0 httpd&quot;#尝试杀死该服务，但不是真正杀死该服务，仅为了测试该服务是否启动而已interval 2# check every 2 secondsweight -2# if failed, decrease 2 of the priorityfall 2# require 2 failures for failuresrise 1# require 1 sucesses for ok&#125;vrrp_script chk_schedown &#123; #定义该vrrp脚本，来实现手动转移IP地址，待会演示效果script &quot;[[ -f /etc/keepalived/down ]] &amp;&amp; exit 1 || exit 0&quot;#如果有这个文件，则返回1，否则返回0interval 2weight -2#优先级减2&#125;vrrp_instance VI_1 &#123;state MASTER #定义该主机为master路由interface eth0virtual_router_id 132#定义虚拟路由组ID号，同一网段内不要使用相同的组ID，否则会报错priority 101#定义其优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu #字符串认证时使用的字符串，可自行修改，但要保证两个keepalived主机上的字符串相同&#125;virtual_ipaddress &#123;172.16.32.5/16dev eth0 label eth0:0#定义VIP，并制定设备和别名&#125;track_script &#123; #健康检查脚本chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master&quot;#如果是master路由器，传递master参数notify_backup &quot;/etc/keepalived/notify.sh backup&quot;#如果是backup路由，传递backup参数notify_fault &quot;/etc/keepalived/notify.sh fault&quot;#如果失败了，传递fault参数&#125;virtual_server 172.16.32.580&#123; #定义虚拟服务器，因我们测试的是http服务，所以端口为80delay_loop 6lb_algo rrlb_kind DRnat_mask 255.255.0.0#该掩码为虚拟服务器的掩码# persistence_timeout 50 #为了待会刷新界面时查看效果，这里我没有启用持久连接，而是将其注释掉了protocol TCPreal_server 172.16.32.3280&#123; #定义RIP和端口号weight 1#权重，在rr调度方式下，该值没有实际意义HTTP_GET &#123; #使用HTTP进行健康检查，假如你使用的是https服务，就需要使用基于SSL的健康检查url &#123;path /status_code 200#状态码&#125;connect_timeout 2nb_get_retry 3delay_before_retry 2&#125;&#125;real_server 172.16.32.3380&#123; #指定另一个RIP和端口号，上边已经提到，当有多个realserver时，该项可以出现多次weight 2HTTP_GET &#123;url &#123;path /status_code 200&#125;connect_timeout 2nb_get_retry 3delay_before_retry 3&#125;&#125;&#125;下面这个脚本时实现健康检查用的。即上边用到的notify.sh脚本。#!/bin/bash# Author: onlyyou# description: An example of notify script#ifalias=$&#123;2:-eth0:0&#125;interface=$(echo $ifalias | awk -F: &apos;&#123;print $1&#125;&apos;)vip=$(ip addr show $interface | grep $ifalias | awk &apos;&#123;print $2&#125;&apos;)contact=&apos;root@localhost&apos;workspace=$(dirname $0)notify() &#123;subject=&quot;$ip change to $1&quot;body=&quot;$ip change to $1 $(date &apos;+%F %H:%M:%S&apos;)&quot;echo $body | mail -s &quot;$1 transition&quot;$contact #实现发送邮件&#125;case &quot;$1&quot;inmaster)notify masterexit 0;;backup)notify backup/etc/rc.d/init.d/httpd restartexit 0;;fault)notify faultexit 0;;*)echo &apos;Usage: $(basename $0) &#123;master|backup|fault&#125;&apos;exit 1;;esacscp /etc/keepalived/keepalived.conf node2:/etc/keepalived/#将配置文件发给另一个keepalived主机，这里发给node2，假如你的主机不是node2，请做相应修改在node2主机上，修改刚传过来的keepalived的配置文件。这里只需修改两项即可：vim /etc/keepalived/keepalived.confvrrp_instance VI_1 &#123;state BACKUP #设置该node2主机为backup路由interface eth0virtual_router_id 132priority 100#设定node2主机的优先级为100，低于node1advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.5/16dev eth0 label eth0:0&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master&quot;notify_backup &quot;/etc/keepalived/notify.sh backup&quot;notify_fault &quot;/etc/keepalived/notify.sh fault&quot;&#125; 在node2主机上，我们只需修改上述两项即可。 修改完成后保存退出，接下来就可以启动keepalived服务了。 在上边给的keepalived设计图中我们已经看到，keepalived是为ipvs提供高可用服务的，并且会生成ipvs规则，但需要我们事先安装ipvsadm软件包。好吧，现在我们去安装ipvsadm软件包，然后再启动keepalived服务。 12ssh node2 &apos;yum -y install ipvsadm&apos; #通过node1主机在node2上安装ipvsadm软件包yum -y install ipvsadm #在node1上安装软件包 安装完毕后启动我们的keepalived服务。 12service keepalived start #启动node1上的keepalived服务ssh node2 &apos;service keepalived start&apos; #在node1上启动node2的keepalived服务 我们先来看下我们的日志，看有没有记录什么信息。 12345678910111213141516171819202122232425262728293031323334353637383940tail /var/log/messages #查看日志May 1618:19:22node1 Keepalived_vrrp[789]: Using LinkWatch kernel netlink reflector...May 1618:19:22node1 Keepalived_vrrp[789]: VRRP sockpool: [ifindex(2), proto(112), fd(11,12)]May 1618:19:22node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Transition to MASTER STATE #传输master状态May 1618:19:22node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Received lower prio advert, forcing new electionMay 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) setting protocol VIPs. #设置VIP地址May 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1618:19:23node1 Keepalived_vrrp[789]: Netlink reflector reports IP 172.16.32.5addedMay 1618:19:23node1 Keepalived_healthcheckers[788]: Netlink reflector reports IP 172.16.32.5addedMay 1618:19:28node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5从上边的日志可以看到，我们的配置已经生效。那一起看下ipvs规则吧。ipvsadm -l -n #使用该命令，查看是否有ipvs规则，显示如下：IP Virtual Server version 1.2.1(size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.16.32.5:80rr-&gt; 172.16.32.33:80Route 100-&gt; 172.16.32.32:80Route 100ifconfig #查看下node1上的IP配置情况eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.30Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:290653errors:1dropped:0overruns:0frame:0TX packets:13874errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:46393162(44.2MiB) TX bytes:2014631(1.9MiB)Interrupt:59Base address:0x2000eth0:0Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.5Bcast:0.0.0.0Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b) 配置的VIP也已生效，我们的ipvs规则已经实现。现在在我们的物理机上访问下172.16.32.5，看一下，是否可以访问，显示什么信息吧。 到目前为止，貌似我们还没有实现IP地址漂移。好吧，从上边给的配置文件，可以看出来，我们只需在master路由主机上，在相应目录下创建一个down文件即可实现手动漂移IP地址。 在node1上 1234567891011121314151617181920212223242526272829303132333435363738394041cd /etc/keepalived/#进入该目录touch down #创建该文件，用来实现手动漂移IP地址tail /var/log/messages #停几秒钟后，查看日志May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering MASTER STATEMay 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) setting protocol VIPs.May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:15:32node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Received higher prio advert #收到更高优先级的通告信息May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering BACKUP STATE #进入backup状态May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) removing protocol VIPs. #转移VIP地址May 1619:15:32node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5removedMay 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5removedifconfig #使用该命令，查看下node1主机的VIP是否存在，可看到已转移到其他主机eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.30Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:347881errors:1dropped:0overruns:0frame:0TX packets:21333errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:50760780(48.4MiB) TX bytes:2653767(2.5MiB)Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b)在node2主机上查看其日志。tail /var/log/messages #查看node2的日志信息May 1619:15:32node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:15:32node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:15:33node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) setting protocol VIPs. #设定VIPMay 1619:15:34node1 Keepalived_healthcheckers[2463]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:34node1 avahi-daemon[3375]: Registering new address record for172.16.32.5on eth0.May 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:15:34node1 Keepalived_vrrp[2464]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:39node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5 在在我们的物理机访问下，看能否访问。依然在浏览器地址栏输入172.16.32.5，可以看到，访问正常。现在，我们删掉/etc/keepalived/down这个文件，看能否实现IP漂移回来。 12345678910111213rm /etc/keepalived/down #删除node1主机上该文件rm: remove regular empty file`down&apos;? ytail /var/log/messages #查看日志信息May 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5removedMay 1619:27:54node1 Keepalived_vrrp[2816]: VRRP_Script(chk_schedown) succeededMay 1619:27:55node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:27:55node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:27:56node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) setting protocol VIPs.May 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:27:57node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5addedMay 1619:27:57node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5added 至此，我们已成功实现了keepalived的相关功能。以上演示的仅仅是主从模式下地址漂移。那我们能否实现在双主模式下实现地址漂移呢？答案是肯定的。 实现双主模式配置： 在node1主机上，修改keepalived配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647vim /etc/keepalived/keepalived.conf #其他信息不变，仅修改如下信息vrrp_instance VI_2 &#123; #添加虚拟路由组2state BACKUP #定义该路由在虚拟路由组2中为backup路由interface eth0virtual_router_id 232#定义组IDpriority 200#定义在该组中的优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.6/16dev eth0 label eth0:1#因是双主模型，定义VIP及设备和别名，同上边定义的别名要区分开&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master eth0:1&quot;#修改这三项，因我们有意定义成其他别名notify_backup &quot;/etc/keepalived/notify.sh backup eth0:1&quot;notify_fault &quot;/etc/keepalived/notify.sh fault eth0:1&quot;&#125;同时注释掉virtual_server部分。在双主模型下，我们不使用virtual_server部分。scp /etc/keepalived/keepalived.conf node2:/etc/keepalived/#将该配置文件传给另一个主机，即node2在node2上，修改keepalived配置文件vim /etc/keepalived/keepalived.confvrrp_instance VI_2 &#123; #在该主机上修改虚拟路由组2state MASTER #修改在该组中本路由为master路由interface eth0virtual_router_id 232#定义组IDpriority 201#修改在该组中的优先级，一定要高于node1中虚拟路由组2的优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.6/16dev eth0 label eth0:1#因是双主模型，定义VIP及设备和别名，同上边定义的别名要区分开&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master eth0:1&quot;notify_backup &quot;/etc/keepalived/notify.sh backup eth0:1&quot;notify_fault &quot;/etc/keepalived/notify.sh fault eth0:1&quot;&#125; 在双主模型下，为了让同一个域名解析到不同的IP上，我们需要用到DNS将其解析到不同的两个IP上。这里，为了查看效果，我们先不安装DNS解析。待会在实现。 以上配置好后，重启keepalived服务。在物理机上在浏览器地址栏分别输入172.16.32.5和172.16.32.6查看显示效果。 可以看到，显示正常。我们的配置是正确的。 在我们的node1主机上，我们来手动实现地址漂移，看能否实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293cd /etc/keepalived #进入该目录touch down #创建该文件tail /var/log/messages #查看日志信息May 1621:25:25node1 named[16646]: zone localhost/IN: loaded serial 0May 1621:25:25node1 named[16646]: zone managed-keys.bind/IN/_meta: loaded serial 10May 1621:25:25node1 named[16646]: runningMay 1621:25:25node1 named[16646]: zone langdu.com/IN: sending notifies (serial 2013005)May 1621:45:59node1 Keepalived_vrrp[11785]: VRRP_Script(chk_schedown) failedMay 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) Received higher prio advertMay 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) Entering BACKUP STATE #可看到进入backup状态May 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) removing protocol VIPs. #VIP漂移成功May 1621:46:00node1 Keepalived_vrrp[11785]: Netlink reflector reports IP 172.16.32.5removedMay 1621:46:00node1 Keepalived_healthcheckers[11784]: Netlink reflector reports IP 172.16.32.5removed现在，一起看下node2上的日志信息：tail /var/log/messagesMay 1621:46:00node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1621:46:00node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1621:46:01node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) setting protocol VIPs.May 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1621:46:02node1 Keepalived_vrrp[11537]: Netlink reflector reports IP 172.16.32.5added #添加另一个VIPMay 1621:46:02node1 Keepalived_healthcheckers[11536]: Netlink reflector reports IP 172.16.32.5addedMay 1621:46:02node1 avahi-daemon[3375]: Registering new address record for172.16.32.5on eth0.May 1621:46:07node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5现在我们来安装DNS服务器，实现将同一个域名解析到不同的IP地址上。这里我们将node1作为DNS服务器来负责域名解析。因此，以下命令在node1主机上执行。yum -y install bind97 bind97-utils #安装bind97，来提供DNS服务，在安装前请确保你的虚拟机上没有安装其他bind软件包vim /etc/named.rfc1912.zones #编辑该文件，添加如下信息zone &quot;langdu.com&quot;IN &#123; #添加该区域，DNS相关配置请查看我的相关博客，里边有详细介绍typemaster; file&quot;langdu.com.zone&quot;;&#125;;zone &quot;32.16.172.in-addr.arpa&quot;IN &#123;typemaster;file&quot;172.16.32.zone&quot;;&#125;;cd /etc/namedvim 172.16.32.zone#编辑该文件，添加如下内容：$TTL 600@ IN SOA ns.langdu.com. admin.langdu.com. (20130054H5M3D1D)IN NS ns.langdu.com.30IN PTR ns.langdu.com.6IN PTR www.langdu.com.5IN PTR www.langdu.com.vim langdu.com.zone #编辑该文件，添加如下内容：$TTL 600@ IN SOA ns.langdu.com. admin.langdu.com. (20130054H5M3D1D)IN NS nsIN NS wwwIN NS wwwns IN A 172.16.32.30www IN A 172.16.32.5#实现将同一个域名解析到不同的IP上www IN A 172.16.32.6named-checkconf #检查配置文件named-checkzone &quot;langdu.com&quot;/var/named/langdu.com.zone #检查区域文件named-checkzone &quot;32.16.172.in-addr.arpa&quot;/var/named/172.16.32.zone在我们的物理机上修改hosts文件。打开C:\Windows\System32\drivers\etc/hosts文件，添加两行信息：172.16.32.5www.langdu.com172.16.32.6www.langdu.com检查没有问题后启动服务。service named startdig -t A www.langdu.com @172.16.32.30#使用该命令查询下DNS服务器是否可用，以下是显示结果：; &lt;&lt;&gt;&gt; DiG 9.7.0-P2-RedHat-9.7.0-6.P2.el5_7.4&lt;&lt;&gt;&gt; -t A www.langdu.com @172.16.32.30;; globaloptions: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;-opcode: QUERY, status: NOERROR, id: 5351;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 1;; QUESTION SECTION:;www.langdu.com. IN A;; ANSWER SECTION:www.langdu.com. 600IN A 172.16.32.5#解析成功，可看到同一域名解析出两个IP地址www.langdu.com. 600IN A 172.16.32.6;; AUTHORITY SECTION:langdu.com. 600IN NS ns.langdu.com.langdu.com. 600IN NS www.langdu.com.;; ADDITIONAL SECTION:ns.langdu.com. 600IN A 172.16.32.30;; Query time: 5msec;; SERVER: 172.16.32.30#53(172.16.32.30);; WHEN: Thu May 1621:42:072013;; MSG SIZE rcvd: 111 我们的DNS服务器也做好了。现在在物理机上的浏览器地址栏输入www.langdu.com，查看下显示效果。因为是DNS服务器解析得到的，因此，刷新多次后可能还是显示同一个界面内容。 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本替换文件中某个字符串]]></title>
    <url>%2F2017%2F12%2F13%2Fshell%E8%84%9A%E6%9C%AC%E6%9B%BF%E6%8D%A2%E6%96%87%E4%BB%B6%E4%B8%AD%E6%9F%90%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[将当前目录下包含jack串的文件中，jack字符串替换为tom 1sed -i &quot;s/jack/tom/g&quot; `grep &quot;jack&quot; -rl ./` 将某个文件中的jack字符串替换为tom 1sed -i &quot;s/jack/tom/g&quot; test.txt linux sed 批量替换多个文件中的字符串 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl yourdir` 例如：替换/home下所有文件中的www.bcak.com.cn为bcak.com.cn 1sed -i &quot;s/www.bcak.com.cn/bcak.com.cn/g&quot; `grep www.bcak.com.cn -rl /home` 下面这条命令： 1perl -pi -e &apos;s|ABCD|Linux|g&apos; `find ./ -type f` 将调用perl执行一条替换命令，把find命令找到的所有文件内容中的ABCD替换为Linux 1find ./ -type f 此命令是显示当前目录下所有的文件,上面的 “s|ABCD|Linux| g” 是perl要执行的脚本，即把所有ABCD替换为Linux,如果不写最后的那个g，“s|ABCD|Linux| ”将只替换每一行开头的ABCD]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大文本处理工具grep与sed及awk简介(转)]]></title>
    <url>%2F2017%2F12%2F13%2F%E4%B8%89%E5%A4%A7%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7grep%E4%B8%8Esed%E5%8F%8Aawk%E7%AE%80%E4%BB%8B-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[grep、sed和awk都是文本处理工具，虽然都是文本处理工具单却都有各自的优缺点，一种文本处理命令是不能被另一个完全替换的，否则也不会出现三个文本处理命令了。只不过，相比较而言，sed和awk功能更强大而已，且已独立成一种语言来介绍。 grep：==文本过滤器==，如果仅仅是 过滤文本，可使用grep，其效率要比其他的高很多； sed：Stream EDitor，==流编辑器==，默认只处理模式空间，不处理原数据，如果你处理的数据是 针对行进行处理的，可以使用sed； awk：==报告生成器==，格式化以后显示。如果对处理的数据需要生成报告之类的信息，或者 你处理的数据是按列进行处理的 ，最好使用awk。 grepgrep是一个最初用于Unix操作系统的命令行工具。在给出文件列表或标准输入后，grep会对匹配一个或多个正则表达式的文本进行搜索，并只输出匹配（或者不匹配）的行或文本。 Unix的grep家族包括grep、egrep和fgrep。 egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符，fgrep就是fixed grep或fastgrep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。 linux使用GNU版本的grep。它功能更强，可以通过-E、-F命令行选项来使用egrep和fgrep的功能。 grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到屏幕，不影响原文件内容。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 grep：根据模式搜索文本，并将符合模式的文本行显示出来。 Pattern：文本字符和正则表达式的元字符组合而成匹配条件 123456789101112131415161718192021222324使用格式：grep [options] PATTERN [FILE...] -i：忽略大小写 --color：匹配到字符用其他颜色显示出来，默认是红色 -v：显示没有被模式匹配到的行 -o：只显示被模式匹配到的字符串，不显示行 -E：使用扩展正则表达式 -A n：表示显示该行及其后n行 -B n：表示显示该行及其前n行 -C n：表示显示该行及其前后各n行 正则表达式：REGular EXPression，REGEXP元字符：.：匹配任意单个字符[]：匹配指定范围内的任意单个字符[^]：匹配指定范围外的任意单个字符 字符集和：[:digit:]，[:lower:]，[:upper:]，[:punct:]，[:space:]，[:alpha:]，[:alnum:] 对应上边：数字 ，小写字母，大写字母，标点符号，空白字符，所有字母，所有数字和字母匹配次数（贪婪模式，即尽可能长的匹配）：*：匹配其前面的字符任意次 匹配.b和.*b，看二者有什么区别，命令和显示效果如下： grep练习： 12345678910111213141516171819202122232425262728293031323334353637381. 显示/proc/meminfo文件中以不区分大小的s开头的行； grep -i &apos;^s&apos; /proc/meminfo 或者 grep &apos;^[sS]&apos; /proc/meminfo #[]表示匹配指定范围内的单个字符，因此也可实现不区分大小写 2. 显示/etc/passwd中以nologin结尾的行; grep &apos;nologin$&apos; /etc/passwd 扩展一：取出默认shell为/sbin/nologin的用户列表 grep &apos;/sbin/nologin&apos; /etc/passwd | cut -d: -f1 或者 grep &apos;/sbin/nologin&apos; /etc/passwd | awk -F: &apos;&#123;print $1&#125;&apos; 或者直接使用awk awk -F: &apos;$7 ~ /nologin/&#123;print $1&#125;&apos; /etc/passwd 扩展二：取出默认shell为bash，且其用户ID号最小的用户的用户名 grep &apos;bash$&apos; /etc/passwd | sort -n -t: -k3 | head -1 | cut -d: -f1 或者 awk -F: &apos;$7 ~ /bash/&#123;print $3,$1&#125;&apos; /etc/passwd | sort -n | head -1 | awk &apos;&#123;print $2&#125;&apos;3. 显示/etc/inittab中以#开头，且后面跟一个或多个空白字符，而后又跟了任意非空白字符的行； grep &apos;^#[[:space:]]\&#123;1,\&#125;[^[:space:]]&apos; /etc/inittab4. 显示/etc/inittab中包含了:一个数字:(即两个冒号中间一个数字)的行； grep &apos;:[0-9]:&apos; /etc/inittab5. 显示/boot/grub/grub.conf文件中以一个或多个空白字符开 头的行； grep &apos;^[[:space:]]\&#123;1,\&#125;&apos; /boot/grub/grub.conf6. 显示/etc/inittab文件中以一个数字开头并以一个与开头数字相同的数字结尾的行； grep &apos;\(^[0-9]\).*\1$&apos; /etc/inittab #在RHEL5.8以前的版本中可查看到效果7. 找出某文件中的，1位数，或2位数； grep &apos;\&lt;[[:digit:]][[:digit:]]\?\&gt;&apos; /etc/inittab 或者 grep &apos;\&lt;[0-9]\&#123;1,2\&#125;\&gt;&apos; /etc/inittab8. 查找当前系统上名字为student(必须出现在行首)的用户的帐号的相关信息, 文件为/etc/passwd grep &apos;^student:&apos; /etc/passwd 扩展：若存在该用户，找出该用户的ID号： grep &apos;^student:&apos; /etc/passwd | cut -d: -f3 或者# id -u student ``` 思考题：用多种方法找出本地的IP地址，这里给出三种方法，如果你还有其他方法可以一起分享下：``` ifconfig eth0|grep -oE &apos;([0-9]&#123;1,3&#125;\.?)&#123;4&#125;&apos;|head -n 1ifconfig eth0|awk -F: &apos;/inet addr/&#123;split($2,a,&quot; &quot;);print a[1];exit&#125;&apos; #这里使用了awk的内置函数，如果不懂可在看完awk的介绍后再来做此题ifconfig |grep &quot;inet addr&quot;|grep -v &quot;127.0.0.1&quot; |awk -F: &apos;&#123;print $2&#125;&apos; |awk &apos;&#123;print $1&#125;&apos; sedsed（意为流编辑器，源自英语“stream editor”的缩写）是Unix常见的命令行程序。 sed 用来把文档或字符串里面的文字经过一系列编辑命令转换为另一种格式输出。 sed 通常用来匹配一个或多个正则表达式的文本进行处理。sed是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等 sed是一个精简的、非交互式的编辑器。它能执行与编辑vi和emacs相同的编辑任务，但sed编辑器不提供交互使用方式，只能在命令行下输入编辑命令。 作为编辑器，当然少不了插入（a/、i/）、删除（d）、查找替换(s)等命令。 sed练习 删除/etc/grub.conf文件中行首的空白符； 1sed -r &apos;s/^[[:space:]]+//&apos; /etc/grub.conf 替换/etc/inittab文件中“id:3:initdefault:”一行中的数字为5； 1sed &apos;s/\(id:\)[0-9]\(:initdefault:\)/\15\2/g&apos; /etc/inittab 删除/etc/inittab文件中的空白行； 1sed &apos;/^$/d&apos; /etc/inittab 删除/etc/inittab文件中开头的#号； 1sed &apos;s/^#//g&apos; /etc/inittab 删除某文件中开头的#号及其后面的空白字符，但要求#号后面必须有空白符； 1sed &apos;s/^#[[:space:]]\&#123;1,\&#125;//g&apos; /etc/inittab 或 sed -r &apos;s/^#[[:space:]]+//g&apos; /etc/inittab 删除某文件中以空白字符后面跟#类的行中的开头的空白字符及# 1sed -r &apos;s/^[[:space:]]+#//g&apos; /etc/inittab 取出一个文件路径的目录名称; 1echo &quot;/etc/rc.d/abc/edu/&quot; | sed -r &apos;s@^(/.*/)[^/]+/?@\1@g&apos; 因sed支持扩展正则表达式，在扩展正则表达式中，+表示匹配其前面的字符至少1次 取出一个文件路径的最后一个文件名； 1echo &quot;/etc/rc.d/abc/edu/&quot; | sed -r &apos;s@^/.*/([^/]+)/?@\1@g&apos; awkawk是一种优良的文本处理工具，Linux及Unix环境中现有的功能最强大的数据处理引擎之一。 这种编程及数据操作语言（其名称得自于它的创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母）的最大功能取决于一个人所拥有的知识。 AWK提供了极其强大的功能：可以进行正则表达式的匹配，样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。它具备了一个完整的语言所应具有的几乎所有精美 特性。 实际上AWK的确拥有自己的语言：AWK程序设计语言，三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。最简单地说，AWK是一种用于处理文本的编程语言工具。 我们现在使用最多的是gawk，gawk是AWK的GNU版本。 AWK的功能是什么？ 与sed和grep很相似，awk是一种样式扫描与处理工具。但其功能却大大强于sed和grep。awk提供了极其强大的功能：它几乎可以完成grep和sed所能完成的全部工作，同时，它还可以可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。它具备了一个完整的语言所应具有的几乎所有精美特性。 awk的命令格式为：awk是一种程序语言，对文档资料的处理具有很强的功能。awk擅长从格式化报文或从一个大的文本文件中抽取数据。 1awk [-F filed-separator] “commands” input-file(s) 每一个命令（commands）都由两部分组成：一个模式（pattern）和一个相应的动作（action） 123/pattern1/&#123;action1&#125; /pattern2/&#123;action2&#125; /pattern3/&#123;action3&#125; awk将一行文字按分隔符（filed-separator）分为多个域，依次记为$ 1，$ 2 . . . $ n。$0代表所有域值。 因此awk更适合于以域为单位来处理文件。加之ARGIND等内置变量，使awk能处理多个文件。典型的应用为查找一个文件中的某个字段是否在另一个文件中出现。 但由于$0代表所有域，即整行，因此awk也有简单行处理能力。 awk的输出：print和printf print print的使用格式： print item1, item2, … 要点： 各项目之间使用逗号隔开，而输出时则以空白字符分隔； 输出的item可以为字符串或数值、当前记录的字段(如$1)、变量或awk的表达式；数值会先转换为字符串，而后再输出； print命令后面的item可以省略，此时其功能相当于print $0, 因此，如果想输出空白行，则需要使用print “”； 例子： 123awk &apos;BEGIN &#123; print &quot;line one\nline two\nline three&quot; &#125;&apos;awk -F: &apos;&#123; print $1, $7 &#125;&apos; /etc/passwd #等价于：awk -v FS=: &apos;&#123;print $1,$7&#125;&apos; /etc/passwd awk变量 awk内置变量之记录变量： 1234FS: field separator，字段分隔符，默认是空白字符；RS: Record separator，记录分隔符，默认是换行符；OFS: Output Filed Separator，输出字段分隔符ORS：Output Row Separator，输出行分隔符 一起来看一个示例： 12345vim test.txt #编辑该文件，添加如下两行信息作为示例使用welcome to redhat linux.how are you?[root@www ~]# awk &apos;BEGIN&#123;OFS=&quot;#&quot;&#125; &#123;print $1,$2&#125;&apos; test.txt #指定输出时的分隔符[root@www ~]# awk &apos;BEGIN&#123;OFS=&quot;#&quot;&#125; &#123;print $1,&quot;hello&quot;,$2&#125;&apos; test.txt #指定输出时的分隔符，并添加显示的内容 awk内置变量之数据变量： 123456789NR: The number of input records，awk命令所处理的记录数；如果有多个文件，这个数目会把处理的多个文件中行统一计数；NF：Number of Field，当前记录的字段个数，有时可用来表示最后一个字段FNR: 与NR不同的是，FNR用于记录正处理的行是当前这一文件中被总共处理的行数；ARGV: 数组，保存命令行本身这个字符串，如awk &apos;&#123;print $0&#125;&apos; a.txt b.txt这个命令中，ARGV[0]保存awk，ARGV[1]保存a.txt；ARGC: awk命令的参数的个数；FILENAME: awk命令所处理的文件的名称；ENVIRON：当前shell环境变量及其值的关联数组；如： 12awk &apos;BEGIN&#123;print ENVIRON[&quot;PATH&quot;]&#125;&apos;awk &apos;&#123;print $NF&#125;&apos; test.txt gawk命令也可以在“脚本”外为变量赋值，并在脚本中进行引用。例如，上述的例子还可以改写为： 123[root@www ~]# awk -v var=&quot;variable testing&quot; &apos;BEGIN&#123;print var&#125;&apos; #与上述的例子一样，显示效果如下variable testing printf printf命令的使用格式：printf format, item1, item2, … 要点： 1231. 其与print命令的最大不同是，printf需要指定格式；2. format用于指定后面的每个item的输出格式；3. printf语句不会自动打印换行，需要显式使用\n换行。 format格式的指示符都以%开头，后跟一个字符；如下：%c: 显示字符的ASCII码；%d, %i：十进制整数；%e, %E：科学计数法显示数值；%f: 显示浮点数；%g, %G: 以科学计数法的格式或浮点数的格式显示数值；%s: 显示字符串；%u: 无符号整数；%%: 显示%自身； 1awk -F: &apos;&#123;printf &quot;%-15s%i\n&quot;,$1,$3&#125;&apos; /etc/passwd 使用printf显示该文件中的第一列和第三列，要求第一列左对齐且占用15个字符宽度，第二列显示十进制整数，显示效果如下所示： 字符串操作符： 只有一个，而且不用写出来，用于实现字符串连接； 12[root@www ~]# awk &apos;BEGIN&#123;print &quot;A&quot; &quot;B&quot;&#125;&apos; #连接A和B两个字符，使其成为一个字符串，显示效果如下所示：AB 总结： 如果文件是格式化的，即由分隔符分为多个域的，优先使用awk awk适合按列（域）操作，sed适合按行操作 awk适合对文件的抽取整理，sed适合对文件的编辑。 grep 主要用于搜索某些字符串 sed，awk 用于处理文本 原文引自]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix3.4安装部署]]></title>
    <url>%2F2017%2F12%2F12%2Fzabbix3-4%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[运行环境： 12345nginx-1.12.1mysql-5.5.56php-7.1.7Ubuntu 16.04Linux localhost 4.9.50-x86_64 前言Linux下常用的系统监控软件有Nagios、Cacti、Zabbix、Monit等，这些开源的软件，可以帮助我们更好的管理机器，在第一时间内发现，并警告系统维护人员。 使用Zabbix的目的，是为了能够更好的监控mysql数据库服务器，并且能够生成图形报表，虽然Nagios也能够生成图形报表，但没有Zabbix这么强大。 Zabbix简介 zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。 zabbix由zabbix server与可选组件zabbix agent两部门组成。 zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视。 zabbix agent需要安装在被监视的目标服务器上，它主要完成对硬件信息或与操作系统有关的内存，CPU等信息的收集。 zabbix的主要特点： 安装与配置简单，学习成本低 支持多语言（包括中文） 免费开源 自动发现服务器与网络设备 分布式监视以及WEB集中管理功能 可以无agent监视 用户安全认证和柔软的授权方式 通过WEB界面设置或查看监视结果 email等通知功能 Zabbix主要功能 CPU负荷 内存使用 磁盘使用 网络状况 端口监视 日志监视 Zabbix安装 zabbix WEB环境搭建 zabbix的安装需要LAMP或者LNMP环境。 2.zabbix 数据库设置 zabbix数据库可以和zabbix服务器分离，采用用专门的mysql服务器存储数据，此时要给zabbix数据库受相应的权限。 对于Zabbix server 和 proxy 守护进程以及Zabbix前端，都需要连接到一个数据库。Zabbix agent不需要数据库的支持。 SQL 脚本 用于创建数据库架构（schema）并插入数据集（dataset）。 Zabbix proxy数据库只需要数据库架构（schema），而Zabbix server数据库在建立数据库架构（schema）后，还需要数据集（dataset）。 建立Zabbix数据库后，可以开始对Zabbix进行编译。 1grant all privileges on zabbix.* to zabbix_user@&apos;ip&apos; identified by &apos;123456&apos;; == 注：ip为zabbix服务器的IP地址。== 关于数据库的安装，可以查看Mysql安装，我习惯使用二进制包。 启动数据库 1/usr/local/mysql/bin/mysqld_safe --user=mysql &amp; 登录数据库，创建帐号和设置权限： 123mysql&gt; use mysql;mysql&gt;create database zabbix character set utf8;mysql&gt;grant all privileges on zabbix.* to zabbix_user@&apos;localhost&apos; identified by &apos;123456&apos;; 安装zabbix服务 增加zabbix用户和组 对于所有Zabbix的守护进程，需要一个无特权的用户。如果Zabbix守护进程以一个无特权的用户账户启动，那么它会使用该用户运行。 然而，如果一个守护进程以‘root’用户启动，它会切换为‘zabbix’用户账户，且这个用户必须存在。在Linux系统中，可以使用下面命令建立一个用户（该用户属于自己的用户组，“zabbix”）： 12#groupadd zabbix#useradd -g zabbix -m zabbix 使用root，bin或其他特殊权限的账户运行Zabbix是一个安全风险。 对于Zabbix前端的安装，不需要使用单独的用户账户。 如果Zabbix server 和 agent 运行在同一台计算机上，建议使用不同的账户运行Server和Agent。否则，如果两个进程使用了同一个用户，Agent就可以访问Server的配置文件，并可轻易地读取Zabbix中任何管理员级别的用户，比如数据库密码。 官网下载解压软件包。 在 Ubuntu 14.04 LTS 上安装 Zabbix 3.4： 1wget http://repo.zabbix.com/zabbix/3.4/ubuntu/pool/main/z/zabbix/zabbix_3.4.4.orig.tar.gz 导入数据库表 12345create user &apos;zabbix&apos;@&apos;localhost&apos; identified by &apos;PASSWORD&apos;;create database zabbix;grant all privileges on `zabbix`.* to &apos;zabbix&apos;@&apos;localhost&apos;;flush privileges;exit; 1234567891011121314151617#cd zabbix-3.4/database/mysqlmysql -u root -p mysql&gt; use zabbix; #进入数据库，按照顺序进行导入，否则会出错。Database changedmysql&gt; source /root/zabbix-3.4.4/database/mysql/schema.sql...Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0 mysql&gt; source /root/zabbix-3.4.4/database/mysql/images.sql... Query OK, 1 row affected (0.01 sec) mysql&gt; source /root/zabbix-3.4.4/database/mysql/data.sql 编译安装zabbix 123./configure --prefix=/usr/local/zabbix --with-mysql=/usr/local/mysql/bin/mysql_config --with-net-snmp --with-libcurl --enable-server --enable-agent --enable-proxymake install 添加服务端口 12345vim /etc/serviceszabbix-agent 10050/tcp # Zabbix Agentzabbix-agent 10050/udp # Zabbix Agentzabbix-trapper 10051/tcp # Zabbix Trapperzabbix-trapper 10051/udp # Zabbix Trapper 添加配置文件 123mkdir -p /etc/zabbixcp -r zabbix-3.4/conf/* /etc/zabbix/chown -R zabbix:zabbix /etc/zabbix 修改server配置文件，添加zabbix数据库密码 1vim /etc/zabbix/zabbix_server.conf 1234567891011LogFile=/tmp/zabbix_server.logPidFile=/tmp/zabbix_server.pidDBName=zabbixDBUser=zabbix_userDBPassword=123456 #指定zabbix数据库密码ListenIP=192.168.10.197 #服务器IP地址 修改Agentd配置文件，更改HOSTNAME为本机的hostname 1vim /etc/zabbix/zabbix_agentd.conf 123456789PidFile=/tmp/zabbix_agentd.pid #进程PIDLogFile=/tmp/zabbix_agentd.log #日志保存位置EnableRemoteCommands=1 #允许执行远程命令Server=192.168.10.197 #agent端的ipHostname=client1 #必须与zabbix创建的host name相同 添加web前段php文件 123cd zabbix-3.4/frontends/cp -rf php /home/httpd/zabbix #虚拟主机目录chown -R zabbix:zabbix zabbix web前端安装配置 修改PHP相关参数 1vim php.ini 123456max_execution_time = 300max_input_time = 300memory_limit = 128Mpost_max_size = 32Mdate.timezone = Asia/Shanghaimbstring.func_overload=2 PHP还必须支持一下模块，在php源码包直接编译安装。详细模块需要在安装是会提示:bcmath.so、gettext.so 在客户端浏览器上面访问zabbix，开始WEB的前端配置，http://ZabbixIP/zabbix 按提示点击下一步 Step1：下一步。Step2：如果全部OK的话才能进行下一步的安装，如果有错误请返回到server端检查相关的软件包是否安装。Step3：需要输入mysql数据库帐号密码,如果数据库不在zabbix服务器上面，在Host里面添加数据库服务器的地址，并且要用grant命令给数据库授权。Step4：输入服务器端 host name or host IP addres； 最后会自动写入配置文件：zabbix.conf.php，配置完成后出现登陆界面，默认的用户名为：admin，密码为：zabbix。 启动zabbix服务在zabbix安装目录下面可以直接启动 123# ./usr/local/zabbix/sbin/zabbix_serverps -A | grep zabbix 设置开启自动启动 systemd是Linux 系统工具，用来启动守护进程，已成为大多数发行版的标准配置。目前debian8和centos7均默认使用了systemd工具来管理服务。在此之前，通常使用init的sysv风格的脚本管理本机服务。缺点是不方便处理并发依赖等。systemd相较sysv则更加方便，更加强大，缺点是体系比较庞大，比较复杂。 Now we’ll add a user for Zabbix to run as. We’ll configure scripts to control the zabbix server daemon. 123- Create the file /etc/systemd/system/multi-user.target.wants/zabbix-server.service- Paste the following contents into the file and save and quit 12345678910111213[Unit]Description=Zabbix ServerAfter=syslog.target network.target mysqld.service[Service]Type=oneshotExecStart=/usr/local/zabbix/sbin/zabbix_server -c /usr/local/zabbix/etc/zabbix_server.confExecReload=/usr/local/zabbix/sbin/zabbix_server -R config_cache_reloadRemainAfterExit=yesPIDFile=/var/run/zabbix/zabbix_server.pid[Install]WantedBy=multi-user.target Now we’ll reload systemd and start the zabbix-server service 12345systemctl daemon-reloadsystemctl start zabbix-server.servicesystemctl start zabbix-agentd.service That completes the configuration needed for zabbix-server! 12345678910111213[Unit]Description=Zabbix AgentdAfter=syslog.target network.target mysqld.service[Service]Type=oneshotExecStart=/usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.confExecReload=/usr/local/zabbix/sbin/zabbix_agentd -R config_cache_reloadRemainAfterExit=yesPIDFile=/var/run/zabbix/zabbix_agentd.pid[Install]WantedBy=multi-user.target 1sed -i &quot;s/zabbix-agentd/zabbix_agentd/g&quot; zabbix-agentd.service 至此，zabbix server端的安装完毕，我们可以通过浏览器来访问! Zabbix主要的配置文件两个:“zabbix_server.conf”负责服务器端的设定； zabbix_agent.conf”用来设置客户端代理参数；“zabbix_proxy.conf”用来设定分布式的部署。 Zabbix_server.conf参数除了保证服务正常运行外还涉及该服务器的性能，如果参数设定不合理可能会导致zabbix添加主机不正常、代理端数据无法正常收集或是zabbix服务器性能严重下降，经常报告CPU占用过高或是IO占用过高等问题。 Zabbix前端已经就绪！默认的用户名是Admin，密码是zabbix。 跳坑 从布署包安装 zabbix的时候，影响到了mysql.导导致mysql启动不起来！运行下面的命令之后，即可！ 1/usr/local/mysql/bin/mysqld_safe 还有就是布署包默认运行的是appace容器，但是我的服务器上面已经有nginx了。所以最后还是手动编译一下。 apt-get install zabbix-server-mysql zabbix-frontend-php 通过这种方式安装zabbix的时候，出现了一个问题就是重启lnmp 时，mysql不能正常启动 configure: error: Invalid Net-SNMP directory - unable to find net-snmp-config 解决方法1apt-get install libsnmp-dev 编译安装zabbix error: MySQL library not found 123find / -name &quot;mysql_config*&quot;/usr/local/mysql/bin/mysql_config 我把–with-mysql改成 1--with-mysql=/usr/local/mysql/bin/mysql_config 正常通过 15. PHP Parse error: syntax error, unexpected &apos;[&apos; in /var/www/html/index.php on line 29 1PHP 5.4 is required 1Get value from agent failed: cannot connect to [[127.0.0.1]:10050]: [111] Connection refused 解决方法： 12127.0.0.1 是我的zabbix server服务器，本身也有监控自己本身的agent功能。出现这种错误是因为忘记在zabbix服务器开户zabbix_agentd。 123find / -name *agentd.logservice zabbix-server restart 7.1Zabbix agent on Zabbix server is unreachable for 5 minutes 修改agent的配置文件，将ServerActive的地址改为zabbix-server的IP地址，重启zabbix_agent 在troubleshouting查看服务日志的时候，可以将注意力集中在有显示“fail”或者“Error”这类失败的关键词上，这样可以快速排错，找到问题的原因，而不必通篇阅读所有的日志，极大的提高效率。 作为运维工程师，脑袋储存的信息可能比较多、杂，时而出现忘记了某个服务、配置文件的绝对路径，如果记得文件或者目录的完整名，可以使用“locate+文件名”命令来定位文件的绝对路径，若是连文件名也记不大清了，没关系，还可以用Linux平台强大的搜索命令find，以全局查找的方式，通过星号来匹配到想要查找的文件的绝对路径，例如：find / -name *agentd.conf （从/目录开始，全局搜索以agentd结尾的.conf文件）。 这些都是作为一名运维工程师应该具备的基本技能，而不必通过死记硬背的方式来记忆所有文件的绝对路径。 参考文档1 zabbix-3.0.4安装部署 官方从部署包安装zabbix Installing Zabbix 3.0 on Ubuntu 16.04 在Ubuntu 上安装 Zabbix Zabbix Documentation 3.4 Zabbix agent on Zabbix server is unreachable for 5 minutes [systemd的简单使用和说明] (http://blog.51cto.com/qixue/1906779)]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[puppet系统配置自动化解决方案(转)]]></title>
    <url>%2F2017%2F12%2F10%2Fpuppet%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[相信做过运维的朋友都会有这样的体会：把一个新的服务器从刚装好系统的状态配置到可以运行应用程序，是个挺麻烦的过程。 就拿一个运行 nginx + php 的 web 服务器为例，可能需要部署 ssh 公钥，设置用户 sudo 权限，关闭密码登录、root 远程登录，配置 iptables 规则。然后安装所需版本的 nginx、 php 到规范的路径，不能搞错版本，以免缺失所需特性或者造成冲突，还要安装应用所需要的 php 扩展比如 gd 之类。 然后是用于监控的客户端程序，比如 nagios 的 nrpe ，或者 zabbix_agent，用于日志轮转的 cronolog 等等。最后还得记得修改 ulimits 、tcp 相关的内核参数。然后还得一一验证所有的设置是否正确。如果是部署了新的东西或者更新了配置，还得写一下安装文档，这样下回安装的时候才不会遗漏什么编译 选项、软件包，或者少设置了系统参数导致故障。 有人请假或则离职的时候，别人也才能接替。需要部署的还不会仅仅是 web 服务器，还有 lvs 、mysql、memcache，也许还会有 redis、sphinx 等等等等。有软件包需要升级，也得对所有用到的机器都升级一遍。更要命的是，往往需要配置的还不是一台机器，而是十几台乃至上百台。 这里可以看出，系统配置本身是件很繁琐的事情。 需要考虑到很多方面的事情，任何一个地方出了纰漏就可能导致故障或者埋下隐患。手动来做很容易出错，也很不够敏 捷高效，难以快速响应需求。为了解决这些问题，我也曾经用 bash 做过一些脚本，来实现部署和系统设置的自动化，但是受到 bash 的表达能力的限制，脚本的编写测试维护并不轻松。 我们碰到的麻烦别人也会碰到。所以就有了自动化好在现在有了一个好用的工具来解决这些问题。 puppet就是这样一个功能强大的系统配置集群管理工具。puppet分为 master端和agent 端，可以实现分布式分发，有强大的配置管理功能，可以实现自动化分发文件、安装软件包、执行命令、添加系统用户、设置 crontab 等等。它的配置文件是一种表达能力强的 DSL，可读性好、容易复用，且本身就可以作为很好的文档，这就免除了维护文档的负担，也避免了文档过时的问题。 puppet还使用了ssl 来保证通讯的安全性，防止敏感的配置信息泄露。支持集群化，以实现大批量主机并行更新维护。引入的 factor 还实现了针对不同系统、不同发行版、不同环境的针对性设置。以及很多方便强大的功能。 我们先看看如何安装 puppet 。最方便的方法是使用包管理器。对于 centos 等 redhat 系发行版，可以通过 yum 来安装。先把 puppet 的官方源加入到系统中： 1234567891011121314# cat &gt; /etc/yum.repos.d/puppet.repo &lt;&lt; EOF[puppet-dependencies]name=puppet dependenciesbaseurl=http://yum.puppetlabs.com/el/\$releasever/dependencies/\$basearch/gpgcheck=1gpgkey=http://yum.puppetlabs.com/RPM-GPG-KEY-puppetlabs[puppet-products]name=puppet dependenciesbaseurl=http://yum.puppetlabs.com/el/\$releasever/products/\$basearch/gpgcheck=1gpgkey=http://yum.puppetlabs.com/RPM-GPG-KEY-puppetlabsEOF 然后 1yum install puppet 就会自动安装好 puppet。从所列出依赖的软件包可以看出， puppet 是用 ruby 实现的。对于 debian 系发行版，也可以在 apt.puppetlabs.com 中找到相应的源和 gpg key。 puppet 分为 master 和 agent 。找两台机器或者两个虚拟机，一台作为 master，一台作为 agent ，两端都安装好后就可以开始配置了。 首先配置 master 。先在 master 端运行一下 1puppet master 初次运行会生成puppet 用户， 在 /etc/puppet 目录下生成默认配置目录结构并在 /var/lib/puppet 生成数据文件。如果提示 permisiton denided ，可以试试 chown puppet:puppet /var/lib/puppet/run 。 然后我们就可以开始写第一个配置文件了。在 /etc/puppet/manifests 目录下建一个 site.pp ，这是 puppet master 的主配置文件。输入 1234567package &#123; &quot;bison&quot;: ensure=&gt;&quot;installed&quot;,&#125;exec &#123; &quot;puppet test&quot;: command=&gt;&quot;/bin/touch /tmp/puppet-test&quot;,&#125; 第一个配置会使 agent 端确保编译 php 所必须的软件包 bison 已经安装好。对于不同的系统，会使用各自的包管理器来安装。第二个配置会在 agent 端执行 /bin/touch /tmp/puppet-test 。 然后配置客户端。先编辑 /etc/hosts，加入 master 的 ip，如： 1192.168.1.101 puppet 在客户端运行一下 1puppet agent --test 初次运行也同样会生成客户端的相应文件，然后就会去连接 master 端执行任务。此时会提示 1warning: peer certificate won&apos;t be verified in this SSL session exiting; no certificate found and waitforcert is disabled 这表示 agent 需要认证。因为 puppet 使用了 ssl 来保证安全，并需要 agent 经过 master 认证才能够访问配置。到 master 端执行一下 1puppet cert list 会列出待认证的 agent 列表。这里可以看到 agent 的主机名。如 1puppet-agent-test-01 (66:62:5C:84:B0:23:73:FB:80:7C:89:48:4C:A6:AF:53) 然后可以使用 1puppet cert sign puppet-agent-test-01 就能完成认证。如果觉得直接使用主机名不够灵活，也可以在运行 agent 时使用 –certname=认证名 来指定。在 agent 端再试一次，这回就可以看到，agent 已经开始干活了。看看 bison 工具是否安装好了，再看看 /tmp 目录下是否生成了 /tmp/puppet-test 文件。 需要注意的是，一个主机可以使用多个不同的certname，但一个certname只能被一台主机使用。如果原有的certname需要移动到另一个主机上使用，就需要在master端先 puppet cert clean “ 认证名” 来清除原有数据。所以，certname应当尽量保持全局唯一。 这里 agent 使用的 –test 让 agent 不以服务方式运行，只执行一次，并输出详细信息。去掉这个参数，puppet agent 就会以服务方式在后台运行，默认每 30 分钟连接一次服务器更新配置。可以用 puppet help master、puppet help agent 查看更多选项。 刚才是把所有的配置都写在了 sites.pp 文件里。在配置项增多，维护的项目增多以后，就会变得过于庞大而难以维护。所以就需要把配置分到不同的模块中去，以模块化的方式来管理配置。 puppet 的模块放在 /etc/puppet/modules 下。模块的目录结构如下图所示： 1234567modules/|-- test |-- files | `-- test.txt `-- manifests |-- init.pp `-- test.pp 在这个例子里，定义了一个 test 模块。其中 files 目录中用于安放该模块所需分发的文件，manifests 目录中是该模块的配置文件。其中 init.pp 是每个模块的主配置文件。内容通常为 import “*” ，来载入该模块的其他配置文件。我们在 files 目录中加入一个 test.txt 文件，并把之前 site.pp 中的内容挪到 test.pp 中，再加入分发文件的配置，定义成一个类： 123456789101112class test1 &#123;package &#123; &quot;bison&quot;: ensure=&gt;&quot;installed&quot;,&#125;exec &#123; &quot;puppet test&quot;: command=&gt;&quot;/bin/touch /tmp/puppet-test&quot;,&#125;file &#123; &quot;/tmp/test.txt&quot;: ensure =&gt; &quot;present&quot;, source =&gt; &quot;puppet:///modules/test/test.txt&quot;&#125;&#125; site.pp 中删除原有的配置，加入 import “test” ，把 test 模块加载进来，然后加入 include “test1” ，应用 test1 类的配置。Include语句也可以再class内使用。以在一个class中复用另一个class的配置。现在，我们在 agent 端再运行一次 puppet agent –test ，agent 还是会照常工作，但是配置已经分到模块中了。实践中，会把每个配置的项目建立一个模块，比如：nginx、php 等等。再看看 /tmp 目录，会发现 master 端的 test.txt 文件已经下载回来。puppet 已经完成了文件分发的工作。module 中的 files 通常用于分发配置文件，把软件包的配置文件集中管理。 file配置也可以用来创建目录。只要使用 ensure =&gt; “directory” 即可。如： 123file &#123; &quot;/tmp/testdir&quot;: ensure =&gt; &quot;directory&quot;,&#125; 到目前为止，我们只使用了一个agent，实际环境中，会有许多台需要不同配置的 agent 。这就需要对不同的 agent 应用不同的配置。在 sites.pp 中把 include test1 替换成针对特定节点的配置： 1234import &quot;test&quot;node &quot;puppet-agent-test-01&quot; &#123; include &quot;test1&quot;&#125; 这里的主机名可以用 “,” 分隔，指定多个主机，也可以用类似 /puppet-agent-test-.*/ 这样的正则表达式来灵活匹配。为了测试配置是否生效，我们可以修改一下之前的配置，再运行一下 agent。还可以再加入几台 agent 试试应用不同的节点的不同配置。 之前加入 agent 时，需要在 master 端手动为 agent 认证。在客户端众多，或者需要完全自动化的时候，可以配置自动签名。当然，前提是能通过别的途径比如 iptables 限制访问 master 的来源，或者是在可信的内网环境下。 在 /etc/puppet 目录中添加 autosign.conf 中输入agent 认证名的模式，如 .test.net 。需要注意的是，这里必须使用类似泛域名通配符的方式。也就是说， 只能出现在前面，而不允许 test.* 这样的形式。所以要应用自动签名，在规划 agent 的认证名的时候就要注意这一点。现在，我们用 puppet agent –test –certname=”agent01.test.net” 试试。这回就不再需要手动认证，直接就能执行 master 分发的配置任务了。 实际应用puppet时，会把puppet的配置文件，以及要分发的软件包的配置文件都加入到svn等源代码管理中。但是我们也会需要用puppet来分发一些我们自己编译打包的软件包等二进制文件。这些二进制文件并不适合放进源代码管理中。另外，需要用puppet分发的证书、密钥等敏感信息也不适合放入。这时，使用模块的文件就不太方便。好在puppet的文件服务器也是可以配置的。建立puppet文件服务器配置文件：/etc/puppet/fileserver.conf，输入 123456[modules] allow *[files] path /data/puppet allow * 这就定义了一个名为files的额外文件服务挂载点，位于 /data/puppet。也放一个 test.txt 在里面，然后就可以使用 123file &#123; &quot;/tmp/test2.txt&quot;: source =&gt; &quot;puppet:///files/test.txt&quot;&#125; 来分发了。这里 puppet:// 是协议名，后面的路径 /files/test.txt 的第一部分是挂载点名称。之前使用的 modules 这个特殊的挂载点名是指向各个模块的文件。如 /modules/test/test.txt 就是test模块下files目录中的test.txt文件。而 /files/test.txt 就是 files 挂载点对应目录下的 test.txt 文件。之后，我们就可以把这些二进制文件等用别的途径部署到自定义挂载点上。 之前我们使用的file、exec、package 在puppet中都被称为资源。每一种资源都有许多参数可以设置。对于 file 资源，可以用 ensure =&gt; “link”, target =&gt; “目标路径” 来建立软链接，用mode=&gt;0644来设置文件访问权限。对于exec资源，可以用cwd参数来设置当前路径，用creates参数来设置执行命令创建的路径，可以用于防止重复执行命令。对于所有的资源类型，都有一些共有的参数，称为元参数。其中最常用的就是require参数。这个参数指定了这个资源所依赖的资源。实际应用中，不同的任务间会有依赖关系。比如安装软件包需要先创建好目标路径的目录，需要先安装好所需的依赖软件包，这就可以用require选项来实现。比如： 12345678exec &#123; &quot;install sth.&quot;: cwd =&gt; &quot;/opt/some_package&quot;, exec =&gt; &quot;/bin/tar -xzvf /path/to/package.tar.gz&quot;, require =&gt; File[&quot;/opt/some_package&quot;],&#125;file &#123; &quot;/opt/some_package&quot;: ensure =&gt; &quot;directory&quot;,&#125; 任务 Exec[“install sth.”] 就会在任务 File[“/opt/some_package”]完成后运行。这里，对于每个类型的资源，可以用“，”分隔；如果要指定多种类型的资源，也可以写成列表形式。如： 1require =&gt; [ File[&quot;/path/to/file1&quot;, &quot;/path/to/file2&quot;], Package[&quot;package1] ] 还有一个与require相反的参数before，可以指定该任务必须在哪些任务前完成。 puppet还提供了多种资源类型来完成不同的任务。比如可以用cron类型来管理定时任务，用host类型来设置hosts文件等。 至此，已经简单介绍了puppet 的基本功能设置，可以针对不同主机安装执行不同的所需任务。 puppet 还有更多强大的功能，您可以参照 puppet 的官方文档，各取所需，在实践中学习应用。 应用puppet，把原先繁琐的系统配置过程自动化了，需要部署一台新的服务器时，只需要在初始化好puppet后，执行一次 puppet agent –test，剩下的就交给它来干了。既省时省力也不会出错。 机械化的操作就应当交给机器来做，这样才能把人的精力省出来做更有价值的事情。 原文引自]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>puppet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL搭建主从复制实现细节分析]]></title>
    <url>%2F2017%2F12%2F09%2FMySQL%E6%90%AD%E5%BB%BA%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概念 主从复制可以使MySQL数据库主服务器的主数据库，复制到一个或多个MySQL从服务器从数据库，默认情况下，复制异步; 根据配置，可以复制数据库中的所有数据库，选定的数据库或甚至选定的表。 MySQL中主从复制的优点横向扩展解决方案在多个从库之间扩展负载以提高性能。在这种环境中，所有写入和更新在主库上进行。但是，读取可能发生在一个或多个从库上。该模型可以提高写入的性能（由于主库专用于更新），同时在多个从库上读取，可以大大提高读取速度。 数据安全性由于主库数据被复制到从库，从库可以暂停复制过程，可以在从库上运行备份服务，而不会破坏对应的主库数据。 分析可以在主库上创建实时数据，而信息分析可以在从库上进行，而不会影响主服务器的性能。 长距离数据分发可以使用复制创建远程站点使用的数据的本地副本，而无需永久访问主库。 1.准备工作Mysql版本：MySQL 5.7.11Master-Server : 192.168.252.123Slave-Server : 192.168.252.124 关闭防火墙 1systemctl stop firewalld.service 安装 MySQL 首先在两台机器上装上，保证正常启动，可以使用 Master-Server 配置修改 my.cnf 配置 Master 以使用基于二进制日志文件位置的复制，必须启用二进制日志记录并建立唯一的服务器ID,否则则无法进行主从复制。 停止MySQL服务。 1service mysql.server stop 开启binlog ，每台设置不同的 server-id 1234$ cat /etc/my.cnf[mysqld]log-bin=mysql-binserver-id=1 启动MySQL服务 1service mysql.server start 登录MySQL 1/usr/local/mysql/bin/mysql -uroot -p 创建用户每个从库使用MySQL用户名和密码连接到主库，因此主库上必须有用户帐户，从库可以连接。任何帐户都可以用于此操作，只要它已被授予 REPLICATION SLAVE权限。可以选择为每个从库创建不同的帐户，或者每个从库使用相同帐户连接到主库 虽然不必专门为复制创建帐户，但应注意，复制用到的用户名和密码会以纯文本格式存储在主信息存储库文件或表中 。因此，需要创建一个单独的帐户，该帐户只具有复制过程的权限，以尽可能减少对其他帐户的危害。 登录MySQL 1/usr/local/mysql/bin/mysql -uroot -p 12mysql&gt; CREATE USER &apos;replication&apos;@&apos;192.168.252.124&apos; IDENTIFIED BY &apos;mima&apos;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;replication&apos;@&apos;192.168.252.124&apos;; Slave-Server 配置修改 my.cnf 停止MySQL服务。 1service mysql.server stop 123$ cat /etc/my.cnf[mysqld]server-id=2 如果要设置多个从库，则每个从库的server-id与主库和其他从库设置不同的唯一值。 启动MySQL服务 1service mysql.server start 登录MySQL 配置主库通信 查看 Master-Server ， binlog File 文件名称和 Position值位置 并且记下来 1mysql&gt; show master status; 要设置从库与主库进行通信，进行复制，使用必要的连接信息配置从库在从库上执行以下语句将选项值替换为与系统相关的实际值 参数格式，请勿执行 123456mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;master_host_name&apos;, -&gt; MASTER_USER=&apos;replication_user_name&apos;, -&gt; MASTER_PASSWORD=&apos;replication_password&apos;, -&gt; MASTER_LOG_FILE=&apos;recorded_log_file_name&apos;, -&gt; MASTER_LOG_POS=recorded_log_position; 1234567mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;192.168.252.123&apos;, -&gt; MASTER_USER=&apos;replication&apos;, -&gt; MASTER_PASSWORD=&apos;mima&apos;, -&gt; MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, -&gt; MASTER_LOG_POS=629;Query OK, 0 rows affected, 2 warnings (0.02 sec) MASTER_LOG_POS=0 写成0 也是可以的 放在一行执行方便 1CHANGE MASTER TO MASTER_HOST=&apos;192.168.252.123&apos;, MASTER_USER=&apos;replication&apos;, MASTER_PASSWORD=&apos;mima&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS=629; 启动从服务器复制线程 12mysql&gt; START SLAVE;Query OK, 0 rows affected (0.00 sec) 查看复制状态 检查主从复制通信状态 Slave_IO_State #从站的当前状态Slave_IO_Running： Yes #读取主程序二进制日志的I/O线程是否正在运行Slave_SQL_Running： Yes #执行读取主服务器中二进制日志事件的SQL线程是否正在运行。与I/O线程一样Seconds_Behind_Master #是否为0，0就是已经同步了 必须都是 Yes 在mysql5.0以后的版本，mysql主从已经相当的成熟了，可以只监控Slave_IO_Running，Slave_SQL_Running，Seconds_Behind_Master状态就可以了，这里不再做说明。 测试主从复制 启动MySQL服务 登录MySQL 在 Master-Server 创建测试库 123mysql&gt; CREATE DATABASE `replication_wwww.ymq.io`;mysql&gt; use `replication_wwww.ymq.io`;mysql&gt; CREATE TABLE `sync_test` (`id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 在 Slave-Server 查看是否同步过来 123mysql&gt; show databases;mysql&gt; use replication_wwww.ymq.iomysql&gt; show tables; 需要注意的问题主服务器与从服务器的时区必须一致，否则mysql执行与时间相关的函数将会导致数据不一致。 一些坑 Last_IO_Errno: 1045错误： 解决方案 Connecting and Last_IO_Errno: 2003 错误 解决方案2 Last_Errno: 1008解决方案3 MySQL错误处理–1146错误 解决方案 Last_SQL_Errno：1062 解决方案 参考文档1 mysql主主和主主集群 双主同步，如果服务器意外挂机 MySQL基于日志（binlog）主从复制搭建 MySQL数据库设置主从同步]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu配置Shadowsocks实现终端代理]]></title>
    <url>%2F2017%2F12%2F09%2FUbuntu%E9%85%8D%E7%BD%AEShadowsocks%E5%AE%9E%E7%8E%B0%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景场景一： 这几天想配置PHP Laravel框架，Laravel框架需要Composer安装。结果安装Composer的时候遭遇到了GFW，光在浏览器上穿墙还不够，还要在终端上穿墙。使用Shadowsocks在浏览器上穿墙很简单，但是在终端穿墙以前没接触过，这次花了5个小时搞定了。 利用linode翻墙安装相关配置时，哪速度，真是让人怀念。所以在局域网的一台机器上面开始动手了。 场景二： 做开发的同学，应该都会经常接触终端，有些时候我们在终端会做一些网络操作，比如下载gradle包等，由于一些你懂我也懂的原因，某些网络操作不是那么理想，这时候我们就需要设置代理来自由地访问网络。 Shadowsocks是我们常用的代理工具，它使用socks5协议，而终端很多工具目前只支持http和https等协议，对socks5协议支持不够好，所以我们为终端设置shadowsocks的思路就是将socks协议转换成http协议，然后为终端设置即可。仔细想想也算是适配器模式的一种现实应用吧。 想要进行转换，需要借助工具，这里我们采用比较知名的polipo来实现。 polipo是一个轻量级的缓存web代理程序。 前题是，已经在服务器端安装好了相关应用。 Shadowsocks 是一个开源安全的 Socks5 代理，中文名称“影梭“，类似于 SSH 代理。与一度非常流行的基于 GAE 的科学上网方式相比，Shadowsocks 部署简单，使用灵活；同时与全局代理的 VPN 不同，Shadowsocks 可以仅针对浏览器代理，轻巧方便，如果说 VPN 是一把 ==屠龙宝刀==，那么 Shadowsocks 就是一把 ==瑞士军刀==，虽小巧但功能强大。 1.ubuntu安装shadowsocks运行环境安装123sudo apt-get updatesudo apt-get install python-pipsudo apt-get install python-setuptools m2crypto 接着安装shadowsocks 1pip install shadowsocks 如果是ubuntu16.04 直接 (16.04 里可以直接用apt 而不用 apt-get 这是一项改进） 2.启动shadowsocks安装好后，在本地我们要用到sslocal ，终端输入sslocal –help 可以查看帮助，像这样 通过帮助提示我们知道各个参数怎么配置，比如 sslocal -c 后面加上我们的json配置文件，或者像下面这样直接命令参数写上运行。 比如 1sslocal -s 11.22.33.44 -p 50003 -k &quot;123456&quot; -l 1080 -t 600 -m aes-256-cfb -s表示服务IP, -p指的是服务端的端口，-l是本地端口默认是1080, -k 是密码（要加””）, -t超时默认300,-m是加密方法默认aes-256-cfb， 为了方便我推荐直接用sslcoal -c 配置文件路径 这样的方式，简单好用。 我们可以在/home/mudao/ 下新建个文件shadowsocks.json (mudao是我在我电脑上的用户名，这里路径你自己看你的)。内容是这样：12345678&#123;&quot;server&quot;:&quot;11.22.33.44&quot;,&quot;server_port&quot;:50003,&quot;local_port&quot;:1080,&quot;password&quot;:&quot;123456&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 确定上面的配置文件没有问题，然后我们就可以在终端输入 1sslocal -c /etc/shadowsocks.json 回车运行。如果没有问题的话，下面会是这样… 3.开机后台自动运行ss如果你上面可以代理上网了可以进行这一步，之前我让你不要关掉终端，因为关掉终端的时候代理就随着关闭了，之后你每次开机或者关掉终端之后，下次你再想用代理就要重新在终端输入这样的命令 sslocal -c /home/mudao/shadowsocks.json ，挺麻烦是不？ 我们现在可以在你的ubuntu上安装一个叫做supervisor的程序来管理你的sslocal启动。关于supervisor在前面介绍安装mod-ss-panel时，有介绍！ 1sudo apt-get install supervisor 安装好后我们可以在/etc/supervisor/目录下找到supervisor.conf配置文件，我们可以用以下命令来编辑 1sudo gedit /etc/supervisor/supervisor.conf 在这个文件的最后加上以下内容 1234567[program:shadowsocks]command=sslocal -c /home/mudao/shadowsocks.jsonautostart=trueautorestart=trueuser=rootlog_stderr=truelogfile=/var/log/shadowsocks.log 现在关掉你之前运行sslocal命令的终端，再打开终端输入 1sudo service supervisor restart 然后去打开浏览器看看可不可以继续代理上网。你也可以用 1ps -ef|grep sslocal 命令查看sslocal是否在运行。 这个时候我们需要在/etc下编辑一个叫rc.local的文件 ，让supervisor开机启动。 1sudo gedit /etc/rc.local 在这个配置文件的 1exit 0 前面一行加上 1service supervisor start 保存。 看你是否配置成功你可以在现在关机重启之后直接打开浏览器看是否代理成功。 以上原文引自这里4. 终端穿墙浏览器能穿墙就已经能满足绝大多数需求了，但是有的时候终端也必须穿墙，就比如Composer。关于终端穿墙，本人尝试了很多种方案，比如Privoxy、Proxychains和Polipo，最后选择了Privoxy。 为什么终端需要单独穿墙呢？难道Shadowsock不能“全局”代理么？这个问题当时困惑了我很久，最后一句话点醒了我。 Shadowsocks是一个使用SOCKS5（或者SOCK4之类）协议的代理，它只接受SOCKS5协议的流量，不接受HTTP或者HTTPS的流量。所以当你在Chrome上能穿墙的时候，是Proxy SwitchyOmega插件把HTTP和HTTPS流量转换成了SOCKS协议的流量，才实现了Shadowsocks的代理。而终端是没有这样的协议转换的，所以没法直接使用Shadowsock进行代理。 这时候就需要一个协议转换器，这里我用了Privoxy(我用privoxy没有成功！但是用polipo成功了)。 1~$ sudo apt-get install privoxy 安装好后进行配置，Privoxy的配置文件在/etc/privoxy/config，这个配置文件中注释很多。 找到 1listen-address 这一节，确认监听的端口号(这个端口号要跟1080区分开来，之前没有成功。估计就是因为把这个端口号改了)。 找到5.2. forward-socks4, forward-socks4a, forward-socks5 and forward-socks5t这一节，加上如下配置，注意最后的点号。 有关Privoxy的配置就结束了，重启一下Privoxy。 1~$ sudo /etc/init.d/privoxy restart 接着配置一下终端的环境，需要如下两句。 12~$ export http_proxy=&quot;127.0.0.1:8118&quot;~$ export https_proxy=&quot;127.0.0.1:8118&quot; 为了方便还是在/etc/rc.local中添加如下命令，注意在exit 0之前。 1sudo /etc/init.d/privoxy start 在/etc/profile的末尾添加如下两句。 12export http_proxy=&quot;127.0.0.1:8118&quot;export https_proxy=&quot;127.0.0.1:8118&quot; 安装privoxy的参考原文在这里 5.Shadowsocks 转换 HTTP 代理(使用Polipo)Shadowsocks 默认是用 Socks5 协议的，对于 ==Terminal== 的 get,wget 等走 Http 协议的地方是无能为力的，所以需要转换成 Http 代理，加强通用性，这里使用的转换方法是基于 Polipo 的。 输入命令安装 Polipo： 1sudo apt-get install polipo 修改配置文件： 1sudo gedit /etc/polipo/config 将下面的内容整个替换到文件中并保存： 123456789101112131415161718# This file only needs to list configuration variables that deviate# from the default values. See /usr/share/doc/polipo/examples/config.sample# and &quot;polipo -v&quot; for variables you can tweak and further information.logSyslog = falselogFile = &quot;/var/log/polipo/polipo.log&quot; socksParentProxy = &quot;127.0.0.1:1080&quot;socksProxyType = socks5 chunkHighMark = 50331648objectHighMark = 16384 serverMaxSlots = 64serverSlots = 16serverSlots1 = 32 proxyAddress = &quot;0.0.0.0&quot;proxyPort = 8123 重启 Polipo： 1/etc/init.d/polipo restart 验证代理是否正常工作： 12export http_proxy=&quot;http://127.0.0.1:8123/&quot;curl www.google.com 如果正常，就会返回抓取到的 Google 网页内容。 第二种验证代理是否正常工作的方法： 安装完成就需要进行验证是否work。这里展示一个最简单的验证方法，打开终端，如下执行 123407:56:24-androidyue/var/log$ curl ip.gs当前 IP：125.39.112.15 来自：中国天津天津 联通08:09:23-androidyue/var/log$ http_proxy=http://localhost:8123 curl ip.gs当前 IP：210.140.193.128 来自：日本日本 如上所示，为某个命令设置代理，前面加上http_proxy=http://localhost:8123 后接命令即可。注：8123是polipo的默认端口，如有需要，可以修改成其他有效端口。 当前会话全局设置如果嫌每次为每一个命令设置代理比较麻烦，可以为当前会话设置全局的代理。 即使用 1export http_proxy=http://localhost:8123 即可。 如果想撤销当前会话的http_proxy代理，使用 1unset http_proxy 1234567821:29:49-androidyue~$ curl ip.gs当前 IP：125.39.112.14 来自：中国天津天津 联通21:29:52-androidyue~$ export http_proxy=http://localhost:812321:30:07-androidyue~$ curl ip.gs当前 IP：210.140.193.128 来自：日本日本 21:30:12-androidyue~$ unset http_proxy21:30:37-androidyue~$ curl ip.gs当前 IP：125.39.112.14 来自：中国天津天津 联通 如果想要更长久的设置代理，可以将 12export http_proxy=http://localhost:8123export https_proxy=http://localhost:8123 加入.bashrc或者.bash_profile文件 另外，在浏览器中输入 1http://127.0.0.1:8123/ 便可以进入到 Polipo 的使用说明和配置界面。 设置浏览器和开机启动 最后就是将转换后的 Http 代理设置到浏览器中，地址是 127.0.0.1，端口 8123，代理类型当然是选择 Http 啦。对于 FireFor 用户来说，插件可以选择 AutoProxy 或 FoxyProxy 配置polipo在原文在这里 引申： 6.设置Git代理（接上面的polipo）复杂一些的设置Git代理 12345678git clone https://android.googlesource.com/tools/repo --config http.proxy=localhost:8123Cloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 492 KiB/s, done.Resolving deltas: 100% (1956/1956), done. 其实这样还是比较复杂，因为需要记忆的东西比较多， 下面是一个更简单的实现 首先，在.bashrc或者.bash_profile文件加入这一句。 1gp=&quot; --config http.proxy=localhost:8123&quot; 然后 执行source操作，更新当前bash配置。 更简单的使用git的方法 12345678git clone https://android.googlesource.com/tools/repo $gpCloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 483 KiB/s, done.Resolving deltas: 100% (1956/1956), done. [在git终端mac终端加入代理原文引自这里]http://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/) 7.apt-get怎么使用代理服务器升级到Ubuntu10.04后，发现apt-get的代理设置有改变了，在9.10以前使用“http_proxy”环境变量就可以令apt-get使用代理. 然后在Ubuntu10.04下就无效了，看来apt-get已经被改成不使用这个环境变量了。 一阵郁闷后，最后我发现在“首选项”-&gt;“网络代理”那里，多了个“System-wide”按钮（我用的是英文环境，不知道中文被翻译成怎样，关闭窗口时也会提示你），在这里设置后，apt-get确实可以使用代理了。 但是我依然鄙视这种改进，因为我通常就是偶尔使用代理，更新几个被墙掉的仓库而已（如dropbox和tor），根本不想使用全局代理，本来用终端就能搞定的事，现在切换代理要点N次鼠标，真烦。 所以我研究了一下，发现那个代理设置修改了两个文件，一个是“/etc/environment”，这个是系统的环境变量，里面定义了“http_proxy”等代理环境变量。另一个是“/etc/apt/apt.conf”，这个就是apt的配置，内容如下 在/etc/apt/apt.conf中追加 123Acquire::http::proxy &quot;http://127.0.0.1:8123/&quot;;Acquire::ftp::proxy &quot;ftp://127.0.0.1:8123/&quot;;Acquire::https::proxy &quot;https://127.0.0.1:8123/&quot;; 很明显的代理设置代码，我看了下apt-get的手册，发现可以用“-c”选项来指定使用配置文件，也就是复制一份为“~/apt_proxy.conf”，然后“网络代理”那里重置回直接连接，以后使用 1sudo apt-get -c ~/apt_proxy.conf update 1sudo apt-get -c ~/apt_proxy.conf install mongodb apt-get 使用代理在的原文在这里]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo + Github Pages搭建个人独立博客]]></title>
    <url>%2F2017%2F12%2F08%2F%E4%BD%BF%E7%94%A8Hexo-Github-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[系统环境配置要使用Hexo，需要在你的系统中支持Nodejs以及Git 安装Hexo1234567$ cd d:/hexo$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo g # 或者hexo generate$ hexo s # 或者hexo server，可以在http://localhost:4000/ 查看 这里有必要提下Hexo常用的几个命令：12$ hexo new &quot;postName&quot; #新建文章$ hexo new page &quot;pageName&quot; #新建页面 常用简写1234$ hexo n == hexo new$ hexo g == hexo generate$ hexo s == hexo server$ hexo d == hexo deploy 123hexo generate (hexo g) 生成静态文件，会在当前目录下生成一个新的叫做public的文件夹hexo server (hexo s) 启动本地web服务，用于博客的预览hexo deploy (hexo d) 部署播客到远端（比如github, heroku等平台） 常用组合12$ hexo d -g #生成部署$ hexo s -g #生成预览 Hexo主题设置 Github Pages设置 什么是Github Pages GitHub Pages 本用于介绍托管在GitHub的项目，不过，由于他的空间免费稳定，用来做搭建一个博客再好不过了。 每个帐号只能有一个仓库来存放个人主页，而且仓库的名字必须是username/username.github.io，这是特殊的命名约定。你可以通过http://username.github.io 来访问你的个人主页。 这里特别提醒一下，需要注意的个人主页的网站内容是在master分支下的。 部署Hexo到Github Pages 首先需要明白所谓部署到github的原理。 之前步骤中在Github上创建的那个特别的repo（jiji262.github.io）一个最大的特点就是其master中的html静态文件，可以通过链接http://jiji262.github.io来直接访问。 Hexo -g 会生成一个静态网站（第一次会生成一个public目录），这个静态文件可以直接访问。需要将hexo生成的静态网站，提交(git commit)到github上。明白了原理，怎么做自然就清晰了. 使用hexo deploy部署hexo deploy可以部署到很多平台，具体可以参考这个链接. 如果部署到github，需要在配置文件_config.xml中作如下修改： 1234deploy: type: git repo: git@github.com:jiji262/jiji262.github.io.git branch: master 然后在命令行中执行 1hexo d 即可完成部署。 如何多账号连接到github，本机连接不再输入密码 私有密钥和公有密钥是成对的两个文件，私有文件保存在自己的本机，公有密钥保存到另一端的服务器，网站等。github就是一种网站。只有保存了私有密钥的机器才能访问远程的服务器等。使用该键的好处是不用使用密码，而是以密钥的方式验证用户。 要想使本机能访问github。有四个步骤： 创建私有密钥和公有密钥 将公有密钥放到github里。 测试是否设置成功。 修改本地git配置文件，发布。 1234567git config --global user.email &quot;XXXX&quot;git config --global user.name &quot;XXX&quot;ssh-keygen -t rsa -C &quot;XXX&quot;ssh -T git@github.com 踩坑提醒 注意需要提前安装一个扩展： 1$ npm install hexo-deployer-git --save Hexo 主题配置参考文档1 参考文档2]]></content>
      <categories>
        <category>前端开发</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
